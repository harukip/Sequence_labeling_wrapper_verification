{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pycrfsuite\n",
    "import sklearn_crfsuite\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_total = 1\n",
    "# How many Set\n",
    "ALL = True\n",
    "if not ALL:\n",
    "    set_num = 1\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_num(path, istrain): # get the max num of leafnode and return\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    num, index = node_num(df['Leafnode'])\n",
    "    if istrain:\n",
    "        max_label = df['Label'].max()\n",
    "        return max(num), max_label\n",
    "    else:\n",
    "        return max(num)\n",
    "    \n",
    "def get_df(path):\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    return df\n",
    "\n",
    "def node_num(data): # generate a list of number of nodes that each page have\n",
    "    count = False\n",
    "    num_list = []\n",
    "    for index in range(len(data)):\n",
    "        if data[index] == 0 and count != False:\n",
    "            num_list.append(data[index-1] + 1)\n",
    "        else:\n",
    "            count = True\n",
    "    num_list.append(data[len(data) - 1] + 1)\n",
    "    #print(num_list)\n",
    "    count = 0\n",
    "    index_list = []\n",
    "    for i in num_list:\n",
    "        if count == 0:\n",
    "            index_list.append(i - 1)\n",
    "            count += 1\n",
    "        else:\n",
    "            index_list.append(index_list[count - 1] + i)\n",
    "            count += 1\n",
    "    #print(index_list)\n",
    "    return num_list, index_list\n",
    "\n",
    "def label_padding(data, num): # padding the data with zero when that page is less than max_num leafnode\n",
    "    #print(num)\n",
    "    output = []\n",
    "    count = 0\n",
    "    for page_num in num:\n",
    "        tmp = []\n",
    "        page = 0\n",
    "        #print('Page num: %s, Max: %s' %(page_num, max_num))\n",
    "        if page_num == max_num:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            #print('Num: %s, Count: %s' %(num[page], count))\n",
    "            page += 1\n",
    "        else:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            for i in range(max_num - page_num):\n",
    "                tmp.append(0) # Pad label with 0\n",
    "            #print('Num: %s, Count: %s' %(num[page], count))\n",
    "            page += 1\n",
    "        output.append(tmp)\n",
    "    return output\n",
    "\n",
    "def node_data(data, num): # padding the data with zero when that page is less than max_num leafnode\n",
    "    #print(num)\n",
    "    output = []\n",
    "    count = 0\n",
    "    for page_num in num:\n",
    "        tmp = []\n",
    "        page = 0\n",
    "        #print('Page num: %s, Max: %s' %(page_num, max_num))\n",
    "        if page_num == max_num:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            #print('Num: %s, Count: %s' %(num[page], count))\n",
    "            page += 1\n",
    "        else:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            for i in range(max_num - page_num):\n",
    "                tmp.append(9999)\n",
    "            #print('Num: %s, Count: %s' %(num[page], count))\n",
    "            page += 1\n",
    "        output.append(tmp)\n",
    "    return output\n",
    "\n",
    "def load_data_csv(df): # load the csv file and convert it to np array\n",
    "    num, index = node_num(df['Leafnode'])\n",
    "    feature_1 = np.array(node_data(df['Leafnode'], num))\n",
    "    df.drop(['Leafnode'], axis=1)\n",
    "    feature_2 = np.array(node_data(df['PTypeSet'], num))\n",
    "    #print(feature_2.shape)\n",
    "    df.drop(['PTypeSet'], axis=1)\n",
    "    feature_3 = np.array(node_data(df['TypeSet'], num))\n",
    "    #print(feature_3.shape)\n",
    "    df.drop(['TypeSet'], axis=1)\n",
    "    feature_4 = np.array(node_data(df['Contentid'], num))\n",
    "    #print(feature_4.shape)\n",
    "    df.drop(['Contentid'], axis=1)\n",
    "    feature_5 = np.array(node_data(df['Pathid'], num))\n",
    "    #print(feature_5.shape)\n",
    "    df.drop(['Pathid'], axis=1)\n",
    "    feature_6 = np.array(node_data(df['Simseqid'], num))\n",
    "    print(feature_6.shape)\n",
    "    df.drop(['Simseqid'], axis=1)\n",
    "    \n",
    "    feature_1 = np.expand_dims(feature_1, -1)\n",
    "    feature_2 = np.expand_dims(feature_2, -1)\n",
    "    feature_3 = np.expand_dims(feature_3, -1)\n",
    "    feature_4 = np.expand_dims(feature_4, -1)\n",
    "    feature_5 = np.expand_dims(feature_5, -1)\n",
    "    feature_6 = np.expand_dims(feature_6, -1)\n",
    "    \n",
    "    feature = np.concatenate([feature_1, feature_2, feature_3, feature_4, feature_5, feature_6], -1)\n",
    "    feature = np.reshape(feature, [feature_1.shape[0]*max_num, 6])\n",
    "    label_array = np.array(label_padding(df['Label'], num))\n",
    "    m_label = df['Label'].max()\n",
    "    df.drop(['Label'], axis=1)\n",
    "    #print(label_array)\n",
    "    label = []\n",
    "    for pages in tqdm(range(len(label_array))): # Loop each page\n",
    "        page = []\n",
    "        for node in range(len(label_array[pages])): # Loop each node\n",
    "            node_label = []\n",
    "            for label_t in range(max_label + 1): # Loop each label and a additional empty label ex.1~142 0 is empty\n",
    "                if label_t == label_array[pages][node]:\n",
    "                    node_label.append(1.0)\n",
    "                else:\n",
    "                    node_label.append(0.0)\n",
    "            page.append(node_label)\n",
    "        label.append(page)\n",
    "    label = np.array(label)\n",
    "    #feature = feature.astype(np.float32)\n",
    "    #feature = np.reshape(feature, [len(label_array)*max_num, 6])\n",
    "    label_array = label_array.flatten()\n",
    "    #print(feature)\n",
    "    #print(feature.shape)\n",
    "    '''\n",
    "        feature.shape = (None, Max_num, 6)\n",
    "        path_emb.shape = (None, Max_num, 50)\n",
    "        content_emb.shape = (None, Max_num, 50)\n",
    "        label.shape =(None, Max_num, Max_label)\n",
    "    '''\n",
    "    return feature, label_array, m_label\n",
    "\n",
    "def word2features(sent, i):\n",
    "    leaf = sent[i][0]\n",
    "    ptyp = sent[i][1]\n",
    "    typ = sent[i][2]\n",
    "    content = sent[i][3]\n",
    "    pathid = sent[i][4]\n",
    "    sim = sent[i][5]\n",
    "    features = {\n",
    "        'ptyp': ptyp,\n",
    "        'typ': typ,\n",
    "        'content': content,\n",
    "        'pathid': pathid,\n",
    "        'sim': sim,\n",
    "    }\n",
    "    if leaf != 0:\n",
    "        ptyp1 = sent[i-1][1]\n",
    "        typ1 = sent[i-1][2]\n",
    "        content1 = sent[i-1][3]\n",
    "        pathid1 = sent[i-1][4]\n",
    "        sim1 = sent[i-1][5]\n",
    "        features.update({\n",
    "            '-1:ptyp': ptyp1,\n",
    "            '-1:typ': typ1,\n",
    "            '-1:content': content1,\n",
    "            '-1:pathid': pathid1,\n",
    "            '-1:sim': sim1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOL'] = True\n",
    "    if i < len(sent)-1:\n",
    "        next_leaf = sent[i+1][0]\n",
    "        if next_leaf != 0:\n",
    "            ptyp1 = sent[i + 1][1]\n",
    "            typ1 = sent[i + 1][2]\n",
    "            content1 = sent[i + 1][3]\n",
    "            pathid1 = sent[i + 1][4]\n",
    "            sim1 = sent[i + 1][5]\n",
    "            features.update({\n",
    "                '+1:ptyp': ptyp1,\n",
    "                '+1:typ': typ1,\n",
    "                '+1:content': content1,\n",
    "                '+1:pathid': pathid1,\n",
    "                '+1:sim': sim1,\n",
    "            })\n",
    "        else:\n",
    "            features['EOL'] = True\n",
    "    else:\n",
    "        features['EOL'] = True\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [str(sent[i]) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check max_num in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939\n",
      "939\n"
     ]
    }
   ],
   "source": [
    "max_num_train, max_label_train = load_data_num(\"./data/train_raw.csv\", True)\n",
    "max_num_test = load_data_num(\"./data/ytest_raw.csv\", False)\n",
    "max_num = max(max_num_train, max_num_test)\n",
    "print(max_num_train)\n",
    "print(max_num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Set index File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 703}\n"
     ]
    }
   ],
   "source": [
    "col_set_dict={}\n",
    "if set_total > 0:\n",
    "    Set_dict = {}\n",
    "    with open(\"./data/Set_idx.txt\", \"r\") as set_file:\n",
    "        Set_dict = eval(set_file.readline())\n",
    "    col_set_dict = dict(map(reversed, Set_dict.items()))\n",
    "    print(Set_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 939)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "max_label = max_label_train\n",
    "df = get_df(\"./data/train_raw.csv\")\n",
    "feature_train, label_train, out_train = load_data_csv(df)\n",
    "page_num = int(len(feature_train)/max_num)\n",
    "#input_shape = feature_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(page_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=5000,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "feature_train = feature_train.tolist()\n",
    "label_train = label_train.tolist()\n",
    "X_train = [sent2features(feature_train)]\n",
    "#X_test = [sent2features(test_sent)]\n",
    "y_train = [sent2labels(label_train)]\n",
    "#y_test = [sent2labels(ga_sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "crf.fit(X_train, y_train)\n",
    "t = time.time()-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 939)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "df = get_df(\"./data/ytest_raw.csv\")\n",
    "feature_test, label_test, out_test = load_data_csv(df)\n",
    "\n",
    "feature_test = feature_test.tolist()\n",
    "label_test = label_test.tolist()\n",
    "X_test = [sent2features(feature_test)]\n",
    "page_test = int(len(feature_test)/max_num)\n",
    "print(page_test)\n",
    "#X_test = [sent2features(test_sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "ts_start = time.time()\n",
    "y_pred = crf.predict(X_test)\n",
    "ts = time.time()-ts_start\n",
    "y_pred = np.array(y_pred)\n",
    "result = np.reshape(y_pred, [page_test, max_num])\n",
    "result = result.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Column Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MD', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MD', 'MD', 'MD', 'OT', 'OD', 'MT', 'MD', 'MT', 'MD', 'MT', 'MD', 'MT', 'MD', 'MT', 'MD', 'MT', 'MD', 'MT', 'MD', 'OT', 'OD', 'OT', 'OC', 'MD', 'MT', 'MD', 'MD', 'MT', 'MD', 'MT', 'MD', 'MT', 'MD', 'MT', 'MD', 'MT', 'MD', 'MT', 'MD', 'MT', 'MD', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MD', 'OD', 'MD', 'OD', 'MD', 'MD', 'MT', 'MT', 'MD', 'MD', 'MD', 'MD', 'MD', 'MT', 'MT', 'MT', 'MC', 'OD', 'OD', 'MD', 'MD', 'MT', 'MT', 'MD', 'MD', 'MD', 'MD', 'MD', 'MT', 'OT', 'MD', 'MD', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OD', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OD', 'OT', 'OD', 'OT', 'OD', 'OT', 'OD', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'MT', 'OT', 'OT', 'MC', 'OD', 'OC', 'OD', 'OD', 'MR', 'OD', 'OD', 'MT', 'MC', 'MT', 'MT', 'OC', 'MT', 'MT', 'MD', 'OT', 'OC', 'OT', 'OT', 'OD', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OT', 'OD', 'OT', 'OT', 'OT', 'OT', 'OD', 'OT', 'OT', 'OD', 'OT', 'OT', 'OD', 'OT', 'OT', 'OD', 'OT', 'OT', 'OD', 'OT', 'OT', 'OD', 'OT', 'OD']\n"
     ]
    }
   ],
   "source": [
    "col_type = []\n",
    "with open(\"./data/TableA.txt\", \"r\") as file:\n",
    "    line = file.readline()\n",
    "    slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "    while(slot[0]!=\"ColType\"):\n",
    "        line = file.readline()\n",
    "        slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "    col_type = slot[1:]\n",
    "print(col_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Set_data = []\n",
    "if DEBUG:\n",
    "    with open(\"./crfsuite/data/predictions.csv\", \"w\") as file: # Create prediction file\n",
    "        for col in col_type: # loop to write the Col type\n",
    "            file.write(col + \"\\t\")\n",
    "            print(col + \"\\t\", end='')\n",
    "        file.write(\"\\n\")\n",
    "        for page in tqdm(range(len(result))): # Loop each page\n",
    "            sets = []\n",
    "            for label in range(max_label + 1): # Loop whole label\n",
    "                print(\"Label: \" + str(label))\n",
    "                if label == 0:\n",
    "                    continue\n",
    "                empty = True\n",
    "                isset = False\n",
    "                data = []\n",
    "                for node in range(len(result[page])):\n",
    "                    if result[page][node] == label:\n",
    "                        if empty == False and not isset:\n",
    "                            print(\" \", end='')\n",
    "                            file.write(\" \")\n",
    "                        empty = False\n",
    "                        if label in col_set_dict.keys() and set_total > 0: # That col is a Set\n",
    "                            isset = True\n",
    "                            data.append(node)\n",
    "                            print(\"Append:\" + str(node))\n",
    "                        else:\n",
    "                            print(str(node), end='')\n",
    "                            file.write(str(node))\n",
    "                if label in col_set_dict.keys() and set_total > 0: # That col is a Set\n",
    "                    print(str(col_set_dict[label])+\"-\"+str(page), end='')\n",
    "                    file.write(str(col_set_dict[label])+\"-\"+str(page))\n",
    "                    sets.append(data)\n",
    "                print(\"\\t\", end='')\n",
    "                file.write(\"\\t\")\n",
    "            print(\"\")\n",
    "            file.write(\"\\n\")\n",
    "            print(data)\n",
    "            Set_data.append(sets)\n",
    "else:\n",
    "    with open(\"./crfsuite/data/predictions.csv\", \"w\") as file: # Create prediction file\n",
    "        for col in col_type: # loop to write the Col type\n",
    "            file.write(col + \"\\t\")\n",
    "        file.write(\"\\n\")\n",
    "        for page in tqdm(range(len(result))): # Loop each page\n",
    "            sets = []\n",
    "            for label in range(max_label + 1): # Loop whole label\n",
    "                if label == 0:\n",
    "                    continue\n",
    "                empty = True\n",
    "                isset = False\n",
    "                data = []\n",
    "                for node in range(len(result[page])):\n",
    "                    if result[page][node] == label:\n",
    "                        if empty == False and not isset:\n",
    "                            file.write(\" \")\n",
    "                        empty = False\n",
    "                        if label in col_set_dict.keys() and set_total > 0: # That col is a Set\n",
    "                            isset = True\n",
    "                            data.append(node)\n",
    "                        else:\n",
    "                            file.write(str(node))\n",
    "                if label in col_set_dict.keys() and set_total > 0: # That col is a Set\n",
    "                    file.write(str(col_set_dict[label])+\"-\"+str(page))\n",
    "                    sets.append(data)\n",
    "                file.write(\"\\t\")\n",
    "            file.write(\"\\n\")\n",
    "            Set_data.append(sets)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Set data output for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    with open(\"./crfsuite/set/Set_data.txt\", \"w\") as set_train_file:\n",
    "        tmp = str(Set_data)\n",
    "        set_train_file.write(tmp)\n",
    "        if DEBUG:\n",
    "            print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Set Train File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data_count = []\n",
    "if set_total > 0:\n",
    "    for set_t in range(set_total):\n",
    "        with open(\"./data/Set-\"+ str(set_t+1) +\".txt\", \"r\") as set_file:\n",
    "            set_tmp = []\n",
    "            output_name = \"./crfsuite/set/Set-\"+ str(set_t+1) +\"_train_raw.csv\"\n",
    "            if DEBUG:\n",
    "                print(\"Generating:\" + output_name + \"\\n\")\n",
    "            output = open(output_name, \"w\")\n",
    "            output.write(\"Leafnode\\tPTypeSet\\tTypeSet\\tContentid\\tPathid\\tSimseqid\\tLabel\\n\")\n",
    "            line = set_file.readline()\n",
    "            slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            while(slot[0]!=\"ColType\"): \n",
    "                line = set_file.readline()\n",
    "                slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            with open(\"./set/Set-\"+ str(set_t+1) +\"_coltype.txt\", \"w\") as col_file:\n",
    "                col_file.write(str(slot[1:]))\n",
    "            line = set_file.readline() # First line of data\n",
    "            page_num = 0\n",
    "            count = 0\n",
    "            while(line != \"\"):\n",
    "                slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                data_info = slot[0].split(\"-\")\n",
    "                if(page_num != int(data_info[1])):\n",
    "                    set_tmp.append(count)\n",
    "                    count = 0\n",
    "                set_num = int(data_info[0])\n",
    "                page_num = int(data_info[1])\n",
    "                if DEBUG:\n",
    "                    print(str(data_info[0])+\"-\"+str(data_info[1])+\"-\"+str(data_info[2]))\n",
    "                idx = 1\n",
    "                sub_list = slot[1:]\n",
    "                while(\"\" in sub_list):\n",
    "                    sub_list.remove(\"\")\n",
    "                while(\" \" in sub_list):\n",
    "                    sub_list.remove(\" \")\n",
    "                for element in sub_list:\n",
    "                    count += 1\n",
    "                    if DEBUG:\n",
    "                        print(element)\n",
    "                    element = int(element)\n",
    "                    #print(content_train[page_num][element])\n",
    "                    output.write(str(feature_train[page_num*max_num+element][0])+\"\\t\")\n",
    "                    output.write(str(feature_train[page_num*max_num+element][1])+\"\\t\")\n",
    "                    output.write(str(feature_train[page_num*max_num+element][2])+\"\\t\")\n",
    "                    output.write(str(feature_train[page_num*max_num+element][3])+\"\\t\")\n",
    "                    output.write(str(feature_train[page_num*max_num+element][4])+\"\\t\")\n",
    "                    output.write(str(feature_train[page_num*max_num+element][5])+\"\\t\")\n",
    "                    output.write(str(idx) + \"\\n\")\n",
    "                    if DEBUG:\n",
    "                        print(feature_train[page_num*max_num+element][0])\n",
    "                    idx += 1\n",
    "                line = set_file.readline()\n",
    "            set_tmp.append(count)\n",
    "            output.close()\n",
    "        set_data_count.append(set_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 10, 4, 5, 6, 4, 4, 4, 10, 1, 6, 4, 7, 13, 18, 1, 14, 4, 5, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "if set_total > 0:\n",
    "    with open(\"./crfsuite/set/set_train_count.txt\", \"w\") as file:\n",
    "        file.write(str(set_data_count))\n",
    "        print(set_data_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Set Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 102466.71it/s]\n"
     ]
    }
   ],
   "source": [
    "set_data_count = []\n",
    "if set_total > 0:\n",
    "    for set_t in range(set_total):\n",
    "        set_tmp = []\n",
    "        with open(\"./crfsuite/set/Set-\"+ str(set_t+1) +\"_ytest_raw.csv\", \"w\") as set_file:\n",
    "            set_file.write(\"Leafnode\\tPTypeSet\\tTypeSet\\tContentid\\tPathid\\tSimseqid\\tLabel\\n\")\n",
    "            for pages in tqdm(range(len(Set_data))):\n",
    "                count = 0\n",
    "                for node in Set_data[pages][set_t]:\n",
    "                    count += 1\n",
    "                    set_file.write(str(feature_train[pages*max_num+node][0])+\"\\t\")\n",
    "                    set_file.write(str(feature_train[pages*max_num+node][1])+\"\\t\")\n",
    "                    set_file.write(str(feature_train[pages*max_num+node][2])+\"\\t\")\n",
    "                    set_file.write(str(feature_train[pages*max_num+node][3])+\"\\t\")\n",
    "                    set_file.write(str(feature_train[pages*max_num+node][4])+\"\\t\")\n",
    "                    set_file.write(str(feature_train[pages*max_num+node][5])+\"\\t\")\n",
    "                    set_file.write(str(0) + \"\\n\")\n",
    "                set_tmp.append(count)\n",
    "        set_data_count.append(set_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    with open(\"./crfsuite/set/set_test_count.txt\", \"w\") as file:\n",
    "        file.write(str(set_data_count))\n",
    "        if DEBUG:\n",
    "            print(set_data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''timef = open(\"./crfsuite/data/time_crf.txt\",\"w\")\n",
    "print(\"\\ntrain time:\"+str(t))\n",
    "timef.write(\"train:\"+str(t)+\"\\n\")\n",
    "print(\"test time:\"+str(ts))\n",
    "print(\"per page:\"+ str(float(ts)/len(result))+\"\\n\")\n",
    "timef.write(\"test:\"+str(ts)+\"\\n\")\n",
    "timef.write(\"per page:\"+ str(float(ts)/len(result))+\"\\n\")\n",
    "timef.close()'''\n",
    "page_c = len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(path):\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    return df\n",
    "\n",
    "def max_num_set(set_data_count, set_total):\n",
    "    max_set = []\n",
    "    for i in range(set_total):\n",
    "        max_set.append(0)\n",
    "    for sets in range(len(set_data_count)):\n",
    "        max_set[sets] = max(set_data_count[sets])\n",
    "    return max_set\n",
    "\n",
    "def feature_padding(df, set_count, set_num):\n",
    "    feature = []\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            feature.append(df[count])\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                feature.append(9999)\n",
    "    return feature\n",
    "\n",
    "def emb_padding(df, set_count, set_num, pad_len):\n",
    "    emb = []\n",
    "    tmp = []\n",
    "    for i in range(pad_len):\n",
    "        tmp.append(0)\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            emb.append(eval(df[count]))\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                emb.append(tmp)\n",
    "    print(count)\n",
    "    return emb\n",
    "\n",
    "def one_of_n(ans, total):\n",
    "    tmp = []\n",
    "    for i in range(int(total)):\n",
    "        if ans == i:\n",
    "            tmp.append(1.0)\n",
    "        else:\n",
    "            tmp.append(0.0)\n",
    "    return tmp\n",
    "\n",
    "def label_padding(df, set_count, set_num):\n",
    "    label = []\n",
    "    #tmp = one_of_n(0, max_label+1)\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            label.append(df[count])\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                label.append(0)\n",
    "    return label\n",
    "\n",
    "def to_train_array(df, set_count, set_num):\n",
    "    feature_1 = np.array(feature_padding(df['Leafnode'], set_count, set_num))\n",
    "    feature_2 = np.array(feature_padding(df['PTypeSet'], set_count, set_num))\n",
    "    feature_3 = np.array(feature_padding(df['TypeSet'], set_count, set_num))\n",
    "    feature_4 = np.array(feature_padding(df['Contentid'], set_count, set_num))\n",
    "    feature_5 = np.array(feature_padding(df['Pathid'], set_count, set_num))\n",
    "    feature_6 = np.array(feature_padding(df['Simseqid'], set_count, set_num))\n",
    "    \n",
    "    feature_1 = np.expand_dims(feature_1, -1)\n",
    "    feature_2 = np.expand_dims(feature_2, -1)\n",
    "    feature_3 = np.expand_dims(feature_3, -1)\n",
    "    feature_4 = np.expand_dims(feature_4, -1)\n",
    "    feature_5 = np.expand_dims(feature_5, -1)\n",
    "    feature_6 = np.expand_dims(feature_6, -1)\n",
    "    \n",
    "    feature = np.concatenate([feature_1, feature_2, feature_3, feature_4, feature_5, feature_6], -1)\n",
    "    feature = np.reshape(feature, [len(set_count[set_num-1])*max_set[set_num-1], 6])\n",
    "    \n",
    "    label = np.array(label_padding(df['Label'], set_count, set_num))\n",
    "    label = np.reshape(label, [len(set_count[set_num-1])*max_set[set_num-1]])\n",
    "    return feature, label\n",
    "\n",
    "def word2features(sent, i):\n",
    "    leaf = sent[i][0]\n",
    "    ptyp = sent[i][1]\n",
    "    typ = sent[i][2]\n",
    "    content = sent[i][3]\n",
    "    pathid = sent[i][4]\n",
    "    sim = sent[i][5]\n",
    "    features = {\n",
    "        'ptyp': ptyp,\n",
    "        'typ': typ,\n",
    "        'content': content,\n",
    "        'pathid': pathid,\n",
    "        'sim': sim,\n",
    "    }\n",
    "    if leaf != 0:\n",
    "        ptyp1 = sent[i-1][1]\n",
    "        typ1 = sent[i-1][2]\n",
    "        content1 = sent[i-1][3]\n",
    "        pathid1 = sent[i-1][4]\n",
    "        sim1 = sent[i-1][5]\n",
    "        features.update({\n",
    "            '-1:ptyp': ptyp1,\n",
    "            '-1:typ': typ1,\n",
    "            '-1:content': content1,\n",
    "            '-1:pathid': pathid1,\n",
    "            '-1:sim': sim1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOL'] = True\n",
    "    if i < len(sent)-1:\n",
    "        next_leaf = sent[i+1][0]\n",
    "        if next_leaf != 0:\n",
    "            ptyp1 = sent[i + 1][1]\n",
    "            typ1 = sent[i + 1][2]\n",
    "            content1 = sent[i + 1][3]\n",
    "            pathid1 = sent[i + 1][4]\n",
    "            sim1 = sent[i + 1][5]\n",
    "            features.update({\n",
    "                '+1:ptyp': ptyp1,\n",
    "                '+1:typ': typ1,\n",
    "                '+1:content': content1,\n",
    "                '+1:pathid': pathid1,\n",
    "                '+1:sim': sim1,\n",
    "            })\n",
    "        else:\n",
    "            features['EOL'] = True\n",
    "    else:\n",
    "        features['EOL'] = True\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [str(sent[i]) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    Set_data = []\n",
    "    set_train_count = []\n",
    "    set_test_count = []\n",
    "    with open(\"./crfsuite/set/Set_data.txt\", \"r\") as set_file:\n",
    "        Set_data = eval(set_file.readline())\n",
    "    with open(\"./crfsuite/set/set_train_count.txt\", \"r\") as set_file:\n",
    "        set_train_count = eval(set_file.readline())\n",
    "    with open(\"./crfsuite/set/set_test_count.txt\", \"r\") as set_file:\n",
    "        set_test_count = eval(set_file.readline())\n",
    "    max_num_train = max_num_set(set_train_count, set_total)\n",
    "    max_num_test = max_num_set(set_test_count, set_total)\n",
    "    max_set = []\n",
    "    for i in range(len(max_num_train)):\n",
    "        max_set.append(max(max_num_train[i], max_num_test[i]))\n",
    "    if DEBUG:\n",
    "        print(max_num_train)\n",
    "        print(max_num_test)\n",
    "        print(max_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 17394.13it/s]\n"
     ]
    }
   ],
   "source": [
    "if set_total > 0:\n",
    "    for num in range(set_total):\n",
    "        set_num = num + 1\n",
    "        # Load Train file & Test file\n",
    "        df = get_df(\"./crfsuite/set/Set-\"+str(set_num)+\"_train_raw.csv\")\n",
    "        max_num = max_set[set_num-1]\n",
    "        max_label = max(df['Label'])\n",
    "        feature_train, label_train = to_train_array(df, set_train_count, set_num)\n",
    "        page_num = int(len(feature_train)/max_num)\n",
    "        crf = sklearn_crfsuite.CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=50000,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "        feature_train = feature_train.tolist()\n",
    "        label_train = label_train.tolist()\n",
    "        X_train = [sent2features(feature_train)]\n",
    "        y_train = [sent2labels(label_train)]\n",
    "        \n",
    "        \n",
    "        # Train\n",
    "        start = time.time()\n",
    "        crf.fit(X_train, y_train)\n",
    "        t += time.time()-start\n",
    "        \n",
    "        # Save model\n",
    "        \n",
    "        \n",
    "        #Load model\n",
    "        \n",
    "        \n",
    "        # Load Test file\n",
    "        df = get_df(\"./crfsuite/set/Set-\"+str(set_num)+\"_ytest_raw.csv\")\n",
    "        feature_test, label_test = to_train_array(df, set_test_count, set_num)\n",
    "        feature_test = feature_test.tolist()\n",
    "        label_test = label_test.tolist()\n",
    "        X_test = [sent2features(feature_test)]\n",
    "        page_test = int(len(feature_test)/max_num)\n",
    "            \n",
    "        # Prediction\n",
    "        labels = list(crf.classes_)\n",
    "        ts_start = time.time()\n",
    "        y_pred = crf.predict(X_test)\n",
    "        ts += time.time()-ts_start\n",
    "        y_pred = np.array(y_pred)\n",
    "        result = np.reshape(y_pred, [page_test, max_num])\n",
    "        result = result.astype(np.int64)\n",
    "        \n",
    "        # Read Col\n",
    "        col_type = []\n",
    "        with open(\"./set/Set-\"+str(set_num)+\"_coltype.txt\", \"r\") as file:\n",
    "            tmp = file.readline()\n",
    "            slot = eval(tmp)\n",
    "            col_type = slot\n",
    "            \n",
    "        # Output\n",
    "        Set = []\n",
    "        if DEBUG:\n",
    "            with open(\"./crfsuite/set/set-\"+str(set_num)+\".csv\", \"w\") as file: # Create prediction file\n",
    "                for col in col_type: # loop to write the Col type\n",
    "                    file.write(col + \"\\t\")\n",
    "                    print(col + \"\\t\", end='')\n",
    "                print(\"\")\n",
    "                file.write(\"\\n\")\n",
    "                current_pos = 1\n",
    "                for page in tqdm(range(len(result))): # Loop each page\n",
    "                    p_tmp = []\n",
    "                    for cols in range(max_label+1):\n",
    "                        c_tmp = []\n",
    "                        for node in range(len(result[page])):\n",
    "                            r = result[page][node]\n",
    "                            if r == cols:\n",
    "                                c_tmp.append(node)\n",
    "                        p_tmp.append(c_tmp)\n",
    "                    Set.append(p_tmp)\n",
    "                Set_tmp = Set.copy()\n",
    "                for page in range(len(Set_tmp)):\n",
    "                    empty = False\n",
    "                    col = []\n",
    "                    for i in range(len(Set_tmp[page])):\n",
    "                        col.append(False)\n",
    "                    col[0] = True\n",
    "                    while(not empty):\n",
    "                        for cols in range(len(Set_tmp[page])):\n",
    "                            if len(Set_tmp[page][cols]) == 0:\n",
    "                                col[cols] = True\n",
    "                                if cols != 0:\n",
    "                                    print(\"\\t\", end=\"\")\n",
    "                                    file.write(\"\\t\")\n",
    "                            else:\n",
    "                                n = str(int(feature_test[page*max_num+Set_tmp[page][cols][0]][0]))\n",
    "                                if cols != 0:\n",
    "                                    print(n+\"\\t\", end=\"\")\n",
    "                                    file.write(n+\"\\t\")\n",
    "                                del Set_tmp[page][cols][0]\n",
    "                                if len(Set_tmp[page][cols]) == 0:\n",
    "                                    col[cols] = True\n",
    "                            empty = True\n",
    "                            for i in col:\n",
    "                                if i == False:\n",
    "                                    empty = False\n",
    "                                    break\n",
    "                        print(\"\\n\", end=\"\")\n",
    "                        file.write(\"\\n\")\n",
    "        else:\n",
    "            with open(\"./crfsuite/set/set-\"+str(set_num)+\".csv\", \"w\") as file: # Create prediction file\n",
    "                for col in col_type: # loop to write the Col type\n",
    "                    file.write(col + \"\\t\")\n",
    "                file.write(\"\\n\")\n",
    "                current_pos = 1\n",
    "                for page in tqdm(range(len(result))): # Loop each page\n",
    "                    p_tmp = []\n",
    "                    for cols in range(max_label+1):\n",
    "                        c_tmp = []\n",
    "                        for node in range(len(result[page])):\n",
    "                            r = result[page][node]\n",
    "                            if r == cols:\n",
    "                                c_tmp.append(node)\n",
    "                        p_tmp.append(c_tmp)\n",
    "                    Set.append(p_tmp)\n",
    "                Set_tmp = Set.copy()\n",
    "                for page in range(len(Set_tmp)):\n",
    "                    empty = False\n",
    "                    col = []\n",
    "                    for i in range(len(Set_tmp[page])):\n",
    "                        col.append(False)\n",
    "                    col[0] = True\n",
    "                    while(not empty):\n",
    "                        for cols in range(len(Set_tmp[page])):\n",
    "                            if len(Set_tmp[page][cols]) == 0:\n",
    "                                col[cols] = True\n",
    "                                if cols != 0:\n",
    "                                    file.write(\"\\t\")\n",
    "                            else:\n",
    "                                n = str(int(feature_test[page*max_num+Set_tmp[page][cols][0]][0]))\n",
    "                                if cols != 0:\n",
    "                                    file.write(n+\"\\t\")\n",
    "                                del Set_tmp[page][cols][0]\n",
    "                                if len(Set_tmp[page][cols]) == 0:\n",
    "                                    col[cols] = True\n",
    "                            empty = True\n",
    "                            for i in col:\n",
    "                                if i == False:\n",
    "                                    empty = False\n",
    "                                    break\n",
    "                        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train time:7650.392967224121\n",
      "test time:39.63529944419861\n",
      "per page:1.3211766481399536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timef = open(\"./crfsuite/data/time_crf.txt\",\"w\")\n",
    "print(\"\\ntrain time:\"+str(t))\n",
    "timef.write(\"train:\"+str(t)+\"\\n\")\n",
    "print(\"test time:\"+str(ts))\n",
    "print(\"per page:\"+ str(float(ts)/page_c)+\"\\n\")\n",
    "timef.write(\"test:\"+str(ts)+\"\\n\")\n",
    "timef.write(\"per page:\"+ str(float(ts)/page_c)+\"\\n\")\n",
    "timef.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
