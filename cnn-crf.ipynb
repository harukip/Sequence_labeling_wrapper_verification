{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, unicode_literals, print_function, absolute_import\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from crflayer import CRF\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn_crfsuite import metrics\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_total = 1\n",
    "# How many Set\n",
    "BATCH_SIZE = 1      # Training bath size\n",
    "VAL_BATCH_SIZE = 1  # Validation batch size\n",
    "\n",
    "DEBUG = False        # Print element\n",
    "SINGLE_GPU = True    # Use Single GPU\n",
    "path_max_len = 30    # padding length\n",
    "path_emb_size = 5    # embedding size\n",
    "\n",
    "con_max_len = 50    # padding length\n",
    "con_emb_size = 5    # embedding size\n",
    "\n",
    "feature_emb_size = 3\n",
    "\n",
    "EPOCHS = 10000        # Train epochs\n",
    "conv_num = 5        # First cnn filter num\n",
    "#max_num = 206       # How many nodes should pad\n",
    "UNTIL_LOSS = 0.001    # When achieve loss then stop\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.004) # Set learning rate\n",
    "NO_IMPROVE = 2     # Stop when no improve for epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "if SINGLE_GPU:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only use the first GPU\n",
    "      try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "      except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "tokenizer_content = tf.keras.preprocessing.text.Tokenizer(num_words=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_num(data): # generate a list of number of nodes that each page have\n",
    "    count = False\n",
    "    num_list = []\n",
    "    for index in range(len(data)):\n",
    "        if data[index] == 0 and count != False:\n",
    "            num_list.append(data[index-1] + 1)\n",
    "        else:\n",
    "            count = True\n",
    "    num_list.append(data[len(data) - 1] + 1)\n",
    "    #print(num_list)\n",
    "    count = 0\n",
    "    index_list = []\n",
    "    for i in num_list:\n",
    "        if count == 0:\n",
    "            index_list.append(i - 1)\n",
    "            count += 1\n",
    "        else:\n",
    "            index_list.append(index_list[count - 1] + i)\n",
    "            count += 1\n",
    "    #print(index_list)\n",
    "    return num_list, index_list\n",
    "\n",
    "\n",
    "def node_data(data, num): # padding the data with zero when that page is less than max_num leafnode\n",
    "    #print(num)\n",
    "    output = []\n",
    "    count = 0\n",
    "    for page_num in num:\n",
    "        tmp = []\n",
    "        page = 0\n",
    "        #print('Page num: %s, Max: %s' %(page_num, max_num))\n",
    "        if page_num == max_num:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            #print('Num: %s, Count: %s' %(num[page], count))\n",
    "            page += 1\n",
    "        else:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            for i in range(max_num - page_num):\n",
    "                tmp.append(99999)\n",
    "            #print('Num: %s, Count: %s' %(num[page], count))\n",
    "            page += 1\n",
    "        output.append(tmp)\n",
    "    return output\n",
    "\n",
    "def label_padding(data, num): # padding the data with zero when that page is less than max_num leafnode\n",
    "    #print(num)\n",
    "    output = []\n",
    "    count = 0\n",
    "    for page_num in num:\n",
    "        tmp = []\n",
    "        page = 0\n",
    "        #print('Page num: %s, Max: %s' %(page_num, max_num))\n",
    "        if page_num == max_num:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            #print('Num: %s, Count: %s' %(num[page], count))\n",
    "            page += 1\n",
    "        else:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            for i in range(max_num - page_num):\n",
    "                tmp.append(0) # Pad label with 0\n",
    "            #print('Num: %s, Count: %s' %(num[page], count))\n",
    "            page += 1\n",
    "        output.append(tmp)\n",
    "    return output\n",
    "\n",
    "\n",
    "def node_emb(data, num, pad_len): # padding the emb with empty when that page is less than max_num leafnode\n",
    "    output = []\n",
    "    count = 0\n",
    "    tmp2 = []\n",
    "    for j in range(pad_len):\n",
    "        tmp2.append(0.0)\n",
    "    for page_num in num:\n",
    "        tmp = []\n",
    "        page = 0\n",
    "        #print('Page num: %s, Max: %s' %(page_num, max_num))\n",
    "        if page_num == max_num:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            #print('Num: %s, Count: %s' %(num[page], count))\n",
    "            page += 1\n",
    "        else:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            for i in range(max_num - page_num):\n",
    "                tmp.append(tmp2)\n",
    "            #print('Num: %s, Count: %s' %(num[page], count))\n",
    "            page += 1\n",
    "        output.append(tmp)\n",
    "    return output\n",
    "\n",
    "def get_df(path): # Read csv\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    return df\n",
    "    \n",
    "\n",
    "def load_data_csv(df): # load the csv file and convert it to np array\n",
    "    path_encoded = tokenizer_path.texts_to_sequences(df['Path'])\n",
    "    df['Content'] = df['Content'].str.replace('/|\\.|\\?|:|=|,|<|>|&|@|\\+|-|#|~|\\|', ' ')\n",
    "    #print(df['Content'])\n",
    "    df['Content'] = df['Content'].astype(str)\n",
    "    content_encoded = tokenizer_content.texts_to_sequences(df['Content'])\n",
    "    path_pad = tf.keras.preprocessing.sequence.pad_sequences(path_encoded, path_max_len, padding='post')\n",
    "    content_pad = tf.keras.preprocessing.sequence.pad_sequences(content_encoded, con_max_len, padding='post')\n",
    "    if DEBUG:\n",
    "        print(path_pad.shape)\n",
    "        print(content_pad.shape)\n",
    "    num, index = node_num(df['Leafnode'])\n",
    "    path = np.array(node_emb(path_pad, num, path_max_len))\n",
    "    content = np.array(node_emb(content_pad, num, con_max_len))\n",
    "    if DEBUG:\n",
    "        print(path.shape)\n",
    "        print(content.shape)\n",
    "    feature_1 = np.array(node_data(df['Leafnode'], num))\n",
    "    #print(len(node_data(df['Leafnode'], num)[0]))\n",
    "    #print(node_data(df['Leafnode'], num)[7])\n",
    "    df.drop(['Leafnode'], axis=1)\n",
    "    feature_2 = np.array(node_data(df['PTypeSet'], num))\n",
    "    #print(feature_2.shape)\n",
    "    df.drop(['PTypeSet'], axis=1)\n",
    "    feature_3 = np.array(node_data(df['TypeSet'], num))\n",
    "    #print(feature_3.shape)\n",
    "    df.drop(['TypeSet'], axis=1)\n",
    "    feature_4 = np.array(node_data(df['Contentid'], num))\n",
    "    #print(feature_4.shape)\n",
    "    df.drop(['Contentid'], axis=1)\n",
    "    feature_5 = np.array(node_data(df['Pathid'], num))\n",
    "    #print(feature_5.shape)\n",
    "    df.drop(['Pathid'], axis=1)\n",
    "    feature_6 = np.array(node_data(df['Simseqid'], num))\n",
    "    #print(feature_6.shape)\n",
    "    df.drop(['Simseqid'], axis=1)\n",
    "    \n",
    "    label_array = np.array(label_padding(df['Label'], num))\n",
    "    m_label = df['Label'].max()\n",
    "    df.drop(['Label'], axis=1)\n",
    "    #print(label_array)\n",
    "    label = []\n",
    "    path_arr = []\n",
    "    content_arr = []\n",
    "    for pages in tqdm(range(len(label_array))): # Loop each page\n",
    "        page = []\n",
    "        path_page = []\n",
    "        content_page = []\n",
    "        for node in range(len(label_array[pages])): # Loop each node\n",
    "            node_label = []\n",
    "            for label_t in range(max_label + 1): # Loop each label and a additional empty label ex.1~142 0 is empty\n",
    "                if label_t == label_array[pages][node]:\n",
    "                    node_label.append(1.0)\n",
    "                else:\n",
    "                    node_label.append(0.0)\n",
    "            page.append(node_label)\n",
    "            path_page.append(path[pages][node])\n",
    "            content_page.append(content[pages][node])\n",
    "        label.append(page)\n",
    "        path_arr.append(path_page)\n",
    "        content_arr.append(content_page)\n",
    "    label = np.array(label)\n",
    "    path_arr = np.array(path_arr)\n",
    "    content_arr = np.array(content_arr)\n",
    "    path_arr = np.reshape(path_arr, [len(label_array), max_num, path_max_len])\n",
    "    content_arr = np.reshape(content_arr, [len(label_array), max_num, con_max_len])\n",
    "    label = np.reshape(label, [len(label_array), max_num, max_label+1])\n",
    "    return feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, path_arr, content_arr, label, m_label\n",
    "\n",
    "\n",
    "def load_data_num(path, istrain): # get the max num of leafnode and return\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    num, index = node_num(df['Leafnode'])\n",
    "    if istrain:\n",
    "        max_label = df['Label'].max()\n",
    "        return max(num), max_label\n",
    "    else:\n",
    "        return max(num)\n",
    "\n",
    "\n",
    "class LossHistory(tf.keras.callbacks.Callback): # Draw the figure of train\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('accuracy'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_accuracy'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('accuracy'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_accuracy'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        f1 = plt.figure(1)\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc')\n",
    "\n",
    "        f2 = plt.figure(2)\n",
    "        plt.plot(iters, self.losses[loss_type], 'r', label='train loss')\n",
    "        plt.plot(iters, self.val_loss[loss_type], 'b', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('loss')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train until loss Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingByLossVal(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='loss', value=UNTIL_LOSS, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check max_num in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939\n",
      "939\n"
     ]
    }
   ],
   "source": [
    "max_num_train, max_label_train = load_data_num(\"./data/train_raw.csv\", True)\n",
    "max_num_test = load_data_num(\"./data/ytest_raw.csv\", False)\n",
    "max_num = max(max_num_train, max_num_test)\n",
    "if DEBUG:\n",
    "    print(max_num_train)\n",
    "    print(max_num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Set index File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 703}\n"
     ]
    }
   ],
   "source": [
    "col_set_dict={}\n",
    "if set_total > 0:\n",
    "    Set_dict = {}\n",
    "    with open(\"./data/Set_idx.txt\", \"r\") as set_file:\n",
    "        Set_dict = eval(set_file.readline())\n",
    "    col_set_dict = dict(map(reversed, Set_dict.items()))\n",
    "    if DEBUG:\n",
    "        print(Set_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.54it/s]\n"
     ]
    }
   ],
   "source": [
    "max_label = max_label_train\n",
    "df = get_df(\"./data/train_raw.csv\")\n",
    "tokenizer_path.fit_on_texts(df['Path'])\n",
    "tokenizer_content.fit_on_texts(df['Content'].astype(str))\n",
    "path_word_size = len(tokenizer_path.index_docs)\n",
    "con_word_size = len(tokenizer_content.index_docs)\n",
    "feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6, path_train, content_train, label_train, out_train = load_data_csv(df)\n",
    "crf = CRF(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(feature_train.shape)\n",
    "    print(label_train.shape)\n",
    "    print(path_word_size)\n",
    "    print(con_word_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    path_input = tf.keras.Input(shape=(max_num, path_max_len), name='Path_emb_input')\n",
    "    content_input = tf.keras.Input(shape=(max_num, con_max_len), name='Content_emb_input')\n",
    "    feature_input_1 = tf.keras.Input(shape=(max_num,), name='Feature_input1')\n",
    "    feature_input_2 = tf.keras.Input(shape=(max_num,), name='Feature_input2')\n",
    "    feature_input_3 = tf.keras.Input(shape=(max_num,), name='Feature_input3')\n",
    "    feature_input_4 = tf.keras.Input(shape=(max_num,), name='Feature_input4')\n",
    "    feature_input_5 = tf.keras.Input(shape=(max_num,), name='Feature_input5')\n",
    "    feature_input_6 = tf.keras.Input(shape=(max_num,), name='Feature_input6')\n",
    "    \n",
    "    path_f = tf.keras.layers.Flatten()(path_input)\n",
    "    content_f = tf.keras.layers.Flatten()(content_input)\n",
    "    \n",
    "    path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_f)\n",
    "    content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_f)\n",
    "    f_1_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_1)\n",
    "    f_2_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_2)\n",
    "    f_3_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_3)\n",
    "    f_4_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_4)\n",
    "    f_5_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_5)\n",
    "    f_6_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_6)\n",
    "    \n",
    "    path_emb = tf.reshape(path_emb, [-1, max_num, path_max_len*path_emb_size])\n",
    "    content_emb = tf.reshape(content_emb, [-1, max_num, con_max_len*con_emb_size])\n",
    "    \n",
    "    path_emb = tf.expand_dims(path_emb, -1)\n",
    "    content_emb = tf.expand_dims(content_emb, -1)\n",
    "    \n",
    "    #print(path_emb.shape)\n",
    "    \n",
    "    path_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3,  path_max_len*path_emb_size), strides=(1, path_max_len*path_emb_size), name='Conv_for_Path_emb', padding='same')(path_emb)\n",
    "    content_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3, con_max_len*con_emb_size), strides=(1, con_max_len*con_emb_size), name='Conv_for_Content_emb', padding='same')(content_emb)\n",
    "    \n",
    "    #print(path_feature.shape)\n",
    "    \n",
    "    path = tf.reshape(path_feature, [-1, conv_num])\n",
    "    content = tf.reshape(content_feature, [-1, conv_num])\n",
    "    \n",
    "    f_1_emb = tf.reshape(f_1_emb, [-1, feature_emb_size])\n",
    "    f_2_emb = tf.reshape(f_2_emb, [-1, feature_emb_size])\n",
    "    f_3_emb = tf.reshape(f_3_emb, [-1, feature_emb_size])\n",
    "    f_4_emb = tf.reshape(f_4_emb, [-1, feature_emb_size])\n",
    "    f_5_emb = tf.reshape(f_5_emb, [-1, feature_emb_size])\n",
    "    f_6_emb = tf.reshape(f_6_emb, [-1, feature_emb_size])\n",
    "\n",
    "    combine = tf.keras.layers.concatenate([path, content, f_1_emb, f_2_emb, f_3_emb, f_4_emb, f_5_emb, f_6_emb], -1)\n",
    "    d = combine\n",
    "    d = tf.keras.layers.Dense(max_label+1)(d)\n",
    "    d = tf.reshape(d, [-1, max_num, max_label+1])\n",
    "    output = crf(d)\n",
    "    model = tf.keras.Model(inputs=[path_input, content_input, feature_input_1, feature_input_2, feature_input_3, feature_input_4, feature_input_5, feature_input_6], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1013 14:21:12.529296 139743629920000 deprecation.py:323] From /home/rick/rick/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:2403: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Path_emb_input (InputLayer)     [(None, 939, 30)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Content_emb_input (InputLayer)  [(None, 939, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 28170)        0           Path_emb_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 46950)        0           Content_emb_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 28170, 5)     135         flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 46950, 5)     24825       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 939, 150)]   0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 939, 250)]   0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 939, 150, 1) 0           tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_1 (Tenso [(None, 939, 250, 1) 0           tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input1 (InputLayer)     [(None, 939)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input2 (InputLayer)     [(None, 939)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input3 (InputLayer)     [(None, 939)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input4 (InputLayer)     [(None, 939)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input5 (InputLayer)     [(None, 939)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input6 (InputLayer)     [(None, 939)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv_for_Path_emb (Conv2D)      (None, 939, 1, 5)    2255        tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Conv_for_Content_emb (Conv2D)   (None, 939, 1, 5)    3755        tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 939, 3)       300000      Feature_input1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 939, 3)       300000      Feature_input2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 939, 3)       300000      Feature_input3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 939, 3)       300000      Feature_input4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 939, 3)       300000      Feature_input5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 939, 3)       300000      Feature_input6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 5)]          0           Conv_for_Path_emb[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 5)]          0           Conv_for_Content_emb[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 3)]          0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 3)]          0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 3)]          0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 3)]          0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 3)]          0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 3)]          0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 28)           0           tf_op_layer_Reshape_2[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_3[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_4[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_5[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_6[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_7[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_8[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 817)          23693       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_10 (TensorF [(None, 939, 817)]   0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "crf (CRF)                       (None, 939, 817)     667489      tf_op_layer_Reshape_10[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,522,152\n",
      "Trainable params: 2,522,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if SINGLE_GPU:\n",
    "    model = get_model()\n",
    "    model.compile(\n",
    "        loss=crf.loss,\n",
    "        optimizer=opt,\n",
    "        metrics=[crf.accuracy]\n",
    "    )\n",
    "else:\n",
    "    mirror_stg = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())\n",
    "    with mirror_stg.scope():\n",
    "        model = get_model()\n",
    "        model.compile(\n",
    "            loss=crf.loss,\n",
    "            optimizer=opt,\n",
    "            metrics=[crf.accuracy]\n",
    "        )\n",
    "print(model.summary())\n",
    "history = LossHistory()\n",
    "stop_when_no_improve = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=0, patience = NO_IMPROVE, restore_best_weights=True)\n",
    "until_loss = EarlyStoppingByLossVal(monitor='loss', value=UNTIL_LOSS, verbose=1)\n",
    "callbacks = [history, stop_when_no_improve, until_loss]\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30 samples\n",
      "Epoch 1/10000\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[1,938,817,817] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node Adam/gradients/loss/crf_loss/Sum_2_grad/Tile (defined at <ipython-input-14-4e32205a78be>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Adam/Adam/update/mul_4/_294]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[1,938,817,817] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node Adam/gradients/loss/crf_loss/Sum_2_grad/Tile (defined at <ipython-input-14-4e32205a78be>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_keras_scratch_graph_3355]\n\nFunction call stack:\nkeras_scratch_graph -> keras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4e32205a78be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_train_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_train_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_train_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_train_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_train_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_train_6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rick/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/rick/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/rick/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rick/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rick/lib/python3.5/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rick/lib/python3.5/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rick/lib/python3.5/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rick/lib/python3.5/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rick/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[1,938,817,817] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node Adam/gradients/loss/crf_loss/Sum_2_grad/Tile (defined at <ipython-input-14-4e32205a78be>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Adam/Adam/update/mul_4/_294]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[1,938,817,817] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node Adam/gradients/loss/crf_loss/Sum_2_grad/Tile (defined at <ipython-input-14-4e32205a78be>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_keras_scratch_graph_3355]\n\nFunction call stack:\nkeras_scratch_graph -> keras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.fit([path_train, content_train, feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6], label_train, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True, batch_size=BATCH_SIZE)\n",
    "t = time.time()-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model.save_weights(\"./crf/data/cnn-crf.h5\")\n",
    "# saving\n",
    "with open(\"./crf/data/tokenizer_path.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer_path, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"./crf/data/tokenizer_content.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer_content, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = get_model()\n",
    "model.compile(\n",
    "    loss=crf.loss,\n",
    "    optimizer=opt,\n",
    "    metrics=[crf.accuracy]\n",
    ")\n",
    "model.load_weights(\"./crf/data/cnn-crf.h5\")\n",
    "# loading\n",
    "with open('./crf/data/tokenizer_path.pickle', 'rb') as handle:\n",
    "    tokenizer_path = pickle.load(handle)\n",
    "with open('./crf/data/tokenizer_content.pickle', 'rb') as handle:\n",
    "    tokenizer_content = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(\"./data/ytest_raw.csv\")\n",
    "feature_test_1, feature_test_2, feature_test_3, feature_test_4, feature_test_5, feature_test_6, path_test, content_test, a, b = load_data_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(max_num)\n",
    "    print(feature_test.shape)\n",
    "    print(path_test.shape)\n",
    "    print(path_word_size)\n",
    "    print(con_word_size)\n",
    "path_word_size = len(tokenizer_path.index_docs)\n",
    "con_word_size = len(tokenizer_content.index_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_start = time.time()\n",
    "predictions = model.predict([path_test, content_test, feature_test_1, feature_test_2, feature_test_3, feature_test_4, feature_test_5, feature_test_6], batch_size=VAL_BATCH_SIZE)\n",
    "ts = time.time()-ts_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(feature_train[0][0])\n",
    "    print(path_train[0][0])\n",
    "    print(content_train[0][0])\n",
    "    print(label_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output & Turn predict back to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for page in range(predictions.shape[0]):\n",
    "    tmp = []\n",
    "    for node in range(max_num):\n",
    "        tmp.append(np.argmax(predictions[page][node]))\n",
    "    result.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Column Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_type = []\n",
    "with open(\"./data/TableA.txt\", \"r\") as file:\n",
    "    line = file.readline()\n",
    "    slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "    while(slot[0]!=\"ColType\"):\n",
    "        line = file.readline()\n",
    "        slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "    col_type = slot[1:]\n",
    "if DEBUG:\n",
    "    print(col_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set_data = []\n",
    "with open(\"./crf/data/predictions.csv\", \"w\") as file: # Create prediction file\n",
    "    for col in col_type: # loop to write the Col type\n",
    "        file.write(col + \"\\t\")\n",
    "        if DEBUG:\n",
    "            print(col + \"\\t\", end='')\n",
    "    file.write(\"\\n\")\n",
    "    for page in tqdm(range(predictions.shape[0])): # Loop each page\n",
    "        sets = []\n",
    "        for label in range(label_train.shape[2] + 1): # Loop whole label\n",
    "            if DEBUG:\n",
    "                print(\"Label: \" + str(label))\n",
    "            if label == 0:\n",
    "                continue\n",
    "            empty = True\n",
    "            isset = False\n",
    "            data = []\n",
    "            for node in range(predictions.shape[1]):\n",
    "                if result[page][node] == label:\n",
    "                    if empty == False and not isset:\n",
    "                        if DEBUG:\n",
    "                            print(\" \", end='')\n",
    "                        file.write(\" \")\n",
    "                    empty = False\n",
    "                    if label in col_set_dict.keys() and set_total > 0: # That col is a Set\n",
    "                        isset = True\n",
    "                        data.append(node)\n",
    "                        if DEBUG:\n",
    "                            print(\"Append:\" + str(node))\n",
    "                    else:\n",
    "                        if DEBUG:\n",
    "                            print(str(node), end='')\n",
    "                        file.write(str(node))\n",
    "            if label in col_set_dict.keys() and set_total > 0: # That col is a Set\n",
    "                if DEBUG:\n",
    "                    print(str(col_set_dict[label])+\"-\"+str(page), end='')\n",
    "                file.write(str(col_set_dict[label])+\"-\"+str(page))\n",
    "                sets.append(data)\n",
    "            if DEBUG:\n",
    "                print(\"\\t\", end='')\n",
    "            file.write(\"\\t\")\n",
    "        if DEBUG:\n",
    "            print(\"\")\n",
    "        file.write(\"\\n\")\n",
    "        if DEBUG:\n",
    "            print(data)\n",
    "        Set_data.append(sets)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(col_set_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Set data output for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    with open(\"./crf/set/Set_data.txt\", \"w\") as set_train_file:\n",
    "        tmp = str(Set_data)\n",
    "        set_train_file.write(tmp)\n",
    "        if DEBUG:\n",
    "            print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Set Train File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(feature_train_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data_count = []\n",
    "if set_total > 0:\n",
    "    for set_t in range(set_total):\n",
    "        with open(\"./data/Set-\"+ str(set_t+1) +\".txt\", \"r\") as set_file:\n",
    "            set_tmp = []\n",
    "            output_name = \"./set/Set-\"+ str(set_t+1) +\"_train_raw.csv\"\n",
    "            if DEBUG:\n",
    "                print(\"Generating:\" + output_name + \"\\n\")\n",
    "            output = open(output_name, \"w\")\n",
    "            output.write(\"Leafnode\\tPTypeSet\\tTypeSet\\tContentid\\tPathid\\tSimseqid\\tPath\\tContent\\tLabel\\n\")\n",
    "            line = set_file.readline()\n",
    "            slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            while(slot[0]!=\"ColType\"): \n",
    "                line = set_file.readline()\n",
    "                slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            with open(\"./crf/set/Set-\"+ str(set_t+1) +\"_coltype.txt\", \"w\") as col_file:\n",
    "                col_file.write(str(slot[1:]))\n",
    "            line = set_file.readline() # First line of data\n",
    "            page_num = 0\n",
    "            count = 0\n",
    "            while(line != \"\"):\n",
    "                slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                data_info = slot[0].split(\"-\")\n",
    "                if(page_num != int(data_info[1])):\n",
    "                    set_tmp.append(count)\n",
    "                    count = 0\n",
    "                set_num = int(data_info[0])\n",
    "                page_num = int(data_info[1])\n",
    "                if DEBUG:\n",
    "                    print(str(data_info[0])+\"-\"+str(data_info[1])+\"-\"+str(data_info[2]))\n",
    "                idx = 1\n",
    "                sub_list = slot[1:]\n",
    "                while(\"\" in sub_list):\n",
    "                    sub_list.remove(\"\")\n",
    "                while(\" \" in sub_list):\n",
    "                    sub_list.remove(\" \")\n",
    "                for element in sub_list:\n",
    "                    count += 1\n",
    "                    if DEBUG:\n",
    "                        print(element)\n",
    "                    element = int(element)\n",
    "                    #print(content_train[page_num][element])\n",
    "                    output.write(str(feature_train_1[page_num][element])+\"\\t\")\n",
    "                    output.write(str(feature_train_2[page_num][element])+\"\\t\")\n",
    "                    output.write(str(feature_train_3[page_num][element])+\"\\t\")\n",
    "                    output.write(str(feature_train_4[page_num][element])+\"\\t\")\n",
    "                    output.write(str(feature_train_5[page_num][element])+\"\\t\")\n",
    "                    output.write(str(feature_train_6[page_num][element])+\"\\t\")\n",
    "                    output.write(str(list(path_train[page_num][element])))\n",
    "                    output.write(\"\\t\")\n",
    "                    output.write(str(list(content_train[page_num][element])))\n",
    "                    output.write(\"\\t\")\n",
    "                    output.write(str(idx) + \"\\n\")\n",
    "                    if DEBUG:\n",
    "                        print(feature_train_1[page_num][element])\n",
    "                    idx += 1\n",
    "                line = set_file.readline()\n",
    "            set_tmp.append(count)\n",
    "            output.close()\n",
    "        set_data_count.append(set_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    with open(\"./crf/set/set_train_count.txt\", \"w\") as file:\n",
    "        file.write(str(set_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Set Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data_count = []\n",
    "if set_total > 0:\n",
    "    for set_t in range(set_total):\n",
    "        set_tmp = []\n",
    "        with open(\"./crf/set/Set-\"+ str(set_t+1) +\"_ytest_raw.csv\", \"w\") as set_file:\n",
    "            set_file.write(\"Leafnode\\tPTypeSet\\tTypeSet\\tContentid\\tPathid\\tSimseqid\\tPath\\tContent\\tLabel\\n\")\n",
    "            co = 0\n",
    "            for pages in tqdm(range(len(Set_data))):\n",
    "                count = 0\n",
    "                for node in Set_data[pages][set_t]:\n",
    "                    co += 1\n",
    "                    count += 1\n",
    "                    set_file.write(str(feature_train_1[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(feature_train_2[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(feature_train_3[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(feature_train_4[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(feature_train_5[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(feature_train_6[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(list(path_train[pages][node])))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(list(content_train[pages][node])))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(0) + \"\\n\")\n",
    "                set_tmp.append(count)\n",
    "            if DEBUG:\n",
    "                print(co)\n",
    "        set_data_count.append(set_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(set_data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    with open(\"./crf/set/set_test_count.txt\", \"w\") as file:\n",
    "        file.write(str(set_data_count))\n",
    "    with open(\"./crf/set/word_size.txt\", \"w\") as file:\n",
    "        file.write(str(path_word_size)+\"\\n\")\n",
    "        file.write(str(con_word_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_c = len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_acc = model.evaluate([path_train, content_train, feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6], label_train, batch_size=BATCH_SIZE)\n",
    "print(\"\\n\\nLoss {}, Acc {}\".format(model_loss, model_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_max_len = 30    # padding length\n",
    "path_emb_size = 10    # embedding size\n",
    "\n",
    "con_max_len = 50    # padding length\n",
    "con_emb_size = 10    # embedding size\n",
    "\n",
    "feature_emb_size = 5\n",
    "\n",
    "EPOCHS = 10000        # Train epochs\n",
    "conv_num = 20        # First cnn filter num\n",
    "UNTIL_LOSS = 0.01    # When achieve loss then stop\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001) # Set learning rate\n",
    "NO_IMPROVE = 50     # Stop when no improve for epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(path):\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    return df\n",
    "\n",
    "def max_num_set(set_data_count, set_total):\n",
    "    max_set = []\n",
    "    for i in range(set_total):\n",
    "        max_set.append(0)\n",
    "    for sets in range(len(set_data_count)):\n",
    "        max_set[sets] = max(set_data_count[sets])\n",
    "    return max_set\n",
    "\n",
    "def feature_padding(df, set_count, set_num):\n",
    "    feature = []\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        t = []\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            t.append(df[count])\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                t.append(9999)\n",
    "        feature.append(t)\n",
    "    return feature\n",
    "\n",
    "def emb_padding(df, set_count, set_num, pad_len):\n",
    "    emb = []\n",
    "    tmp = []\n",
    "    for i in range(pad_len):\n",
    "        tmp.append(0)\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            emb.append(eval(df[count]))\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                emb.append(tmp)\n",
    "    return emb\n",
    "\n",
    "def one_of_n(ans, total):\n",
    "    tmp = []\n",
    "    for i in range(int(total)):\n",
    "        if ans == i:\n",
    "            tmp.append(1.0)\n",
    "        else:\n",
    "            tmp.append(0.0)\n",
    "    return tmp\n",
    "\n",
    "def label_padding(df, set_count, set_num):\n",
    "    label = []\n",
    "    tmp = one_of_n(0, max_label+1)\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            label.append(one_of_n(df[count], max_label+1))\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                label.append(tmp)\n",
    "    return label\n",
    "\n",
    "def to_train_array(df, set_count, set_num):\n",
    "    feature_1 = np.array(feature_padding(df['Leafnode'], set_count, set_num))\n",
    "    feature_2 = np.array(feature_padding(df['PTypeSet'], set_count, set_num))\n",
    "    feature_3 = np.array(feature_padding(df['TypeSet'], set_count, set_num))\n",
    "    feature_4 = np.array(feature_padding(df['Contentid'], set_count, set_num))\n",
    "    feature_5 = np.array(feature_padding(df['Pathid'], set_count, set_num))\n",
    "    feature_6 = np.array(feature_padding(df['Simseqid'], set_count, set_num))\n",
    "    \n",
    "    path = np.array(emb_padding(df['Path'], set_count, set_num, path_max_len))\n",
    "    path = np.reshape(path, [len(set_count[set_num-1]), max_set[set_num-1], path_max_len])\n",
    "    content = np.array(emb_padding(df['Content'], set_count, set_num, con_max_len))\n",
    "    content = np.reshape(content, [len(set_count[set_num-1]), max_set[set_num-1], con_max_len])\n",
    "    \n",
    "    label = np.array(label_padding(df['Label'], set_count, set_num))\n",
    "    label = np.reshape(label, [len(set_count[set_num-1]), max_set[set_num-1], int(max_label+1)])\n",
    "    return feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, path, content, label\n",
    "\n",
    "class LossHistory(tf.keras.callbacks.Callback): # Draw the figure of train\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('accuracy'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_accuracy'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('accuracy'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_accuracy'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        f1 = plt.figure(1)\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc')\n",
    "\n",
    "        f2 = plt.figure(2)\n",
    "        plt.plot(iters, self.losses[loss_type], 'r', label='train loss')\n",
    "        plt.plot(iters, self.val_loss[loss_type], 'b', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('loss')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    Set_data = []\n",
    "    set_train_count = []\n",
    "    set_test_count = []\n",
    "    with open(\"./crf/set/Set_data.txt\", \"r\") as set_file:\n",
    "        Set_data = eval(set_file.readline())\n",
    "    with open(\"./crf/set/set_train_count.txt\", \"r\") as set_file:\n",
    "        set_train_count = eval(set_file.readline())\n",
    "    with open(\"./crf/set/set_test_count.txt\", \"r\") as set_file:\n",
    "        set_test_count = eval(set_file.readline())\n",
    "    with open(\"./crf/set/word_size.txt\", \"r\") as file:\n",
    "        path_word_size = eval(file.readline())\n",
    "        con_word_size = eval(file.readline())\n",
    "    max_num_train = max_num_set(set_train_count, set_total)\n",
    "    max_num_test = max_num_set(set_test_count, set_total)\n",
    "    max_set = []\n",
    "    for i in range(len(max_num_train)):\n",
    "        max_set.append(max(max_num_train[i], max_num_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    for num in range(set_total):\n",
    "        set_num = num + 1\n",
    "        df = get_df(\"./set/Set-\"+str(set_num)+\"_train_raw.csv\")\n",
    "        max_num = max_set[set_num-1]\n",
    "        max_label = max(df['Label'])\n",
    "        feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6, path_train, content_train, label_train = to_train_array(df, set_train_count, set_num)\n",
    "        crf = CRF(False)\n",
    "        BATCH_SIZE = max_num      # Training bath size\n",
    "        VAL_BATCH_SIZE = max_num  # Validation batch size\n",
    "        \n",
    "        def get_model():\n",
    "            path_input = tf.keras.Input(shape=(max_num, path_max_len), name='Path_emb_input')\n",
    "            content_input = tf.keras.Input(shape=(max_num, con_max_len), name='Content_emb_input')\n",
    "            feature_input_1 = tf.keras.Input(shape=(max_num,), name='Feature_input1')\n",
    "            feature_input_2 = tf.keras.Input(shape=(max_num,), name='Feature_input2')\n",
    "            feature_input_3 = tf.keras.Input(shape=(max_num,), name='Feature_input3')\n",
    "            feature_input_4 = tf.keras.Input(shape=(max_num,), name='Feature_input4')\n",
    "            feature_input_5 = tf.keras.Input(shape=(max_num,), name='Feature_input5')\n",
    "            feature_input_6 = tf.keras.Input(shape=(max_num,), name='Feature_input6')\n",
    "\n",
    "            path_f = tf.keras.layers.Flatten()(path_input)\n",
    "            content_f = tf.keras.layers.Flatten()(content_input)\n",
    "\n",
    "            path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_f)\n",
    "            content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_f)\n",
    "            f_1_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_1)\n",
    "            f_2_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_2)\n",
    "            f_3_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_3)\n",
    "            f_4_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_4)\n",
    "            f_5_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_5)\n",
    "            f_6_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_6)\n",
    "\n",
    "            path_emb = tf.reshape(path_emb, [-1, max_num, path_max_len*path_emb_size])\n",
    "            content_emb = tf.reshape(content_emb, [-1, max_num, con_max_len*con_emb_size])\n",
    "\n",
    "            path_emb = tf.expand_dims(path_emb, -1)\n",
    "            content_emb = tf.expand_dims(content_emb, -1)\n",
    "\n",
    "            path_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3,  path_max_len*path_emb_size), strides=(1, path_max_len*path_emb_size), name='Conv_for_Path_emb', padding='same')(path_emb)\n",
    "            content_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3, con_max_len*con_emb_size), strides=(1, con_max_len*con_emb_size), name='Conv_for_Content_emb', padding='same')(content_emb)\n",
    "\n",
    "            path = tf.reshape(path_feature, [-1, conv_num])\n",
    "            content = tf.reshape(content_feature, [-1, conv_num])\n",
    "\n",
    "            f_1_emb = tf.reshape(f_1_emb, [-1, feature_emb_size])\n",
    "            f_2_emb = tf.reshape(f_2_emb, [-1, feature_emb_size])\n",
    "            f_3_emb = tf.reshape(f_3_emb, [-1, feature_emb_size])\n",
    "            f_4_emb = tf.reshape(f_4_emb, [-1, feature_emb_size])\n",
    "            f_5_emb = tf.reshape(f_5_emb, [-1, feature_emb_size])\n",
    "            f_6_emb = tf.reshape(f_6_emb, [-1, feature_emb_size])\n",
    "\n",
    "            combine = tf.keras.layers.concatenate([path, content, f_1_emb, f_2_emb, f_3_emb, f_4_emb, f_5_emb, f_6_emb], -1)\n",
    "            d = combine\n",
    "            d = tf.keras.layers.Dense(max_label+1)(d)\n",
    "            d = tf.reshape(d, [-1, max_num, max_label+1])\n",
    "            output = crf(d)\n",
    "            model = tf.keras.Model(inputs=[path_input, content_input, feature_input_1, feature_input_2, feature_input_3, feature_input_4, feature_input_5, feature_input_6], outputs=output)\n",
    "\n",
    "            return model\n",
    "        \n",
    "        model = get_model()\n",
    "        model.compile(\n",
    "            loss=crf.loss,\n",
    "            optimizer=opt,\n",
    "            metrics=[crf.accuracy]\n",
    "        )\n",
    "        history = LossHistory()\n",
    "        stop_when_no_improve = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=0, patience = NO_IMPROVE, restore_best_weights=True)\n",
    "        until_loss = EarlyStoppingByLossVal(monitor='loss', value=UNTIL_LOSS, verbose=1)\n",
    "        callbacks = [history, stop_when_no_improve, until_loss]\n",
    "        \n",
    "        start = time.time()\n",
    "        model.fit([path_train, content_train, feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6], label_train, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True, batch_size=BATCH_SIZE)\n",
    "        t += time.time()-start\n",
    "        \n",
    "        model_loss, model_acc = model.evaluate([path_train, content_train, feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6], label_train, batch_size=BATCH_SIZE)\n",
    "        print(\"\\n\\nLoss {}, Acc {}\".format(model_loss, model_acc))\n",
    "        model.save_weights(\"./crf/set/set-\"+str(set_num)+\"_cnn-crf.h5\")\n",
    "        del model\n",
    "        \n",
    "        model = get_model()\n",
    "        model.compile(\n",
    "            loss=crf.loss,\n",
    "            optimizer=opt,\n",
    "            metrics=[crf.accuracy]\n",
    "        )\n",
    "        model.load_weights(\"./crf/set/set-\"+str(set_num)+\"_cnn-crf.h5\")\n",
    "        \n",
    "        df = get_df(\"./crf/set/Set-\"+str(set_num)+\"_ytest_raw.csv\")\n",
    "        feature_test_1, feature_test_2, feature_test_3, feature_test_4, feature_test_5, feature_test_6, path_test, content_test, label_test = to_train_array(df, set_test_count, set_num)\n",
    "        \n",
    "        with open(\"./crf/set/word_size.txt\", \"r\") as file:\n",
    "            path_word_size = eval(file.readline())\n",
    "            con_word_size = eval(file.readline())\n",
    "        ts_start = time.time()\n",
    "        predictions = model.predict([path_test, content_test, feature_test_1, feature_test_2, feature_test_3, feature_test_4, feature_test_5, feature_test_6], batch_size=VAL_BATCH_SIZE)\n",
    "        ts += time.time()-ts_start\n",
    "        \n",
    "        result = []\n",
    "        for page in range(predictions.shape[0]):\n",
    "            tmp = []\n",
    "            for node in range(max_num):\n",
    "                tmp.append(np.argmax(predictions[page][node]))\n",
    "            result.append(tmp)\n",
    "            \n",
    "        col_type = []\n",
    "        with open(\"./crf/set/Set-\"+str(set_num)+\"_coltype.txt\", \"r\") as file:\n",
    "            tmp = file.readline()\n",
    "            slot = eval(tmp)\n",
    "            col_type = slot\n",
    "        Set = []\n",
    "        if DEBUG:\n",
    "            with open(\"./crf/set/set-\"+str(set_num)+\".csv\", \"w\") as file: # Create prediction file\n",
    "                for col in col_type: # loop to write the Col type\n",
    "                    file.write(col + \"\\t\")\n",
    "                    print(col + \"\\t\", end='')\n",
    "                print(\"\")\n",
    "                file.write(\"\\n\")\n",
    "                current_pos = 1\n",
    "                for page in tqdm(range(len(result))): # Loop each page\n",
    "                    p_tmp = []\n",
    "                    for cols in range(max_label+1):\n",
    "                        c_tmp = []\n",
    "                        for node in range(len(result[page])):\n",
    "                            r = result[page][node]\n",
    "                            if r == cols:\n",
    "                                c_tmp.append(node)\n",
    "                        p_tmp.append(c_tmp)\n",
    "                    Set.append(p_tmp)\n",
    "                Set_tmp = Set.copy()\n",
    "                for page in range(len(Set_tmp)):\n",
    "                    empty = False\n",
    "                    col = []\n",
    "                    for i in range(len(Set_tmp[page])):\n",
    "                        col.append(False)\n",
    "                    col[0] = True\n",
    "                    while(not empty):\n",
    "                        for cols in range(len(Set_tmp[page])):\n",
    "                            if len(Set_tmp[page][cols]) == 0:\n",
    "                                col[cols] = True\n",
    "                                if cols != 0:\n",
    "                                    print(\"\\t\", end=\"\")\n",
    "                                    file.write(\"\\t\")\n",
    "                            else:\n",
    "                                n = str(int(feature_test_1[page][Set_tmp[page][cols][0]]))\n",
    "                                if cols != 0:\n",
    "                                    print(n+\"\\t\", end=\"\")\n",
    "                                    file.write(n+\"\\t\")\n",
    "                                del Set_tmp[page][cols][0]\n",
    "                                if len(Set_tmp[page][cols]) == 0:\n",
    "                                    col[cols] = True\n",
    "                            empty = True\n",
    "                            for i in col:\n",
    "                                if i == False:\n",
    "                                    empty = False\n",
    "                                    break\n",
    "                        print(\"\\n\", end=\"\")\n",
    "                        file.write(\"\\n\")\n",
    "        else:\n",
    "            with open(\"./crf/set/set-\"+str(set_num)+\".csv\", \"w\") as file: # Create prediction file\n",
    "                for col in col_type: # loop to write the Col type\n",
    "                    file.write(col + \"\\t\")\n",
    "                file.write(\"\\n\")\n",
    "                current_pos = 1\n",
    "                for page in tqdm(range(len(result))): # Loop each page\n",
    "                    p_tmp = []\n",
    "                    for cols in range(max_label+1):\n",
    "                        c_tmp = []\n",
    "                        for node in range(len(result[page])):\n",
    "                            r = result[page][node]\n",
    "                            if r == cols:\n",
    "                                c_tmp.append(node)\n",
    "                        p_tmp.append(c_tmp)\n",
    "                    Set.append(p_tmp)\n",
    "                Set_tmp = Set.copy()\n",
    "                for page in range(len(Set_tmp)):\n",
    "                    empty = False\n",
    "                    col = []\n",
    "                    for i in range(len(Set_tmp[page])):\n",
    "                        col.append(False)\n",
    "                    col[0] = True\n",
    "                    while(not empty):\n",
    "                        for cols in range(len(Set_tmp[page])):\n",
    "                            if len(Set_tmp[page][cols]) == 0:\n",
    "                                col[cols] = True\n",
    "                                if cols != 0:\n",
    "                                    file.write(\"\\t\")\n",
    "                            else:\n",
    "                                n = str(int(feature_test_1[page][Set_tmp[page][cols][0]]))\n",
    "                                if cols != 0:\n",
    "                                    file.write(n+\"\\t\")\n",
    "                                del Set_tmp[page][cols][0]\n",
    "                                if len(Set_tmp[page][cols]) == 0:\n",
    "                                    col[cols] = True\n",
    "                            empty = True\n",
    "                            for i in col:\n",
    "                                if i == False:\n",
    "                                    empty = False\n",
    "                                    break\n",
    "                        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timef = open(\"./crfsuite/data/time_crf.txt\",\"w\")\n",
    "print(\"\\ntrain time:\"+str(t))\n",
    "timef.write(\"train:\"+str(t)+\"\\n\")\n",
    "print(\"test time:\"+str(ts))\n",
    "print(\"per page:\"+ str(float(ts)/page_c)+\"\\n\")\n",
    "timef.write(\"test:\"+str(ts)+\"\\n\")\n",
    "timef.write(\"per page:\"+ str(float(ts)/page_c)+\"\\n\")\n",
    "timef.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
