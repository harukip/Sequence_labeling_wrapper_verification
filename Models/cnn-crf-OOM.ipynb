{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, unicode_literals, print_function, absolute_import\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from crflayer import CRF\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn_crfsuite import metrics\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_total = 0\n",
    "OOM_Split = 1\n",
    "# How many Set\n",
    "BATCH_SIZE = 1      # Training bath size\n",
    "VAL_BATCH_SIZE = 1  # Validation batch size\n",
    "\n",
    "DEBUG = False        # Print element\n",
    "path_max_len = 30    # padding length\n",
    "path_emb_size = 5    # embedding size\n",
    "\n",
    "con_max_len = 50    # padding length\n",
    "con_emb_size = 5    # embedding size\n",
    "\n",
    "feature_emb_size = 3\n",
    "\n",
    "EPOCHS = 10000        # Train epochs\n",
    "conv_num = 5        # First cnn filter num\n",
    "#max_num = 206       # How many nodes should pad\n",
    "UNTIL_LOSS = 0.001    # When achieve loss then stop\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.004) # Set learning rate\n",
    "NO_IMPROVE = 2     # Stop when no improve for epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tokenizer to convert words to encoding for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "tokenizer_content = tf.keras.preprocessing.text.Tokenizer(num_words=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_num(data):\n",
    "    '''\n",
    "    To generate a list of numbers of nodes that each page have\n",
    "    '''\n",
    "    count = False\n",
    "    num_list = []\n",
    "    for index in range(len(data)):\n",
    "        if data[index] == 0 and count != False:\n",
    "            num_list.append(data[index-1] + 1)\n",
    "        else:\n",
    "            count = True\n",
    "    num_list.append(data[len(data) - 1] + 1)\n",
    "    count = 0\n",
    "    index_list = []\n",
    "    for i in num_list:\n",
    "        if count == 0:\n",
    "            index_list.append(i - 1)\n",
    "            count += 1\n",
    "        else:\n",
    "            index_list.append(index_list[count - 1] + i)\n",
    "            count += 1\n",
    "    return num_list, index_list\n",
    "\n",
    "\n",
    "def node_data(data, num):\n",
    "    '''\n",
    "    Padding the data with zero when that page is less than max_num leafnode\n",
    "    '''\n",
    "    output = []\n",
    "    count = 0\n",
    "    for page_num in num:\n",
    "        tmp = []\n",
    "        page = 0\n",
    "        if page_num == max_num:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            page += 1\n",
    "        else:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            for i in range(max_num - page_num):\n",
    "                tmp.append(99999)\n",
    "            page += 1\n",
    "        output.append(tmp)\n",
    "    return output\n",
    "\n",
    "def label_padding(data, num):\n",
    "    '''\n",
    "    Padding the labels with zero when that page is less than max_num leafnode\n",
    "    '''\n",
    "    output = []\n",
    "    count = 0\n",
    "    for page_num in num:\n",
    "        tmp = []\n",
    "        page = 0\n",
    "        if page_num == max_num:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            page += 1\n",
    "        else:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            for i in range(max_num - page_num):\n",
    "                tmp.append(0) # Pad label with 0\n",
    "            page += 1\n",
    "        output.append(tmp)\n",
    "    return output\n",
    "\n",
    "\n",
    "def node_emb(data, num, pad_len):\n",
    "    '''\n",
    "    Padding the embedding with empty when that page is less than max_num leafnode.\n",
    "    '''\n",
    "    output = []\n",
    "    count = 0\n",
    "    tmp2 = []\n",
    "    for j in range(pad_len):\n",
    "        tmp2.append(0.0)\n",
    "    for page_num in num:\n",
    "        tmp = []\n",
    "        page = 0\n",
    "        if page_num == max_num:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            page += 1\n",
    "        else:\n",
    "            for i in range(page_num):\n",
    "                tmp.append(data[count])\n",
    "                count += 1\n",
    "            for i in range(max_num - page_num):\n",
    "                tmp.append(tmp2)\n",
    "            page += 1\n",
    "        output.append(tmp)\n",
    "    return output\n",
    "\n",
    "def get_df(path):\n",
    "    '''\n",
    "    Read csv file and return pandas dataframe.\n",
    "    '''\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    return df\n",
    "    \n",
    "\n",
    "def load_data_csv(df):\n",
    "    '''\n",
    "    Load the pandas dataframe and convert it to numpy array for train and test.\n",
    "    '''\n",
    "    path_encoded = tokenizer_path.texts_to_sequences(df['Path'])\n",
    "    df['Content'] = df['Content'].str.replace('/|\\.|\\?|:|=|,|<|>|&|@|\\+|-|#|~|\\|', ' ')\n",
    "    df['Content'] = df['Content'].astype(str)\n",
    "    content_encoded = tokenizer_content.texts_to_sequences(df['Content'])\n",
    "    path_pad = tf.keras.preprocessing.sequence.pad_sequences(path_encoded, path_max_len, padding='post')\n",
    "    content_pad = tf.keras.preprocessing.sequence.pad_sequences(content_encoded, con_max_len, padding='post')\n",
    "    if DEBUG:\n",
    "        print(path_pad.shape)\n",
    "        print(content_pad.shape)\n",
    "    num, index = node_num(df['Leafnode'])\n",
    "    path = np.array(node_emb(path_pad, num, path_max_len))\n",
    "    content = np.array(node_emb(content_pad, num, con_max_len))\n",
    "    if DEBUG:\n",
    "        print(path.shape)\n",
    "        print(content.shape)\n",
    "    feature_1 = np.array(node_data(df['Leafnode'], num))\n",
    "    df.drop(['Leafnode'], axis=1)\n",
    "    feature_2 = np.array(node_data(df['PTypeSet'], num))\n",
    "    df.drop(['PTypeSet'], axis=1)\n",
    "    feature_3 = np.array(node_data(df['TypeSet'], num))\n",
    "    df.drop(['TypeSet'], axis=1)\n",
    "    feature_4 = np.array(node_data(df['Contentid'], num))\n",
    "    df.drop(['Contentid'], axis=1)\n",
    "    feature_5 = np.array(node_data(df['Pathid'], num))\n",
    "    df.drop(['Pathid'], axis=1)\n",
    "    feature_6 = np.array(node_data(df['Simseqid'], num))\n",
    "    df.drop(['Simseqid'], axis=1)\n",
    "    \n",
    "    label_array = np.array(label_padding(df['Label'], num))\n",
    "    m_label = df['Label'].max()\n",
    "    df.drop(['Label'], axis=1)\n",
    "    label = []\n",
    "    path_arr = []\n",
    "    content_arr = []\n",
    "    for pages in tqdm(range(len(label_array))): # Loop each page\n",
    "        page = []\n",
    "        path_page = []\n",
    "        content_page = []\n",
    "        for node in range(len(label_array[pages])): # Loop each node\n",
    "            node_label = []\n",
    "            for label_t in range(max_label + 1): # Loop each label and a additional empty label ex.1~142 0 is empty\n",
    "                if label_t == label_array[pages][node]:\n",
    "                    node_label.append(1.0)\n",
    "                else:\n",
    "                    node_label.append(0.0)\n",
    "            page.append(node_label)\n",
    "            path_page.append(path[pages][node])\n",
    "            content_page.append(content[pages][node])\n",
    "        label.append(page)\n",
    "        path_arr.append(path_page)\n",
    "        content_arr.append(content_page)\n",
    "    label = np.array(label)\n",
    "    path_arr = np.array(path_arr)\n",
    "    content_arr = np.array(content_arr)\n",
    "    path_arr = np.reshape(path_arr, [len(label_array), max_num, path_max_len])\n",
    "    content_arr = np.reshape(content_arr, [len(label_array), max_num, con_max_len])\n",
    "    label = np.reshape(label, [len(label_array), max_num, max_label+1])\n",
    "    \n",
    "    # OOM part\n",
    "    feature_1 = np.reshape(feature_1, [-1, int(max_num/OOM_Split)])\n",
    "    feature_2 = np.reshape(feature_2, [-1, int(max_num/OOM_Split)])\n",
    "    feature_3 = np.reshape(feature_3, [-1, int(max_num/OOM_Split)])\n",
    "    feature_4 = np.reshape(feature_4, [-1, int(max_num/OOM_Split)])\n",
    "    feature_5 = np.reshape(feature_5, [-1, int(max_num/OOM_Split)])\n",
    "    feature_6 = np.reshape(feature_6, [-1, int(max_num/OOM_Split)])\n",
    "    label = np.reshape(label, [-1, int(max_num/OOM_Split), max_label+1])\n",
    "    path_arr = np.reshape(path_arr, [-1, int(max_num/OOM_Split), path_max_len])\n",
    "    content_arr = np.reshape(content_arr, [-1, int(max_num/OOM_Split), con_max_len])\n",
    "    \n",
    "    return feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, path_arr, content_arr, label, m_label\n",
    "\n",
    "\n",
    "def load_data_num(path, istrain):\n",
    "    '''\n",
    "    Get the max num of leafnodes and return.\n",
    "    '''\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    num, index = node_num(df['Leafnode'])\n",
    "    if istrain:\n",
    "        max_label = df['Label'].max()\n",
    "        return max(num), max_label\n",
    "    else:\n",
    "        return max(num)\n",
    "\n",
    "\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    '''\n",
    "    Draw the figure of train.\n",
    "    '''\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('accuracy'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_accuracy'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('accuracy'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_accuracy'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        f1 = plt.figure(1)\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc')\n",
    "\n",
    "        f2 = plt.figure(2)\n",
    "        plt.plot(iters, self.losses[loss_type], 'r', label='train loss')\n",
    "        plt.plot(iters, self.val_loss[loss_type], 'b', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('loss')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train until loss Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingByLossVal(tf.keras.callbacks.Callback):\n",
    "    '''\n",
    "    Early stop when training value less than setting value.\n",
    "    '''\n",
    "    def __init__(self, monitor='loss', value=UNTIL_LOSS, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check max_num in train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the train file and test file to check max number of nodes for each page to give the number for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_train, max_label_train = load_data_num(\"./data/train_raw.csv\", True)\n",
    "max_num_test = load_data_num(\"./data/ytest_raw.csv\", False)\n",
    "max_num = max(max_num_train, max_num_test)\n",
    "if max_num%OOM_Split != 0: # Let max num can be spilt into 10.\n",
    "    max_num += OOM_Split - max_num%OOM_Split\n",
    "if DEBUG:\n",
    "    print(max_num_train)\n",
    "    print(max_num_test)\n",
    "    print(max_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Set index File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Set_idx file to check which label is a Set in the training file generated by the training file generation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_set_dict={}\n",
    "if set_total > 0:\n",
    "    Set_dict = {}\n",
    "    with open(\"./data/Set_idx.txt\", \"r\") as set_file:\n",
    "        Set_dict = eval(set_file.readline())\n",
    "    col_set_dict = dict(map(reversed, Set_dict.items()))\n",
    "    if DEBUG:\n",
    "        print(Set_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Training file and make tokenizer to fit on Path and Content to get the encoding for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 27372.01it/s]\n"
     ]
    }
   ],
   "source": [
    "max_label = max_label_train\n",
    "df = get_df(\"./data/train_raw.csv\")\n",
    "tokenizer_path.fit_on_texts(df['Path'])\n",
    "tokenizer_content.fit_on_texts(df['Content'].astype(str))\n",
    "path_word_size = len(tokenizer_path.index_docs)\n",
    "con_word_size = len(tokenizer_content.index_docs)\n",
    "feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6, path_train, content_train, label_train, out_train = load_data_csv(df)\n",
    "crf = CRF(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(feature_train_1.shape)\n",
    "    print(label_train.shape)\n",
    "    print(path_word_size)\n",
    "    print(con_word_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    '''\n",
    "    Model definition for our experiments using tensorflow keras.\n",
    "    '''\n",
    "    path_input = tf.keras.Input(shape=(int(max_num/OOM_Split), path_max_len), name='Path_emb_input')\n",
    "    content_input = tf.keras.Input(shape=(int(max_num/OOM_Split), con_max_len), name='Content_emb_input')\n",
    "    feature_input_1 = tf.keras.Input(shape=(int(max_num/OOM_Split),), name='Feature_input1')\n",
    "    feature_input_2 = tf.keras.Input(shape=(int(max_num/OOM_Split),), name='Feature_input2')\n",
    "    feature_input_3 = tf.keras.Input(shape=(int(max_num/OOM_Split),), name='Feature_input3')\n",
    "    feature_input_4 = tf.keras.Input(shape=(int(max_num/OOM_Split),), name='Feature_input4')\n",
    "    feature_input_5 = tf.keras.Input(shape=(int(max_num/OOM_Split),), name='Feature_input5')\n",
    "    feature_input_6 = tf.keras.Input(shape=(int(max_num/OOM_Split),), name='Feature_input6')\n",
    "    \n",
    "    path_f = tf.keras.layers.Flatten()(path_input)\n",
    "    content_f = tf.keras.layers.Flatten()(content_input)\n",
    "    \n",
    "    path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_f)\n",
    "    content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_f)\n",
    "    f_1_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_1)\n",
    "    f_2_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_2)\n",
    "    f_3_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_3)\n",
    "    f_4_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_4)\n",
    "    f_5_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_5)\n",
    "    f_6_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_6)\n",
    "    \n",
    "    path_emb = tf.reshape(path_emb, [-1, int(max_num/OOM_Split), path_max_len*path_emb_size])\n",
    "    content_emb = tf.reshape(content_emb, [-1, int(max_num/OOM_Split), con_max_len*con_emb_size])\n",
    "    \n",
    "    path_emb = tf.expand_dims(path_emb, -1)\n",
    "    content_emb = tf.expand_dims(content_emb, -1)\n",
    "    \n",
    "    path_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3,  path_max_len*path_emb_size), strides=(1, path_max_len*path_emb_size), name='Conv_for_Path_emb', padding='same')(path_emb)\n",
    "    content_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3, con_max_len*con_emb_size), strides=(1, con_max_len*con_emb_size), name='Conv_for_Content_emb', padding='same')(content_emb)\n",
    "    \n",
    "    path = tf.reshape(path_feature, [-1, conv_num])\n",
    "    content = tf.reshape(content_feature, [-1, conv_num])\n",
    "    \n",
    "    f_1_emb = tf.reshape(f_1_emb, [-1, feature_emb_size])\n",
    "    f_2_emb = tf.reshape(f_2_emb, [-1, feature_emb_size])\n",
    "    f_3_emb = tf.reshape(f_3_emb, [-1, feature_emb_size])\n",
    "    f_4_emb = tf.reshape(f_4_emb, [-1, feature_emb_size])\n",
    "    f_5_emb = tf.reshape(f_5_emb, [-1, feature_emb_size])\n",
    "    f_6_emb = tf.reshape(f_6_emb, [-1, feature_emb_size])\n",
    "\n",
    "    combine = tf.keras.layers.concatenate([path, content, f_1_emb, f_2_emb, f_3_emb, f_4_emb, f_5_emb, f_6_emb], -1)\n",
    "    d = combine\n",
    "    d = tf.keras.layers.Dense(max_label+1)(d)\n",
    "    d = tf.reshape(d, [-1, int(max_num/OOM_Split), max_label+1])\n",
    "    output = crf(d)\n",
    "    output = tf.reshape(output, [-1, int(max_num/OOM_Split), max_label+1])\n",
    "    model = tf.keras.Model(inputs=[path_input, content_input, feature_input_1, feature_input_2, feature_input_3, feature_input_4, feature_input_5, feature_input_6], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model with parameters (loss, optimizer and metrics), and set up the early stop callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Path_emb_input (InputLayer)     [(None, 10, 30)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Content_emb_input (InputLayer)  [(None, 10, 50)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 300)          0           Path_emb_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 500)          0           Content_emb_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 300, 5)       30          flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 500, 5)       7380        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 10, 150)]    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 10, 250)]    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 10, 150, 1)] 0           tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_1 (Tenso [(None, 10, 250, 1)] 0           tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input1 (InputLayer)     [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input2 (InputLayer)     [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input3 (InputLayer)     [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input4 (InputLayer)     [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input5 (InputLayer)     [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input6 (InputLayer)     [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv_for_Path_emb (Conv2D)      (None, 10, 1, 5)     2255        tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Conv_for_Content_emb (Conv2D)   (None, 10, 1, 5)     3755        tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 10, 3)        300000      Feature_input1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 10, 3)        300000      Feature_input2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 10, 3)        300000      Feature_input3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 10, 3)        300000      Feature_input4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 10, 3)        300000      Feature_input5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 10, 3)        300000      Feature_input6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 5)]          0           Conv_for_Path_emb[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 5)]          0           Conv_for_Content_emb[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 3)]          0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 3)]          0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 3)]          0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 3)]          0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 3)]          0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 3)]          0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 28)           0           tf_op_layer_Reshape_2[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_3[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_4[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_5[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_6[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_7[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_8[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            58          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_10 (TensorF [(None, 10, 2)]      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "crf (CRF)                       (None, 10, 2)        4           tf_op_layer_Reshape_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_11 (TensorF [(None, 10, 2)]      0           crf[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 1,813,482\n",
      "Trainable params: 1,813,482\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.compile(\n",
    "    loss=crf.loss,\n",
    "    optimizer=opt,\n",
    "    metrics=[crf.accuracy]\n",
    ")\n",
    "print(model.summary())\n",
    "history = LossHistory()\n",
    "stop_when_no_improve = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=0, patience = NO_IMPROVE, restore_best_weights=True)\n",
    "until_loss = EarlyStoppingByLossVal(monitor='loss', value=UNTIL_LOSS, verbose=1)\n",
    "callbacks = [history, stop_when_no_improve, until_loss]\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training and recording the time consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30 samples\n",
      "Epoch 1/10000\n",
      "30/30 [==============================] - 3s 117ms/sample - loss: 2.2947 - accuracy: 0.7333\n",
      "Epoch 2/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.1603 - accuracy: 1.0000\n",
      "Epoch 3/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 4/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 5/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 6/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 7/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 8/10000\n",
      "30/30 [==============================] - 0s 16ms/sample - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 9/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 10/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 11/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 12/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 13/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 14/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 15/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 16/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 17/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 18/10000\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 19/10000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 9.2973e-04 - accuracy: 1.0000Epoch 00018: early stopping THR\n",
      "30/30 [==============================] - 0s 15ms/sample - loss: 9.1120e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.fit([path_train, content_train, feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6], label_train, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True, batch_size=BATCH_SIZE)\n",
    "t = time.time()-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the training accuracy-epochs and loss-epochs graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save tokenizer data and model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#model.save_weights(\"./crf/data/cnn-crf.h5\")\n",
    "# saving\n",
    "with open(\"./crf/data/tokenizer_path.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer_path, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"./crf/data/tokenizer_content.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer_content, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "model.save_weights(\"./crf/data/model.h5\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and tokenizer back from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\"\"\"model = tf.keras.models.load_model(\"./crf/data/model.h5\", custom_objects={'CRF':CRF, 'loss':crf.loss, 'metrics':[crf.accuracy], 'optimizer':opt})\n",
    "model.summary()\"\"\"\n",
    "model = get_model()\n",
    "model.compile(\n",
    "    loss=crf.loss,\n",
    "    optimizer=opt,\n",
    "    metrics=[crf.accuracy]\n",
    ")\n",
    "model.load_weights(\"./crf/data/model.h5\")\n",
    "# loading\n",
    "with open('./crf/data/tokenizer_path.pickle', 'rb') as handle:\n",
    "    tokenizer_path = pickle.load(handle)\n",
    "with open('./crf/data/tokenizer_content.pickle', 'rb') as handle:\n",
    "    tokenizer_content = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 22869.71it/s]\n"
     ]
    }
   ],
   "source": [
    "df = get_df(\"./data/ytest_raw.csv\")\n",
    "feature_test_1, feature_test_2, feature_test_3, feature_test_4, feature_test_5, feature_test_6, path_test, content_test, a, b = load_data_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(max_num)\n",
    "    print(feature_test_1.shape)\n",
    "    print(path_test.shape)\n",
    "    print(path_word_size)\n",
    "    print(con_word_size)\n",
    "path_word_size = len(tokenizer_path.index_docs)\n",
    "con_word_size = len(tokenizer_content.index_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on test file and record the testing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_start = time.time()\n",
    "predictions = model.predict([path_test, content_test, feature_test_1, feature_test_2, feature_test_3, feature_test_4, feature_test_5, feature_test_6], batch_size=VAL_BATCH_SIZE)\n",
    "ts = time.time()-ts_start\n",
    "predictions = np.reshape(predictions, [-1, max_num, max_label+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(feature_test_1[0][0])\n",
    "    print(path_test[0][0])\n",
    "    print(content_test[0][0])\n",
    "    print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output & Turn predict back to label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick maximum argument label as prediction and save in result list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for page in range(predictions.shape[0]):\n",
    "    tmp = []\n",
    "    for node in range(max_num):\n",
    "        tmp.append(np.argmax(predictions[page][node]))\n",
    "    result.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Column Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Column type from TableA for file ColType output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_type = []\n",
    "with open(\"./data/TableA.txt\", \"r\") as file:\n",
    "    line = file.readline()\n",
    "    slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "    while(slot[0]!=\"ColType\"):\n",
    "        line = file.readline()\n",
    "        slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "    col_type = slot[1:]\n",
    "if DEBUG:\n",
    "    print(col_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File prediction output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the prediction.csv file for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 47411.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Set_data = []\n",
    "with open(\"./crf/data/predictions.csv\", \"w\") as file: # Create prediction file\n",
    "    for col in col_type: # loop to write the Col type\n",
    "        file.write(col + \"\\t\")\n",
    "        if DEBUG:\n",
    "            print(col + \"\\t\", end='')\n",
    "    file.write(\"\\n\")\n",
    "    for page in tqdm(range(predictions.shape[0])): # Loop each page\n",
    "        sets = []\n",
    "        for label in range(label_train.shape[2] + 1): # Loop whole label\n",
    "            if DEBUG:\n",
    "                print(\"Label: \" + str(label))\n",
    "            if label == 0:\n",
    "                continue\n",
    "            empty = True\n",
    "            isset = False\n",
    "            data = []\n",
    "            for node in range(predictions.shape[1]):\n",
    "                if result[page][node] == label:\n",
    "                    if empty == False and not isset:\n",
    "                        if DEBUG:\n",
    "                            print(\" \", end='')\n",
    "                        file.write(\" \")\n",
    "                    empty = False\n",
    "                    if label in col_set_dict.keys() and set_total > 0: # That col is a Set\n",
    "                        isset = True\n",
    "                        data.append(node)\n",
    "                        if DEBUG:\n",
    "                            print(\"Append:\" + str(node))\n",
    "                    else:\n",
    "                        if DEBUG:\n",
    "                            print(str(node), end='')\n",
    "                        file.write(str(node))\n",
    "            if label in col_set_dict.keys() and set_total > 0: # That col is a Set\n",
    "                if DEBUG:\n",
    "                    print(str(col_set_dict[label])+\"-\"+str(page), end='')\n",
    "                file.write(str(col_set_dict[label])+\"-\"+str(page))\n",
    "                sets.append(data)\n",
    "            if DEBUG:\n",
    "                print(\"\\t\", end='')\n",
    "            file.write(\"\\t\")\n",
    "        if DEBUG:\n",
    "            print(\"\")\n",
    "        file.write(\"\\n\")\n",
    "        if DEBUG:\n",
    "            print(data)\n",
    "        Set_data.append(sets)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 16ms/sample - loss: 6.1938e-04 - accuracy: 1.0000\n",
      "\n",
      "\n",
      "Loss 0.0008725484212239583, Acc 1.0\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_acc = model.evaluate([path_train, content_train, feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6], label_train, batch_size=BATCH_SIZE)\n",
    "print(\"\\n\\nLoss {}, Acc {}\".format(model_loss, model_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(col_set_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Set data output for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the Set data that being predicted in the Set by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    with open(\"./crf/set/Set_data.txt\", \"w\") as set_train_file:\n",
    "        tmp = str(Set_data)\n",
    "        set_train_file.write(tmp)\n",
    "        if DEBUG:\n",
    "            print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Set Train File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train file for Set Model from DCADE Set Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(feature_train_1.shape)\n",
    "\n",
    "feature_train_1 = np.reshape(feature_train_1, [-1, max_num])\n",
    "feature_train_2 = np.reshape(feature_train_2, [-1, max_num])\n",
    "feature_train_3 = np.reshape(feature_train_3, [-1, max_num])\n",
    "feature_train_4 = np.reshape(feature_train_4, [-1, max_num])\n",
    "feature_train_5 = np.reshape(feature_train_5, [-1, max_num])\n",
    "feature_train_6 = np.reshape(feature_train_6, [-1, max_num])\n",
    "label_train = np.reshape(label_train, [-1, max_num, max_label+1])\n",
    "path_train = np.reshape(path_train, [-1, max_num, path_max_len])\n",
    "content_train = np.reshape(content_train, [-1, max_num, con_max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data_count = []\n",
    "if set_total > 0:\n",
    "    for set_t in range(set_total):\n",
    "        with open(\"./data/Set-\"+ str(set_t+1) +\".txt\", \"r\") as set_file:\n",
    "            set_tmp = []\n",
    "            output_name = \"./set/Set-\"+ str(set_t+1) +\"_train_raw.csv\"\n",
    "            if DEBUG:\n",
    "                print(\"Generating:\" + output_name + \"\\n\")\n",
    "            output = open(output_name, \"w\")\n",
    "            output.write(\"Leafnode\\tPTypeSet\\tTypeSet\\tContentid\\tPathid\\tSimseqid\\tPath\\tContent\\tLabel\\n\")\n",
    "            line = set_file.readline()\n",
    "            slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            while(slot[0]!=\"ColType\"): \n",
    "                line = set_file.readline()\n",
    "                slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            with open(\"./crf/set/Set-\"+ str(set_t+1) +\"_coltype.txt\", \"w\") as col_file:\n",
    "                col_file.write(str(slot[1:]))\n",
    "            line = set_file.readline() # First line of data\n",
    "            page_num = 0\n",
    "            count = 0\n",
    "            while(line != \"\"):\n",
    "                slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                data_info = slot[0].split(\"-\")\n",
    "                if(page_num != int(data_info[1])):\n",
    "                    set_tmp.append(count)\n",
    "                    count = 0\n",
    "                set_num = int(data_info[0])\n",
    "                page_num = int(data_info[1])\n",
    "                if DEBUG:\n",
    "                    print(str(data_info[0])+\"-\"+str(data_info[1])+\"-\"+str(data_info[2]))\n",
    "                idx = 1\n",
    "                sub_list = slot[1:]\n",
    "                while(\"\" in sub_list):\n",
    "                    sub_list.remove(\"\")\n",
    "                while(\" \" in sub_list):\n",
    "                    sub_list.remove(\" \")\n",
    "                for element in sub_list:\n",
    "                    count += 1\n",
    "                    if DEBUG:\n",
    "                        print(element)\n",
    "                    element = int(element)\n",
    "                    output.write(str(feature_train_1[page_num][element])+\"\\t\")\n",
    "                    output.write(str(feature_train_2[page_num][element])+\"\\t\")\n",
    "                    output.write(str(feature_train_3[page_num][element])+\"\\t\")\n",
    "                    output.write(str(feature_train_4[page_num][element])+\"\\t\")\n",
    "                    output.write(str(feature_train_5[page_num][element])+\"\\t\")\n",
    "                    output.write(str(feature_train_6[page_num][element])+\"\\t\")\n",
    "                    output.write(str(list(path_train[page_num][element])))\n",
    "                    output.write(\"\\t\")\n",
    "                    output.write(str(list(content_train[page_num][element])))\n",
    "                    output.write(\"\\t\")\n",
    "                    output.write(str(idx) + \"\\n\")\n",
    "                    if DEBUG:\n",
    "                        print(feature_train_1[page_num][element])\n",
    "                    idx += 1\n",
    "                line = set_file.readline()\n",
    "            set_tmp.append(count)\n",
    "            output.close()\n",
    "        set_data_count.append(set_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    with open(\"./crf/set/set_train_count.txt\", \"w\") as file:\n",
    "        file.write(str(set_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Set Test file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate test file from node data being predicted in a Set by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data_count = []\n",
    "if set_total > 0:\n",
    "    for set_t in range(set_total):\n",
    "        set_tmp = []\n",
    "        with open(\"./crf/set/Set-\"+ str(set_t+1) +\"_ytest_raw.csv\", \"w\") as set_file:\n",
    "            set_file.write(\"Leafnode\\tPTypeSet\\tTypeSet\\tContentid\\tPathid\\tSimseqid\\tPath\\tContent\\tLabel\\n\")\n",
    "            co = 0\n",
    "            for pages in tqdm(range(len(Set_data))):\n",
    "                count = 0\n",
    "                for node in Set_data[pages][set_t]:\n",
    "                    co += 1\n",
    "                    count += 1\n",
    "                    set_file.write(str(feature_train_1[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(feature_train_2[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(feature_train_3[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(feature_train_4[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(feature_train_5[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(feature_train_6[pages][node]))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(list(path_train[pages][node])))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(list(content_train[pages][node])))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(0) + \"\\n\")\n",
    "                set_tmp.append(count)\n",
    "            if DEBUG:\n",
    "                print(co)\n",
    "        set_data_count.append(set_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(set_data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    with open(\"./crf/set/set_test_count.txt\", \"w\") as file:\n",
    "        file.write(str(set_data_count))\n",
    "    with open(\"./crf/set/word_size.txt\", \"w\") as file:\n",
    "        file.write(str(path_word_size)+\"\\n\")\n",
    "        file.write(str(con_word_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_c = len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_max_len = 30    # padding length\n",
    "path_emb_size = 10    # embedding size\n",
    "\n",
    "con_max_len = 50    # padding length\n",
    "con_emb_size = 10    # embedding size\n",
    "\n",
    "feature_emb_size = 5\n",
    "\n",
    "EPOCHS = 10000        # Train epochs\n",
    "conv_num = 20        # First cnn filter num\n",
    "UNTIL_LOSS = 0.01    # When achieve loss then stop\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001) # Set learning rate\n",
    "NO_IMPROVE = 50     # Stop when no improve for epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_num_set(set_data_count, set_total):\n",
    "    max_set = []\n",
    "    for i in range(set_total):\n",
    "        max_set.append(0)\n",
    "    for sets in range(len(set_data_count)):\n",
    "        max_set[sets] = max(set_data_count[sets])\n",
    "    return max_set\n",
    "\n",
    "def feature_padding_set(df, set_count, set_num):\n",
    "    feature = []\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        t = []\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            t.append(df[count])\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                t.append(9999)\n",
    "        feature.append(t)\n",
    "    return feature\n",
    "\n",
    "def emb_padding_set(df, set_count, set_num, pad_len):\n",
    "    emb = []\n",
    "    tmp = []\n",
    "    for i in range(pad_len):\n",
    "        tmp.append(0)\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            emb.append(eval(df[count]))\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                emb.append(tmp)\n",
    "    return emb\n",
    "\n",
    "def one_of_n(ans, total):\n",
    "    tmp = []\n",
    "    for i in range(int(total)):\n",
    "        if ans == i:\n",
    "            tmp.append(1.0)\n",
    "        else:\n",
    "            tmp.append(0.0)\n",
    "    return tmp\n",
    "\n",
    "def label_padding_set(df, set_count, set_num):\n",
    "    label = []\n",
    "    tmp = one_of_n(0, max_label+1)\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            label.append(one_of_n(df[count], max_label+1))\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                label.append(tmp)\n",
    "    return label\n",
    "\n",
    "def to_train_array_set(df, set_count, set_num):\n",
    "    feature_1 = np.array(feature_padding_set(df['Leafnode'], set_count, set_num))\n",
    "    feature_2 = np.array(feature_padding_set(df['PTypeSet'], set_count, set_num))\n",
    "    feature_3 = np.array(feature_padding_set(df['TypeSet'], set_count, set_num))\n",
    "    feature_4 = np.array(feature_padding_set(df['Contentid'], set_count, set_num))\n",
    "    feature_5 = np.array(feature_padding_set(df['Pathid'], set_count, set_num))\n",
    "    feature_6 = np.array(feature_padding_set(df['Simseqid'], set_count, set_num))\n",
    "    \n",
    "    path = np.array(emb_padding_set(df['Path'], set_count, set_num, path_max_len))\n",
    "    path = np.reshape(path, [len(set_count[set_num-1]), max_set[set_num-1], path_max_len])\n",
    "    content = np.array(emb_padding_set(df['Content'], set_count, set_num, con_max_len))\n",
    "    content = np.reshape(content, [len(set_count[set_num-1]), max_set[set_num-1], con_max_len])\n",
    "    \n",
    "    label = np.array(label_padding_set(df['Label'], set_count, set_num))\n",
    "    label = np.reshape(label, [len(set_count[set_num-1]), max_set[set_num-1], int(max_label+1)])\n",
    "    return feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, path, content, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    Set_data = []\n",
    "    set_train_count = []\n",
    "    set_test_count = []\n",
    "    with open(\"./crf/set/Set_data.txt\", \"r\") as set_file:\n",
    "        Set_data = eval(set_file.readline())\n",
    "    with open(\"./crf/set/set_train_count.txt\", \"r\") as set_file:\n",
    "        set_train_count = eval(set_file.readline())\n",
    "    with open(\"./crf/set/set_test_count.txt\", \"r\") as set_file:\n",
    "        set_test_count = eval(set_file.readline())\n",
    "    with open(\"./crf/set/word_size.txt\", \"r\") as file:\n",
    "        path_word_size = eval(file.readline())\n",
    "        con_word_size = eval(file.readline())\n",
    "    max_num_train = max_num_set(set_train_count, set_total)\n",
    "    max_num_test = max_num_set(set_test_count, set_total)\n",
    "    max_set = []\n",
    "    for i in range(len(max_num_train)):\n",
    "        max_set.append(max(max_num_train[i], max_num_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop all the set for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    for num in range(set_total):\n",
    "        set_num = num + 1\n",
    "        df = get_df(\"./set/Set-\"+str(set_num)+\"_train_raw.csv\")\n",
    "        max_num = max_set[set_num-1]\n",
    "        max_label = max(df['Label'])\n",
    "        feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6, path_train, content_train, label_train = to_train_array_set(df, set_train_count, set_num)\n",
    "        crf = CRF(False)\n",
    "        BATCH_SIZE = max_num      # Training bath size\n",
    "        VAL_BATCH_SIZE = max_num  # Validation batch size\n",
    "        \n",
    "        def get_model():\n",
    "            path_input = tf.keras.Input(shape=(max_num, path_max_len), name='Path_emb_input')\n",
    "            content_input = tf.keras.Input(shape=(max_num, con_max_len), name='Content_emb_input')\n",
    "            feature_input_1 = tf.keras.Input(shape=(max_num,), name='Feature_input1')\n",
    "            feature_input_2 = tf.keras.Input(shape=(max_num,), name='Feature_input2')\n",
    "            feature_input_3 = tf.keras.Input(shape=(max_num,), name='Feature_input3')\n",
    "            feature_input_4 = tf.keras.Input(shape=(max_num,), name='Feature_input4')\n",
    "            feature_input_5 = tf.keras.Input(shape=(max_num,), name='Feature_input5')\n",
    "            feature_input_6 = tf.keras.Input(shape=(max_num,), name='Feature_input6')\n",
    "\n",
    "            path_f = tf.keras.layers.Flatten()(path_input)\n",
    "            content_f = tf.keras.layers.Flatten()(content_input)\n",
    "\n",
    "            path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_f)\n",
    "            content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_f)\n",
    "            f_1_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_1)\n",
    "            f_2_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_2)\n",
    "            f_3_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_3)\n",
    "            f_4_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_4)\n",
    "            f_5_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_5)\n",
    "            f_6_emb = tf.keras.layers.Embedding(100000, feature_emb_size)(feature_input_6)\n",
    "\n",
    "            path_emb = tf.reshape(path_emb, [-1, max_num, path_max_len*path_emb_size])\n",
    "            content_emb = tf.reshape(content_emb, [-1, max_num, con_max_len*con_emb_size])\n",
    "\n",
    "            path_emb = tf.expand_dims(path_emb, -1)\n",
    "            content_emb = tf.expand_dims(content_emb, -1)\n",
    "\n",
    "            path_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3,  path_max_len*path_emb_size), strides=(1, path_max_len*path_emb_size), name='Conv_for_Path_emb', padding='same')(path_emb)\n",
    "            content_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3, con_max_len*con_emb_size), strides=(1, con_max_len*con_emb_size), name='Conv_for_Content_emb', padding='same')(content_emb)\n",
    "\n",
    "            path = tf.reshape(path_feature, [-1, conv_num])\n",
    "            content = tf.reshape(content_feature, [-1, conv_num])\n",
    "\n",
    "            f_1_emb = tf.reshape(f_1_emb, [-1, feature_emb_size])\n",
    "            f_2_emb = tf.reshape(f_2_emb, [-1, feature_emb_size])\n",
    "            f_3_emb = tf.reshape(f_3_emb, [-1, feature_emb_size])\n",
    "            f_4_emb = tf.reshape(f_4_emb, [-1, feature_emb_size])\n",
    "            f_5_emb = tf.reshape(f_5_emb, [-1, feature_emb_size])\n",
    "            f_6_emb = tf.reshape(f_6_emb, [-1, feature_emb_size])\n",
    "\n",
    "            combine = tf.keras.layers.concatenate([path, content, f_1_emb, f_2_emb, f_3_emb, f_4_emb, f_5_emb, f_6_emb], -1)\n",
    "            d = combine\n",
    "            d = tf.keras.layers.Dense(max_label+1)(d)\n",
    "            d = tf.reshape(d, [-1, max_num, max_label+1])\n",
    "            output = crf(d)\n",
    "            model = tf.keras.Model(inputs=[path_input, content_input, feature_input_1, feature_input_2, feature_input_3, feature_input_4, feature_input_5, feature_input_6], outputs=output)\n",
    "\n",
    "            return model\n",
    "        \n",
    "        model = get_model()\n",
    "        model.compile(\n",
    "            loss=crf.loss,\n",
    "            optimizer=opt,\n",
    "            metrics=[crf.accuracy]\n",
    "        )\n",
    "        history = LossHistory()\n",
    "        stop_when_no_improve = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=0, patience = NO_IMPROVE, restore_best_weights=True)\n",
    "        until_loss = EarlyStoppingByLossVal(monitor='loss', value=UNTIL_LOSS, verbose=1)\n",
    "        callbacks = [history, stop_when_no_improve, until_loss]\n",
    "        \n",
    "        start = time.time()\n",
    "        model.fit([path_train, content_train, feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6], label_train, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True, batch_size=BATCH_SIZE)\n",
    "        t += time.time()-start\n",
    "        \n",
    "        model_loss, model_acc = model.evaluate([path_train, content_train, feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6], label_train, batch_size=BATCH_SIZE)\n",
    "        print(\"\\n\\nLoss {}, Acc {}\".format(model_loss, model_acc))\n",
    "        \"\"\"model.save(\"./crf/set/set-\"+str(set_num)+\"_model.h5\")\n",
    "        del model\n",
    "        \n",
    "        model = tf.keras.models.load_model(\"./crf/set/set-\"+str(set_num)+\"_model.h5\")\n",
    "        '''model.compile(\n",
    "            loss=crf.loss,\n",
    "            optimizer=opt,\n",
    "            metrics=[crf.accuracy]\n",
    "        )\n",
    "        model.load_weights(\"./crf/set/set-\"+str(set_num)+\"_cnn-crf.h5\")'''\"\"\"\n",
    "        \n",
    "        df = get_df(\"./crf/set/Set-\"+str(set_num)+\"_ytest_raw.csv\")\n",
    "        feature_test_1, feature_test_2, feature_test_3, feature_test_4, feature_test_5, feature_test_6, path_test, content_test, label_test = to_train_array_set(df, set_test_count, set_num)\n",
    "        \n",
    "        with open(\"./crf/set/word_size.txt\", \"r\") as file:\n",
    "            path_word_size = eval(file.readline())\n",
    "            con_word_size = eval(file.readline())\n",
    "        ts_start = time.time()\n",
    "        predictions = model.predict([path_test, content_test, feature_test_1, feature_test_2, feature_test_3, feature_test_4, feature_test_5, feature_test_6], batch_size=VAL_BATCH_SIZE)\n",
    "        ts += time.time()-ts_start\n",
    "        \n",
    "        result = []\n",
    "        for page in range(predictions.shape[0]):\n",
    "            tmp = []\n",
    "            for node in range(max_num):\n",
    "                tmp.append(np.argmax(predictions[page][node]))\n",
    "            result.append(tmp)\n",
    "            \n",
    "        col_type = []\n",
    "        with open(\"./crf/set/Set-\"+str(set_num)+\"_coltype.txt\", \"r\") as file:\n",
    "            tmp = file.readline()\n",
    "            slot = eval(tmp)\n",
    "            col_type = slot\n",
    "        Set = []\n",
    "        with open(\"./crf/set/set-\"+str(set_num)+\".csv\", \"w\") as file: # Create prediction file\n",
    "            for col in col_type: # loop to write the Col type\n",
    "                file.write(col + \"\\t\")\n",
    "                if DEBUG:\n",
    "                    print(col + \"\\t\", end='')\n",
    "            if DEBUG:\n",
    "                print(\"\")\n",
    "            file.write(\"\\n\")\n",
    "            current_pos = 1\n",
    "            for page in tqdm(range(len(result))): # Loop each page\n",
    "                p_tmp = []\n",
    "                for cols in range(max_label+1):\n",
    "                    c_tmp = []\n",
    "                    for node in range(len(result[page])):\n",
    "                        r = result[page][node]\n",
    "                        if r == cols:\n",
    "                            c_tmp.append(node)\n",
    "                    p_tmp.append(c_tmp)\n",
    "                Set.append(p_tmp)\n",
    "            Set_tmp = Set.copy()\n",
    "            for page in range(len(Set_tmp)):\n",
    "                empty = False\n",
    "                col = []\n",
    "                for i in range(len(Set_tmp[page])):\n",
    "                    col.append(False)\n",
    "                col[0] = True\n",
    "                while(not empty):\n",
    "                    for cols in range(len(Set_tmp[page])):\n",
    "                        if len(Set_tmp[page][cols]) == 0:\n",
    "                            col[cols] = True\n",
    "                            if cols != 0:\n",
    "                                if DEBUG:\n",
    "                                    print(\"\\t\", end=\"\")\n",
    "                                file.write(\"\\t\")\n",
    "                        else:\n",
    "                            n = str(int(feature_test_1[page][Set_tmp[page][cols][0]]))\n",
    "                            if cols != 0:\n",
    "                                if DEBUG:\n",
    "                                    print(n+\"\\t\", end=\"\")\n",
    "                                file.write(n+\"\\t\")\n",
    "                            del Set_tmp[page][cols][0]\n",
    "                            if len(Set_tmp[page][cols]) == 0:\n",
    "                                col[cols] = True\n",
    "                        empty = True\n",
    "                        for i in col:\n",
    "                            if i == False:\n",
    "                                empty = False\n",
    "                                break\n",
    "                    if DEBUG:\n",
    "                        print(\"\\n\", end=\"\")\n",
    "                    file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train time:11.723287582397461\n",
      "test time:0.25821900367736816\n",
      "per page:0.008607300122578938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timef = open(\"./crf/data/time_crf.txt\",\"w\")\n",
    "print(\"\\ntrain time:\"+str(t))\n",
    "timef.write(\"train:\"+str(t)+\"\\n\")\n",
    "print(\"test time:\"+str(ts))\n",
    "print(\"per page:\"+ str(float(ts)/page_c)+\"\\n\")\n",
    "timef.write(\"test:\"+str(ts)+\"\\n\")\n",
    "timef.write(\"per page:\"+ str(float(ts)/page_c)+\"\\n\")\n",
    "timef.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
