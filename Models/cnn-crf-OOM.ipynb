{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, unicode_literals, print_function, absolute_import\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from crflayer import CRF\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import func\n",
    "import prepare_train_with_set as prepare\n",
    "import set_func\n",
    "import glob\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1      # Training bath size\n",
    "VAL_BATCH_SIZE = 1  # Validation batch size\n",
    "\n",
    "DEBUG = False        # Print element\n",
    "path_max_len = 30    # padding length\n",
    "path_emb_size = 5    # embedding size\n",
    "\n",
    "con_max_len = 50    # padding length\n",
    "con_emb_size = 5    # embedding size\n",
    "\n",
    "feature_emb_size = 3\n",
    "\n",
    "EPOCHS = 10000        # Train epochs\n",
    "conv_num = 5        # First cnn filter num\n",
    "UNTIL_LOSS = 0.001    # When achieve loss then stop\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.004) # Set learning rate\n",
    "NO_IMPROVE = 3     # Stop when no improve for epochs\n",
    "current_path = os.path.join(os.path.expanduser(\"~\"), \"jupyter\", \"Sequence_Labeling_Wrapper_Verification\", \"data\")\n",
    "data_path = os.path.join(current_path, \"data\")\n",
    "set_total = len(glob.glob(os.path.join(data_path, \"Set-*\")))\n",
    "print(\"Set:\", set_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_limit(num):\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only use the first GPU\n",
    "      try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[num], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "      except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tokenizer to convert words to encoding for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer():\n",
    "    tokenizer_path = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "    tokenizer_content = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "    return tokenizer_path, tokenizer_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train until loss Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingByLossVal(tf.keras.callbacks.Callback):\n",
    "    '''\n",
    "    Early stop when training value less than setting value.\n",
    "    '''\n",
    "    def __init__(self, monitor='loss', value=UNTIL_LOSS, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_model(crf, max_num, max_label, OOM_Split):\n",
    "    '''\n",
    "    Model definition for our experiments using tensorflow keras.\n",
    "    '''\n",
    "    feature_input = tf.keras.Input(shape=(int(max_num/OOM_Split), 6), name='Feature_input')\n",
    "    path_input = tf.keras.Input(shape=(int(max_num/OOM_Split), path_max_len), name='Path_emb_input')\n",
    "    content_input = tf.keras.Input(shape=(int(max_num/OOM_Split), con_max_len), name='Content_emb_input')\n",
    "    \n",
    "    path_f = tf.keras.layers.Flatten()(path_input)\n",
    "    content_f = tf.keras.layers.Flatten()(content_input)\n",
    "    \n",
    "    path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_f)\n",
    "    content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_f)\n",
    "    \n",
    "    path_emb = tf.reshape(path_emb, [-1, int(max_num/OOM_Split), path_max_len*path_emb_size])\n",
    "    content_emb = tf.reshape(content_emb, [-1, int(max_num/OOM_Split), con_max_len*con_emb_size])\n",
    "    \n",
    "    path_emb = tf.expand_dims(path_emb, -1)\n",
    "    content_emb = tf.expand_dims(content_emb, -1)\n",
    "    \n",
    "    path_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3,  path_max_len*path_emb_size), strides=(1, path_max_len*path_emb_size), name='Conv_for_Path_emb', padding='same')(path_emb)\n",
    "    content_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3, con_max_len*con_emb_size), strides=(1, con_max_len*con_emb_size), name='Conv_for_Content_emb', padding='same')(content_emb)\n",
    "    \n",
    "    path = tf.reshape(path_feature, [-1, conv_num])\n",
    "    content = tf.reshape(content_feature, [-1, conv_num])\n",
    "    \n",
    "    features = tf.reshape(feature_input, [-1, 6])\n",
    "\n",
    "    combine = tf.keras.layers.concatenate([path, content, features], -1)\n",
    "    d = combine\n",
    "    d = tf.keras.layers.Dense(max_label+1)(d)\n",
    "    d = tf.reshape(d, [-1, int(max_num/OOM_Split), max_label+1])\n",
    "    output = crf(d)\n",
    "    output = tf.reshape(output, [-1, int(max_num/OOM_Split), max_label+1])\n",
    "    model = tf.keras.Model(inputs=[feature_input, path_input, content_input], outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_word_only(crf, max_num, max_label, OOM_Split):\n",
    "    '''\n",
    "    Model definition for our experiments using tensorflow keras.\n",
    "    '''\n",
    "    feature_input = tf.keras.Input(shape=(int(max_num/OOM_Split), 6), name='Feature_input')\n",
    "    path_input = tf.keras.Input(shape=(int(max_num/OOM_Split), path_max_len), name='Path_emb_input')\n",
    "    content_input = tf.keras.Input(shape=(int(max_num/OOM_Split), con_max_len), name='Content_emb_input')\n",
    "    \n",
    "    path_f = tf.keras.layers.Flatten()(path_input)\n",
    "    content_f = tf.keras.layers.Flatten()(content_input)\n",
    "    \n",
    "    path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_f)\n",
    "    content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_f)\n",
    "    \n",
    "    path_emb = tf.reshape(path_emb, [-1, int(max_num/OOM_Split), path_max_len*path_emb_size])\n",
    "    content_emb = tf.reshape(content_emb, [-1, int(max_num/OOM_Split), con_max_len*con_emb_size])\n",
    "    \n",
    "    path_emb = tf.expand_dims(path_emb, -1)\n",
    "    content_emb = tf.expand_dims(content_emb, -1)\n",
    "    \n",
    "    path_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3,  path_max_len*path_emb_size), strides=(1, path_max_len*path_emb_size), name='Conv_for_Path_emb', padding='same')(path_emb)\n",
    "    content_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3, con_max_len*con_emb_size), strides=(1, con_max_len*con_emb_size), name='Conv_for_Content_emb', padding='same')(content_emb)\n",
    "    \n",
    "    path = tf.reshape(path_feature, [-1, conv_num])\n",
    "    content = tf.reshape(content_feature, [-1, conv_num])\n",
    "\n",
    "    combine = tf.keras.layers.concatenate([path, content], -1)\n",
    "    d = combine\n",
    "    d = tf.keras.layers.Dense(max_label+1)(d)\n",
    "    d = tf.reshape(d, [-1, int(max_num/OOM_Split), max_label+1])\n",
    "    output = crf(d)\n",
    "    output = tf.reshape(output, [-1, int(max_num/OOM_Split), max_label+1])\n",
    "    model = tf.keras.Model(inputs=[feature_input, path_input, content_input], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    import pickle\n",
    "    with open(os.path.join(current_path, \"crf\", \"data\", \"model.h5\"), \"wb\") as handle:\n",
    "        pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # saving\n",
    "    with open(os.path.join(current_path, \"crf\", \"data\", \"tokenizer_path.pickle\"), \"wb\") as handle:\n",
    "        pickle.dump(tokenizer_path, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(current_path, \"crf\", \"data\", \"tokenizer_content.pickle\"), \"wb\") as handle:\n",
    "        pickle.dump(tokenizer_content, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    import pickle\n",
    "    with open(os.path.join(current_path, \"crf\", \"data\", \"model.h5\"), 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    model.summary()\n",
    "    # loading\n",
    "    with open(os.path.join(current_path, \"crf\", \"data\", \"tokenizer_path.pickle\"), 'rb') as handle:\n",
    "        tokenizer_path = pickle.load(handle)\n",
    "    with open(os.path.join(current_path, \"crf\", \"data\", \"tokenizer_content.pickle\"), 'rb') as handle:\n",
    "        tokenizer_content = pickle.load(handle)        \n",
    "    path_word_size = len(tokenizer_path.index_docs)\n",
    "    con_word_size = len(tokenizer_content.index_docs)\n",
    "    return model, tokenizer_path, tokenizer_content, path_word_size, con_word_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(predictions, max_num):\n",
    "    result = []\n",
    "    count = 0\n",
    "    for page in range(int(len(predictions)/max_num)):\n",
    "        tmp = []\n",
    "        for node in range(max_num):\n",
    "            tmp.append(np.argmax(predictions[count]))\n",
    "            count += 1\n",
    "        result.append(tmp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_process_data(df, max_num, tokenizer_path, tokenizer_content, path_max_len, con_max_len, OOM_Split, one_hot, isTrain):\n",
    "    '''\n",
    "    Load the csv file and convert it to np array.\n",
    "    '''\n",
    "    num, index = func.node_num(df['Leafnode'])\n",
    "    _, max_label = func.load_data_num(df, True)\n",
    "    num_cols = ['Leafnode', 'PTypeSet', 'TypeSet', 'Contentid', 'Pathid', 'Simseqid']\n",
    "    features = []\n",
    "    word_features = []\n",
    "    tokenizer_path.fit_on_texts(df['Path'])\n",
    "    tokenizer_content.fit_on_texts(df['Content'])\n",
    "    path_encoded = tokenizer_path.texts_to_sequences(df['Path'])\n",
    "    df['Content'] = df['Content'].str.replace('/|\\.|\\?|:|=|,|<|>|&|@|\\+|-|#|~|\\|', ' ')\n",
    "    df['Content'] = df['Content'].astype(str)\n",
    "    content_encoded = tokenizer_content.texts_to_sequences(df['Content'])\n",
    "    path_pad = tf.keras.preprocessing.sequence.pad_sequences(path_encoded, path_max_len, padding='post')\n",
    "    content_pad = tf.keras.preprocessing.sequence.pad_sequences(content_encoded, con_max_len, padding='post')\n",
    "    \n",
    "    word_cols = [path_pad, content_pad]\n",
    "    word_max_len = [path_max_len, con_max_len]\n",
    "    \n",
    "    for c in range(len(num_cols)):\n",
    "        features.append(np.array(func.node_data(df[num_cols[c]], num, max_num)).astype('int32'))\n",
    "        features[c] = np.expand_dims(features[c], -1)\n",
    "    \n",
    "    for c in range(len(word_cols)):\n",
    "        word_features.append(np.array(func.node_emb(word_cols[c], num, word_max_len[c], max_num)).astype('int32'))\n",
    "    label_array = np.array(func.label_padding(df['Label'], num, max_num)).astype('int32')\n",
    "    feature = np.concatenate([feature for feature in features], -1)\n",
    "    \n",
    "    # OOM\n",
    "    feature = np.reshape(feature, [-1, int(max_num/OOM_Split), 6])\n",
    "    \n",
    "    word = [np.reshape(word_features[c], [-1, int(max_num/OOM_Split), word_max_len[c]]) for c in range(len(word_cols))]\n",
    "    \n",
    "    feature = feature.astype('float32')\n",
    "    label = []\n",
    "    for page in range(label_array.shape[0]):\n",
    "        for node in range(label_array.shape[1]):\n",
    "            label.append(func.one_of_n(label_array[page][node], max_label+1))\n",
    "    y_onehot = np.reshape(np.array(label), [-1, int(max_num/OOM_Split), max_label+1])\n",
    "    return feature, word, y_onehot, max_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_padding_set(df, set_count, max_num, set_num, pad_len):\n",
    "    emb = []\n",
    "    tmp = []\n",
    "    for i in range(pad_len):\n",
    "        tmp.append(0)\n",
    "    count = 0\n",
    "    for pages in set_count[set_num]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            emb.append(df[count])\n",
    "            count += 1\n",
    "        if set_len != max_num:\n",
    "            for i in range(max_num-set_len):\n",
    "                emb.append(tmp)\n",
    "    if DEBUG:\n",
    "        print(count)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_process_set(df, set_count, set_num, max_set, path_max_len, con_max_len, OOM_Split, one_hot, isTrain):\n",
    "    max_num = max_set[set_num]\n",
    "    if max_num%OOM_Split != 0: # Let max num can be spilt into 10.\n",
    "        max_num += OOM_Split - max_num%OOM_Split\n",
    "    cols = ['Leafnode', 'PTypeSet', 'TypeSet', 'Contentid', 'Pathid', 'Simseqid']\n",
    "    features = []\n",
    "    word_features = []\n",
    "    for c in range(len(cols)):\n",
    "        features.append(np.array(set_func.feature_padding_set(df[cols[c]], set_count, set_num, max_num)).astype('int32'))\n",
    "        features[c] = np.expand_dims(features[c], -1)\n",
    "    \n",
    "    word_cols = [\"Path\", \"Content\"]\n",
    "    word_max_len = [path_max_len, con_max_len]\n",
    "    for c in range(len(word_cols)):\n",
    "        word_features.append(np.array(emb_padding_set(df[word_cols[c]], set_count, max_num, set_num, word_max_len[c])).astype('int32'))\n",
    "    \n",
    "    features = np.concatenate([feature for feature in features], -1)\n",
    "    features = np.reshape(features, [-1, int(max_num/OOM_Split), 6])\n",
    "    \n",
    "    word = [np.reshape(word_features[c], [-1, int(max_num/OOM_Split), word_max_len[c]]) for c in range(len(word_cols))]\n",
    "    \n",
    "    features = features.astype('float32')\n",
    "    label_array = np.array(set_func.label_padding_set(df['Label'], set_count, set_num, max_num)).astype('int32')\n",
    "    label = []\n",
    "    for page in range(label_array.shape[0]):\n",
    "        for node in range(label_array.shape[1]):\n",
    "            label.append(func.one_of_n(label_array[page][node], max_label+1))\n",
    "    y_onehot = np.reshape(label, [-1, int(max_num/OOM_Split), max_label+1])\n",
    "    return features, word, y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    OOM_Split = 1\n",
    "    model_name = \"crf\"\n",
    "    current_path = os.path.join(os.path.expanduser(\"~\"), \"jupyter\", \"Sequence_Labeling_Wrapper_Verification\", \"data\")\n",
    "    data_path = os.path.join(current_path, \"data\")\n",
    "    set_total = len(glob.glob(os.path.join(data_path, \"Set-*\")))\n",
    "    print(\"Set:\", set_total)\n",
    "    # GPU\n",
    "    gpu_limit(1)\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer_path, tokenizer_content = tokenizer()\n",
    "    \n",
    "    # Process training file\n",
    "    train_data, Set_dict = prepare.train_file_generate(set_total, current_path)\n",
    "    test_data = prepare.test_file_generate(current_path)\n",
    "    max_num_train, max_label_train = func.load_data_num(train_data, True)\n",
    "    max_num_test = func.load_data_num(test_data, False)\n",
    "    max_num = max(max_num_train, max_num_test)\n",
    "    \n",
    "    BATCH_SIZE = 1      # batch size\n",
    "    VAL_BATCH_SIZE = 1  # Validation batch size\n",
    "    \n",
    "    col_set_dict = dict(map(reversed, Set_dict.items()))\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            print(\"OOM:\", OOM_Split, \"process train features.\")\n",
    "            if max_num%OOM_Split != 0: # Let max num can be spilt into 10.\n",
    "                max_num += OOM_Split - max_num%OOM_Split\n",
    "\n",
    "            one_hot = OneHotEncoder()\n",
    "            X_train, word_train, y_train, _ = crf_process_data(train_data, max_num, tokenizer_path, \n",
    "                                                               tokenizer_content, path_max_len, \n",
    "                                                               con_max_len, OOM_Split, one_hot, True)\n",
    "\n",
    "            path_word_size = len(tokenizer_path.index_docs)\n",
    "            con_word_size = len(tokenizer_content.index_docs)\n",
    "            page_num = int(len(y_train)/max_num)\n",
    "\n",
    "            # Define model\n",
    "            print(\"Start training.\")\n",
    "            crf = CRF(False)\n",
    "            model = model_word_only(crf, max_num, max_label_train, OOM_Split)\n",
    "            history = func.LossHistory()\n",
    "            model.compile(\n",
    "                loss=crf.loss,\n",
    "                optimizer=opt,\n",
    "                metrics=[crf.accuracy]\n",
    "            )\n",
    "            print(model.summary())\n",
    "\n",
    "            stop_when_no_improve = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=0, patience = NO_IMPROVE, restore_best_weights=True)\n",
    "            until_loss = EarlyStoppingByLossVal(monitor='loss', value=UNTIL_LOSS, verbose=1)\n",
    "            callbacks = [history, stop_when_no_improve, until_loss]\n",
    "\n",
    "            # Start training\n",
    "            start = time.time()\n",
    "            model.fit([X_train, word_train[0], word_train[1]], y_train, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True, batch_size=BATCH_SIZE)\n",
    "            t = time.time()-start\n",
    "            break\n",
    "        except:\n",
    "            OOM_Split += 1\n",
    "    # Graph\n",
    "    #history.loss_plot('epoch')\n",
    "    \n",
    "    # Load test feature\n",
    "    print(\"process test features\")\n",
    "    X_test, word_test, y_test, _ = crf_process_data(test_data, max_num, tokenizer_path, \n",
    "                                                    tokenizer_content, path_max_len, \n",
    "                                                    con_max_len, OOM_Split, one_hot, False)\n",
    "    \n",
    "    # Start testing\n",
    "    ts_start = time.time()\n",
    "    pred = model.predict([X_test, word_test[0], word_test[1]], batch_size=VAL_BATCH_SIZE)\n",
    "    ts = time.time()-ts_start\n",
    "    \n",
    "    pred = np.reshape(pred, [-1, max_label_train+1])\n",
    "    # Process output\n",
    "    result = get_result(pred, max_num)\n",
    "    col_type = func.get_col_type(current_path)\n",
    "    Set_data = func.predict_output(set_total, current_path, model_name, col_type, result, max_label_train, col_set_dict)\n",
    "    \n",
    "    word_max_len = [path_max_len, con_max_len]\n",
    "    set_X_train = np.reshape(X_train, [-1, 6])\n",
    "    set_word_train = [np.reshape(word_train[c], [-1, word_max_len[c]]) for c in range(len(word_train))]\n",
    "    set_X_test = np.reshape(X_test, [-1, 6])\n",
    "    set_word_test = [np.reshape(word_test[c], [-1, word_max_len[c]]) for c in range(len(word_train))]\n",
    "    \n",
    "    set_train_data, set_train_count = set_func.Set_train_file_generate(set_total, current_path, model_name, \n",
    "                                                                       set_X_train, set_word_train, max_num)\n",
    "    set_test_data, set_test_count = set_func.Set_test_file_generate(set_total, current_path, model_name, \n",
    "                                                                    Set_data, set_X_test, set_word_test, max_num)\n",
    "    page_c = len(result)\n",
    "    \n",
    "    # Process set\n",
    "    max_num_train = set_func.max_num_set(set_total, set_train_count)\n",
    "    max_num_test = set_func.max_num_set(set_total, set_test_count)\n",
    "    max_set = []\n",
    "    for i in range(len(max_num_train)):\n",
    "        max_set.append(max(max_num_train[i], max_num_test[i]))\n",
    "    \n",
    "    set_crf = [CRF(False) for _ in range(set_total)]\n",
    "    set_model = []\n",
    "    set_one_hot = []\n",
    "    for num in range(set_total):\n",
    "        max_num = max_set[num]\n",
    "        max_label = max(set_train_data[num]['Label'])\n",
    "        OOM_Split = 1\n",
    "        set_one_hot.append(OneHotEncoder())\n",
    "        while True:\n",
    "            try:\n",
    "                print(\"OOM:\", OOM_Split, \"process train features.\")\n",
    "                set_X_train, set_word_train, set_y_train = crf_process_set(set_train_data[num], \n",
    "                                                                           set_train_count, num, \n",
    "                                                                           max_set, path_max_len, \n",
    "                                                                           con_max_len, OOM_Split, set_one_hot[num], True)\n",
    "\n",
    "                page_num = int(len(set_X_train)/max_num)\n",
    "                set_crf[num] = CRF(False)\n",
    "                set_model.append(model_word_only(set_crf[num], max_num, max_label, OOM_Split))\n",
    "                history = func.LossHistory()\n",
    "                set_model[num].compile(\n",
    "                    loss=set_crf[num].loss,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[set_crf[num].accuracy]\n",
    "                )\n",
    "                print(set_model[num].summary())\n",
    "                stop_when_no_improve = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=0, patience = NO_IMPROVE, restore_best_weights=True)\n",
    "                until_loss = EarlyStoppingByLossVal(monitor='loss', value=UNTIL_LOSS, verbose=1)\n",
    "                callbacks = [history, stop_when_no_improve, until_loss]\n",
    "\n",
    "                # Train\n",
    "                print(\"Start training.\")\n",
    "                start = time.time()\n",
    "                set_model[num].fit([set_X_train, set_word_train[0], set_word_train[1]], set_y_train, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True, batch_size=BATCH_SIZE)\n",
    "                tst = time.time()-start\n",
    "                break\n",
    "            except:\n",
    "                OOM_Split += 1\n",
    "        t += tst\n",
    "\n",
    "        # Load Test file\n",
    "        print(\"process test features.\")\n",
    "        set_X_test, set_word_test, set_y_test = crf_process_set(set_test_data[num], \n",
    "                                                                set_test_count, num, \n",
    "                                                                max_set, path_max_len, \n",
    "                                                                con_max_len, OOM_Split, set_one_hot[num], False)\n",
    "        \n",
    "        page_test = int(len(set_X_test) / max_num)\n",
    "\n",
    "        # Prediction\n",
    "        ts_start = time.time()\n",
    "        pred = set_model[num].predict([set_X_test, set_word_test[0], set_word_test[1]], batch_size=VAL_BATCH_SIZE)\n",
    "        tsp = time.time()-ts_start\n",
    "        ts += tsp\n",
    "        pred = np.reshape(pred, [-1, max_label+1])\n",
    "        result = get_result(pred, max_num)\n",
    "        \n",
    "        # Read Col\n",
    "        set_col_type = set_func.get_col_type(current_path, num)\n",
    "\n",
    "        # Output\n",
    "        set_X_test = np.reshape(set_X_test, [-1, 6])\n",
    "        set_func.predict_output(current_path, model_name, num, set_col_type, result, max_label, set_X_test, max_num)\n",
    "    \n",
    "    # Process time\n",
    "    func.process_time(current_path, model_name, t, ts, page_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
