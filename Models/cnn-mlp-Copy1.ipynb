{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, unicode_literals, print_function, absolute_import\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from crflayer import CRF\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn_crfsuite import metrics\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import func\n",
    "import prepare_train_with_set as prepare\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_total = 1\n",
    "# How many Set\n",
    "DEBUG = False        # Print element\n",
    "path_max_len = 30    # padding length\n",
    "path_emb_size = 5    # embedding size\n",
    "\n",
    "con_max_len = 50    # padding length\n",
    "con_emb_size = 5    # embedding size\n",
    "\n",
    "EPOCHS = 10000        # Train epochs\n",
    "conv_num = 5        # First cnn filter num\n",
    "#max_num = 206       # How many nodes should pad\n",
    "UNTIL_LOSS = 0.001    # When achieve loss then stop\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005) # Set learning rate\n",
    "NO_IMPROVE = 3     # Stop when no improve for epochs\n",
    "current_path = os.path.join(os.path.expanduser(\"~\"), \"jupyter\", \"Sequence_Labeling_Wrapper_Verification\", \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_limit(num):\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only use the first GPU\n",
    "      try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[num], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "      except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tokenizer to convert words to encoding for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer():\n",
    "    tokenizer_path = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "    tokenizer_content = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "    return tokenizer_path, tokenizer_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_training(set_total, current_path, tokenizer_path, tokenizer_content, path_len, con_len):\n",
    "    train_data, Set_dict = prepare.train_file_generate(set_total, current_path)\n",
    "    test_data = prepare.test_file_generate(current_path)\n",
    "    max_num_train, max_label_train = func.load_data_num(train_data, True)\n",
    "    max_num_test = func.load_data_num(test_data, False)\n",
    "    max_num = max(max_num_train, max_num_test)\n",
    "    col_set_dict = dict(map(reversed, Set_dict.items()))\n",
    "    feature_train, word_train, label_train, out_train = func.cnn_process_data(train_data, tokenizer_path, tokenizer_content, path_len, con_len)\n",
    "    #feature_train = feature_train.tolist()\n",
    "    #label_train = label_train.tolist()\n",
    "    #word_train = [word_train[i].tolist() for i in range(len(word_train))]\n",
    "    return test_data, feature_train, word_train, label_train, max_label_train, max_num, col_set_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingByLossVal(tf.keras.callbacks.Callback):\n",
    "    '''\n",
    "    Early stop when training value less than setting value.\n",
    "    '''\n",
    "    def __init__(self, monitor='loss', value=UNTIL_LOSS, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_model(max_num, max_label):\n",
    "    '''\n",
    "    Model definition for our experiments using tensorflow keras.\n",
    "    '''\n",
    "    path_input = tf.keras.Input(shape=(path_max_len,), name='Path_emb_input')\n",
    "    content_input = tf.keras.Input(shape=(con_max_len,), name='Content_emb_input')\n",
    "    feature_input = tf.keras.Input(shape=(6,), name='Feature_input')\n",
    "    \n",
    "    path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_input)\n",
    "    content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_input)\n",
    "    \n",
    "    feature = tf. reshape(feature_input, [-1, max_num, 6])\n",
    "    path_emb = tf.reshape(path_emb, [-1, max_num, path_max_len*path_emb_size, 1])\n",
    "    content_emb = tf.reshape(content_emb, [-1, max_num, con_max_len*con_emb_size, 1])\n",
    "    \n",
    "    path = tf.keras.layers.Conv2D(conv_num, (3, path_max_len*path_emb_size), (1, path_max_len*path_emb_size), padding='same')(path_emb)\n",
    "    con = tf.keras.layers.Conv2D(conv_num, (3, con_max_len*con_emb_size), (1, con_max_len*con_emb_size), padding='same')(content_emb)\n",
    "    \n",
    "    path_emb = tf.reshape(path, [-1, max_num, conv_num])\n",
    "    content_emb = tf.reshape(con, [-1, max_num, conv_num])\n",
    "    \n",
    "    combine = tf.keras.layers.concatenate([feature, path_emb, content_emb], -1)\n",
    "    \n",
    "    mlp = combine\n",
    "    d = tf.reshape(mlp, [-1, 6 + conv_num*2])\n",
    "    d = tf.keras.layers.Dense(max_label+200, activation='tanh')(d)\n",
    "    d = tf.keras.layers.Dense(max_label+1, activation='softmax')(d)\n",
    "    output = d\n",
    "    model = tf.keras.Model(inputs=[feature_input, path_input, content_input], outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_word_only(max_num, max_label):\n",
    "    '''\n",
    "    Model definition for our experiments using tensorflow keras.\n",
    "    '''\n",
    "    path_input = tf.keras.Input(shape=(path_max_len,), name='Path_emb_input')\n",
    "    content_input = tf.keras.Input(shape=(con_max_len,), name='Content_emb_input')\n",
    "    feature_input = tf.keras.Input(shape=(6,), name='Feature_input')\n",
    "    \n",
    "    path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_input)\n",
    "    content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_input)\n",
    "    \n",
    "    path_emb = tf.reshape(path_emb, [-1, max_num, path_max_len*path_emb_size, 1])\n",
    "    content_emb = tf.reshape(content_emb, [-1, max_num, con_max_len*con_emb_size, 1])\n",
    "    \n",
    "    path = tf.keras.layers.Conv2D(conv_num, (3, path_max_len*path_emb_size), (1, path_max_len*path_emb_size), padding='same')(path_emb)\n",
    "    con = tf.keras.layers.Conv2D(conv_num, (3, con_max_len*con_emb_size), (1, con_max_len*con_emb_size), padding='same')(content_emb)\n",
    "    \n",
    "    path_emb = tf.reshape(path, [-1, max_num, conv_num])\n",
    "    content_emb = tf.reshape(con, [-1, max_num, conv_num])\n",
    "    \n",
    "    combine = tf.keras.layers.concatenate([path_emb, content_emb], -1)\n",
    "    \n",
    "    mlp = combine\n",
    "    d = tf.reshape(mlp, [-1, conv_num*2])\n",
    "    d = tf.keras.layers.Dense(max_label+200, activation='tanh')(d)\n",
    "    d = tf.keras.layers.Dense(max_label+1, activation='softmax')(d)\n",
    "    output = d\n",
    "    model = tf.keras.Model(inputs=[feature_input, path_input, content_input], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_compile(history):\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(model.summary())\n",
    "    stop_when_no_improve = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=0, patience = NO_IMPROVE, restore_best_weights=True)\n",
    "    until_loss = EarlyStoppingByLossVal(monitor='loss', value=UNTIL_LOSS, verbose=1)\n",
    "    callbacks = [history, stop_when_no_improve, until_loss]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, path, content, y, callbacks):\n",
    "    start = time.time()\n",
    "    model.fit([X_train, path, content], y, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True, batch_size=BATCH_SIZE)\n",
    "    t = time.time()-start\n",
    "    return model, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, feature, path, content):\n",
    "    ts_start = time.time()\n",
    "    predictions = model.predict([feature, path, content], batch_size=VAL_BATCH_SIZE)\n",
    "    ts = time.time()-ts_start\n",
    "    return predictions, ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    import pickle\n",
    "    #model.save_weights(\"./cnn/data/cnn.h5\")\n",
    "    model.save(os.path.join(current_path, \"cnn\", \"data\", \"model.h5\"))\n",
    "    # saving\n",
    "    with open(os.path.join(current_path, \"cnn\", \"data\", \"tokenizer_path.pickle\"), \"wb\") as handle:\n",
    "        pickle.dump(tokenizer_path, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(current_path, \"cnn\", \"data\", \"tokenizer_content.pickle\"), \"wb\") as handle:\n",
    "        pickle.dump(tokenizer_content, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    import pickle\n",
    "    model = tf.keras.models.load_model(os.path.join(current_path, \"cnn\", \"data\", \"model.h5\"))\n",
    "    model.summary()\n",
    "    # loading\n",
    "    with open(os.path.join(current_path, \"cnn\", \"data\", \"tokenizer_path.pickle\"), 'rb') as handle:\n",
    "        tokenizer_path = pickle.load(handle)\n",
    "    with open(os.path.join(current_path, \"cnn\", \"data\", \"tokenizer_content.pickle\"), 'rb') as handle:\n",
    "        tokenizer_content = pickle.load(handle)        \n",
    "    path_word_size = len(tokenizer_path.index_docs)\n",
    "    con_word_size = len(tokenizer_content.index_docs)\n",
    "    return model, tokenizer_path, tokenizer_content, path_word_size, con_word_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(predictions, max_num):\n",
    "    result = []\n",
    "    count = 0\n",
    "    for page in range(int(len(predictions)/max_num)):\n",
    "        tmp = []\n",
    "        for node in range(max_num):\n",
    "            tmp.append(np.argmax(predictions[count]))\n",
    "            count += 1\n",
    "        result.append(tmp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n",
      "Train Table Opening:/home/rick/jupyter/Sequence_Labeling_Wrapper_Verification/data/data/TableA.txt\n",
      "\n",
      "Train file find coltype\n",
      " --------------------------------------------------------------------------------\n",
      "Test Table Opening:/home/rick/jupyter/Sequence_Labeling_Wrapper_Verification/data/data/GA.txt\n",
      "\n",
      "Test file find coltype\n",
      " --------------------------------------------------------------------------------\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Path_emb_input (InputLayer)     [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Content_emb_input (InputLayer)  [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 5)        80          Path_emb_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 5)        7550        Content_emb_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 99, 150, 1)] 0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 99, 250, 1)] 0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Feature_input (InputLayer)      [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 99, 1, 5)     2255        tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 99, 1, 5)     3755        tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 99, 6)]      0           Feature_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 99, 5)]      0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 99, 5)]      0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 99, 16)       0           tf_op_layer_Reshape[0][0]        \n",
      "                                                                 tf_op_layer_Reshape_3[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 16)]         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 236)          4012        tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 37)           8769        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 26,421\n",
      "Trainable params: 26,421\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 1980 samples\n",
      "Epoch 1/10000\n",
      "1980/1980 [==============================] - 2s 1ms/sample - loss: 3.4506 - accuracy: 0.0732\n",
      "Epoch 2/10000\n",
      "1980/1980 [==============================] - 0s 42us/sample - loss: 1.7159 - accuracy: 0.6328\n",
      "Epoch 3/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 1.3296 - accuracy: 0.6778\n",
      "Epoch 4/10000\n",
      "1980/1980 [==============================] - 0s 41us/sample - loss: 1.1389 - accuracy: 0.7106\n",
      "Epoch 5/10000\n",
      "1980/1980 [==============================] - 0s 42us/sample - loss: 1.0417 - accuracy: 0.7273\n",
      "Epoch 6/10000\n",
      "1980/1980 [==============================] - 0s 42us/sample - loss: 0.9753 - accuracy: 0.7465\n",
      "Epoch 7/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.9297 - accuracy: 0.7460\n",
      "Epoch 8/10000\n",
      "1980/1980 [==============================] - 0s 42us/sample - loss: 0.8938 - accuracy: 0.7581\n",
      "Epoch 9/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.8642 - accuracy: 0.7672\n",
      "Epoch 10/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.8346 - accuracy: 0.7732\n",
      "Epoch 11/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.8112 - accuracy: 0.7697\n",
      "Epoch 12/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.7880 - accuracy: 0.7763\n",
      "Epoch 13/10000\n",
      "1980/1980 [==============================] - 0s 46us/sample - loss: 0.7727 - accuracy: 0.7768\n",
      "Epoch 14/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.7464 - accuracy: 0.7823\n",
      "Epoch 15/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.7336 - accuracy: 0.7884\n",
      "Epoch 16/10000\n",
      "1980/1980 [==============================] - 0s 55us/sample - loss: 0.7114 - accuracy: 0.7869\n",
      "Epoch 17/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.6883 - accuracy: 0.7949\n",
      "Epoch 18/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.6723 - accuracy: 0.7843\n",
      "Epoch 19/10000\n",
      "1980/1980 [==============================] - 0s 55us/sample - loss: 0.6537 - accuracy: 0.7929\n",
      "Epoch 20/10000\n",
      "1980/1980 [==============================] - 0s 55us/sample - loss: 0.6346 - accuracy: 0.7929\n",
      "Epoch 21/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.6172 - accuracy: 0.7980\n",
      "Epoch 22/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.5942 - accuracy: 0.7995\n",
      "Epoch 23/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.5834 - accuracy: 0.8020\n",
      "Epoch 24/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.5664 - accuracy: 0.8061\n",
      "Epoch 25/10000\n",
      "1980/1980 [==============================] - 0s 44us/sample - loss: 0.5538 - accuracy: 0.7990\n",
      "Epoch 26/10000\n",
      "1980/1980 [==============================] - 0s 44us/sample - loss: 0.5403 - accuracy: 0.8126\n",
      "Epoch 27/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.5250 - accuracy: 0.8091\n",
      "Epoch 28/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.5121 - accuracy: 0.8212\n",
      "Epoch 29/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.4984 - accuracy: 0.8242\n",
      "Epoch 30/10000\n",
      "1980/1980 [==============================] - 0s 54us/sample - loss: 0.4912 - accuracy: 0.8227\n",
      "Epoch 31/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.4795 - accuracy: 0.8429\n",
      "Epoch 32/10000\n",
      "1980/1980 [==============================] - 0s 43us/sample - loss: 0.4735 - accuracy: 0.8389\n",
      "Epoch 33/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.4671 - accuracy: 0.8374\n",
      "Epoch 34/10000\n",
      "1980/1980 [==============================] - 0s 44us/sample - loss: 0.4560 - accuracy: 0.8384\n",
      "Epoch 35/10000\n",
      "1980/1980 [==============================] - 0s 44us/sample - loss: 0.4466 - accuracy: 0.8581\n",
      "Epoch 36/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 0s 43us/sample - loss: 0.4391 - accuracy: 0.8525\n",
      "Epoch 37/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.4303 - accuracy: 0.8515\n",
      "Epoch 38/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.4266 - accuracy: 0.8500\n",
      "Epoch 39/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.4222 - accuracy: 0.8581\n",
      "Epoch 40/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.4126 - accuracy: 0.8530\n",
      "Epoch 41/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.4084 - accuracy: 0.8520\n",
      "Epoch 42/10000\n",
      "1980/1980 [==============================] - 0s 54us/sample - loss: 0.4008 - accuracy: 0.8712\n",
      "Epoch 43/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.3962 - accuracy: 0.8662\n",
      "Epoch 44/10000\n",
      "1980/1980 [==============================] - 0s 53us/sample - loss: 0.3935 - accuracy: 0.8677\n",
      "Epoch 45/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.3853 - accuracy: 0.8682\n",
      "Epoch 46/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.3803 - accuracy: 0.8722\n",
      "Epoch 47/10000\n",
      "1980/1980 [==============================] - 0s 53us/sample - loss: 0.3766 - accuracy: 0.8753\n",
      "Epoch 48/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.3731 - accuracy: 0.8616\n",
      "Epoch 49/10000\n",
      "1980/1980 [==============================] - 0s 53us/sample - loss: 0.3664 - accuracy: 0.8662\n",
      "Epoch 50/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.3608 - accuracy: 0.8712\n",
      "Epoch 51/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.3570 - accuracy: 0.8778\n",
      "Epoch 52/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.3545 - accuracy: 0.8742\n",
      "Epoch 53/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.3533 - accuracy: 0.8636\n",
      "Epoch 54/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.3468 - accuracy: 0.8874\n",
      "Epoch 55/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.3424 - accuracy: 0.8788\n",
      "Epoch 56/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.3365 - accuracy: 0.8823\n",
      "Epoch 57/10000\n",
      "1980/1980 [==============================] - 0s 56us/sample - loss: 0.3332 - accuracy: 0.8889\n",
      "Epoch 58/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.3325 - accuracy: 0.8702\n",
      "Epoch 59/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.3306 - accuracy: 0.8828\n",
      "Epoch 60/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.3229 - accuracy: 0.8828\n",
      "Epoch 61/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.3220 - accuracy: 0.8889\n",
      "Epoch 62/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.3196 - accuracy: 0.8899\n",
      "Epoch 63/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.3149 - accuracy: 0.8939\n",
      "Epoch 64/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.3141 - accuracy: 0.8869\n",
      "Epoch 65/10000\n",
      "1980/1980 [==============================] - 0s 46us/sample - loss: 0.3081 - accuracy: 0.8970\n",
      "Epoch 66/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.3061 - accuracy: 0.8879\n",
      "Epoch 67/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.3022 - accuracy: 0.8965\n",
      "Epoch 68/10000\n",
      "1980/1980 [==============================] - 0s 43us/sample - loss: 0.3006 - accuracy: 0.8949\n",
      "Epoch 69/10000\n",
      "1980/1980 [==============================] - 0s 46us/sample - loss: 0.2988 - accuracy: 0.8874\n",
      "Epoch 70/10000\n",
      "1980/1980 [==============================] - 0s 53us/sample - loss: 0.2954 - accuracy: 0.9010\n",
      "Epoch 71/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.2921 - accuracy: 0.9005\n",
      "Epoch 72/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.2915 - accuracy: 0.8995\n",
      "Epoch 73/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2867 - accuracy: 0.9025\n",
      "Epoch 74/10000\n",
      "1980/1980 [==============================] - 0s 53us/sample - loss: 0.2861 - accuracy: 0.9056\n",
      "Epoch 75/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.2850 - accuracy: 0.9056\n",
      "Epoch 76/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.2798 - accuracy: 0.8995\n",
      "Epoch 77/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.2771 - accuracy: 0.9071\n",
      "Epoch 78/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.2778 - accuracy: 0.9035\n",
      "Epoch 79/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.2786 - accuracy: 0.9086\n",
      "Epoch 80/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2731 - accuracy: 0.9066\n",
      "Epoch 81/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.2702 - accuracy: 0.9056\n",
      "Epoch 82/10000\n",
      "1980/1980 [==============================] - 0s 55us/sample - loss: 0.2681 - accuracy: 0.9101\n",
      "Epoch 83/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.2702 - accuracy: 0.8965\n",
      "Epoch 84/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.2668 - accuracy: 0.9056\n",
      "Epoch 85/10000\n",
      "1980/1980 [==============================] - 0s 53us/sample - loss: 0.2648 - accuracy: 0.9030\n",
      "Epoch 86/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2598 - accuracy: 0.9126\n",
      "Epoch 87/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.2589 - accuracy: 0.9111\n",
      "Epoch 88/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.2590 - accuracy: 0.9056\n",
      "Epoch 89/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2532 - accuracy: 0.9242\n",
      "Epoch 90/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.2547 - accuracy: 0.9005\n",
      "Epoch 91/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2488 - accuracy: 0.9162\n",
      "Epoch 92/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.2489 - accuracy: 0.9167\n",
      "Epoch 93/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.2454 - accuracy: 0.9202\n",
      "Epoch 94/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.2452 - accuracy: 0.9202\n",
      "Epoch 95/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2425 - accuracy: 0.9182\n",
      "Epoch 96/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.2424 - accuracy: 0.9192\n",
      "Epoch 97/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.2403 - accuracy: 0.9212\n",
      "Epoch 98/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2376 - accuracy: 0.9247\n",
      "Epoch 99/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.2395 - accuracy: 0.9197\n",
      "Epoch 100/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2378 - accuracy: 0.9247\n",
      "Epoch 101/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.2367 - accuracy: 0.9217\n",
      "Epoch 102/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.2341 - accuracy: 0.9268\n",
      "Epoch 103/10000\n",
      "1980/1980 [==============================] - 0s 53us/sample - loss: 0.2337 - accuracy: 0.9167\n",
      "Epoch 104/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.2328 - accuracy: 0.9222\n",
      "Epoch 105/10000\n",
      "1980/1980 [==============================] - 0s 42us/sample - loss: 0.2313 - accuracy: 0.9263\n",
      "Epoch 106/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2300 - accuracy: 0.9237\n",
      "Epoch 107/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2280 - accuracy: 0.9182\n",
      "Epoch 108/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.2264 - accuracy: 0.9283\n",
      "Epoch 109/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.2298 - accuracy: 0.9298\n",
      "Epoch 110/10000\n",
      "1980/1980 [==============================] - 0s 55us/sample - loss: 0.2255 - accuracy: 0.9187\n",
      "Epoch 111/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.2228 - accuracy: 0.9308\n",
      "Epoch 112/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.2244 - accuracy: 0.9232\n",
      "Epoch 113/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.2225 - accuracy: 0.9247\n",
      "Epoch 114/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2218 - accuracy: 0.9283\n",
      "Epoch 115/10000\n",
      "1980/1980 [==============================] - 0s 44us/sample - loss: 0.2186 - accuracy: 0.9268\n",
      "Epoch 116/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.2197 - accuracy: 0.9308\n",
      "Epoch 117/10000\n",
      "1980/1980 [==============================] - 0s 53us/sample - loss: 0.2148 - accuracy: 0.9308\n",
      "Epoch 118/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.2168 - accuracy: 0.9308\n",
      "Epoch 119/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.2147 - accuracy: 0.9278\n",
      "Epoch 120/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2132 - accuracy: 0.9333\n",
      "Epoch 121/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.2109 - accuracy: 0.9343\n",
      "Epoch 122/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.2120 - accuracy: 0.9343\n",
      "Epoch 123/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2103 - accuracy: 0.9268\n",
      "Epoch 124/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2107 - accuracy: 0.9278\n",
      "Epoch 125/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.2090 - accuracy: 0.9298\n",
      "Epoch 126/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.2080 - accuracy: 0.9328\n",
      "Epoch 127/10000\n",
      "1980/1980 [==============================] - 0s 55us/sample - loss: 0.2072 - accuracy: 0.9293\n",
      "Epoch 128/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.2060 - accuracy: 0.9333\n",
      "Epoch 129/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.2059 - accuracy: 0.9328\n",
      "Epoch 130/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2029 - accuracy: 0.9303\n",
      "Epoch 131/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.2059 - accuracy: 0.9348\n",
      "Epoch 132/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.2020 - accuracy: 0.9343\n",
      "Epoch 133/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.2018 - accuracy: 0.9318\n",
      "Epoch 134/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.2043 - accuracy: 0.9283\n",
      "Epoch 135/10000\n",
      "1980/1980 [==============================] - 0s 46us/sample - loss: 0.2026 - accuracy: 0.9313\n",
      "Epoch 136/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2004 - accuracy: 0.9263\n",
      "Epoch 137/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.2010 - accuracy: 0.9359\n",
      "Epoch 138/10000\n",
      "1980/1980 [==============================] - 0s 54us/sample - loss: 0.1994 - accuracy: 0.9288\n",
      "Epoch 139/10000\n",
      "1980/1980 [==============================] - 0s 42us/sample - loss: 0.2002 - accuracy: 0.9333\n",
      "Epoch 140/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.1982 - accuracy: 0.9364\n",
      "Epoch 141/10000\n",
      "1980/1980 [==============================] - 0s 51us/sample - loss: 0.1964 - accuracy: 0.9283\n",
      "Epoch 142/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.1971 - accuracy: 0.9318\n",
      "Epoch 143/10000\n",
      "1980/1980 [==============================] - 0s 50us/sample - loss: 0.1952 - accuracy: 0.9359\n",
      "Epoch 144/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.1948 - accuracy: 0.9359\n",
      "Epoch 145/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.1947 - accuracy: 0.9328\n",
      "Epoch 146/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.1940 - accuracy: 0.9338\n",
      "Epoch 147/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.1936 - accuracy: 0.9323\n",
      "Epoch 148/10000\n",
      "1980/1980 [==============================] - 0s 52us/sample - loss: 0.1938 - accuracy: 0.9318\n",
      "Epoch 149/10000\n",
      "1980/1980 [==============================] - 0s 53us/sample - loss: 0.1908 - accuracy: 0.9323\n",
      "Epoch 150/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.1927 - accuracy: 0.9354\n",
      "Epoch 151/10000\n",
      "1980/1980 [==============================] - 0s 46us/sample - loss: 0.1933 - accuracy: 0.9318\n",
      "Epoch 152/10000\n",
      "1980/1980 [==============================] - 0s 49us/sample - loss: 0.1903 - accuracy: 0.9343\n",
      "Epoch 153/10000\n",
      "1980/1980 [==============================] - 0s 42us/sample - loss: 0.1903 - accuracy: 0.9394\n",
      "Epoch 154/10000\n",
      "1980/1980 [==============================] - 0s 44us/sample - loss: 0.1895 - accuracy: 0.9333\n",
      "Epoch 155/10000\n",
      "1980/1980 [==============================] - 0s 45us/sample - loss: 0.1888 - accuracy: 0.9328\n",
      "Epoch 156/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.1885 - accuracy: 0.9384\n",
      "Epoch 157/10000\n",
      "1980/1980 [==============================] - 0s 47us/sample - loss: 0.1877 - accuracy: 0.9354\n",
      "Epoch 158/10000\n",
      "1980/1980 [==============================] - 0s 42us/sample - loss: 0.1889 - accuracy: 0.9364\n",
      "Epoch 159/10000\n",
      "1980/1980 [==============================] - 0s 48us/sample - loss: 0.1908 - accuracy: 0.9333\n",
      "Epoch 160/10000\n",
      "1980/1980 [==============================] - 0s 46us/sample - loss: 0.1887 - accuracy: 0.9328\n",
      "int32 int32 int32\n",
      "(1980, 37)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # How many Set\n",
    "    set_total = 1\n",
    "    model_name = \"cnn\"\n",
    "    current_path = os.path.join(os.path.expanduser(\"~\"), \"jupyter\", \"Sequence_Labeling_Wrapper_Verification\", \"data\")\n",
    "    \n",
    "    # GPU\n",
    "    gpu_limit(1)\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer_path, tokenizer_content = tokenizer()\n",
    "    \n",
    "    # Process training file\n",
    "    test_data, X_train, word_train, y_train, max_label_train, max_num, col_set_dict = process_training(set_total, \n",
    "                                                                                                    current_path, \n",
    "                                                                                                    tokenizer_path, \n",
    "                                                                                                    tokenizer_content, \n",
    "                                                                                                    path_max_len, con_max_len)\n",
    "    \n",
    "    BATCH_SIZE = max_num      # batch size\n",
    "    VAL_BATCH_SIZE = max_num  # Validation batch size\n",
    "    path_word_size = len(tokenizer_path.index_docs)\n",
    "    con_word_size = len(tokenizer_content.index_docs)\n",
    "    page_num = int(len(y_train)/max_num)\n",
    "    \n",
    "    # Define model\n",
    "    model = full_model(max_num, max_label_train)\n",
    "    history = func.LossHistory()\n",
    "    callables = model_compile(history)\n",
    "    \n",
    "    # Start training\n",
    "    model, t = train(model, X_train, word_train[0], word_train[1], y_train, callables)\n",
    "    \n",
    "    # Graph\n",
    "    #history.loss_plot('epoch')\n",
    "    \n",
    "    # Load test feature\n",
    "    X_test, word_test, y_test, _ = func.cnn_process_data(test_data, tokenizer_path, tokenizer_content, path_max_len, con_max_len)\n",
    "    \n",
    "    # Start testing\n",
    "    pred, ts = predict(model, X_test, word_test[0], word_test[1])\n",
    "    \n",
    "    # Process output\n",
    "    result = get_result(pred, max_num)\n",
    "    col_type = func.get_col_type(current_path)\n",
    "    Set_data = func.predict_output(set_total, current_path, model_name, col_type, result, max_label_train, col_set_dict)\n",
    "    set_train_data, set_train_count = set_func.Set_train_file_generate(set_total, current_path, model_name, feature_train, max_num)\n",
    "    set_test_data, set_test_count = set_func.Set_test_file_generate(set_total, current_path, model_name, Set_data, feature_test, max_num)\n",
    "    page_c = len(result)\n",
    "    \n",
    "    # Process set\n",
    "            \n",
    "    # Process time\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Set Train File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train file for Set Model from DCADE Set Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data_count = []\n",
    "if set_total > 0:\n",
    "    for set_t in range(set_total):\n",
    "        with open(os.path.join(current_path, \"data\", \"Set-\" + str(set_t+1) + \".txt\"), \"r\") as set_file:\n",
    "            set_tmp = []\n",
    "            output_name = os.path.join(current_path, \"cnn\", \"set\", \"Set-\" + str(set_t+1) + \"_train_raw.csv\")\n",
    "            if DEBUG:\n",
    "                print(\"Generating:\" + output_name + \"\\n\")\n",
    "            output = open(output_name, \"w\")\n",
    "            output.write(\"Leafnode\\tPTypeSet\\tTypeSet\\tContentid\\tPathid\\tSimseqid\\tPath\\tContent\\tLabel\\n\")\n",
    "            line = set_file.readline()\n",
    "            slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            while(slot[0]!=\"ColType\"): \n",
    "                line = set_file.readline()\n",
    "                slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            with open(os.path.join(current_path, \"cnn\", \"set\", \"Set-\"+ str(set_t+1) + \"_coltype.txt\"), \"w\") as col_file:\n",
    "                col_file.write(str(slot[1:]))\n",
    "            line = set_file.readline() # First line of data\n",
    "            page_num = 0\n",
    "            count = 0\n",
    "            while(line != \"\"):\n",
    "                slot = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                data_info = slot[0].split(\"-\")\n",
    "                if(page_num != int(data_info[1])):\n",
    "                    set_tmp.append(count)\n",
    "                    count = 0\n",
    "                set_num = int(data_info[0])\n",
    "                page_num = int(data_info[1])\n",
    "                if DEBUG:\n",
    "                    print(str(data_info[0])+\"-\"+str(data_info[1])+\"-\"+str(data_info[2]))\n",
    "                idx = 1\n",
    "                sub_list = slot[1:]\n",
    "                while(\"\" in sub_list):\n",
    "                    sub_list.remove(\"\")\n",
    "                while(\" \" in sub_list):\n",
    "                    sub_list.remove(\" \")\n",
    "                for element in sub_list:\n",
    "                    count += 1\n",
    "                    if DEBUG:\n",
    "                        print(element)\n",
    "                    element = int(element)\n",
    "                    output.write(str(feature_train_1[page_num*max_num+element])+\"\\t\")\n",
    "                    output.write(str(feature_train_2[page_num*max_num+element])+\"\\t\")\n",
    "                    output.write(str(feature_train_3[page_num*max_num+element])+\"\\t\")\n",
    "                    output.write(str(feature_train_4[page_num*max_num+element])+\"\\t\")\n",
    "                    output.write(str(feature_train_5[page_num*max_num+element])+\"\\t\")\n",
    "                    output.write(str(feature_train_6[page_num*max_num+element])+\"\\t\")\n",
    "                    output.write(str(list(path_train[page_num*max_num+element])))\n",
    "                    output.write(\"\\t\")\n",
    "                    output.write(str(list(content_train[page_num*max_num+element])))\n",
    "                    output.write(\"\\t\")\n",
    "                    output.write(str(idx) + \"\\n\")\n",
    "                    if DEBUG:\n",
    "                        print(feature_train_1[page_num*max_num+element])\n",
    "                    idx += 1\n",
    "                line = set_file.readline()\n",
    "            set_tmp.append(count)\n",
    "            output.close()\n",
    "        set_data_count.append(set_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    with open(os.path.join(current_path, \"cnn\", \"set\", \"set_train_count.txt\"), \"w\") as file:\n",
    "        file.write(str(set_data_count))\n",
    "        if DEBUG:\n",
    "            print(set_data_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Set Test file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate test file from node data being predicted in a Set by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data_count = []\n",
    "if set_total > 0:\n",
    "    for set_t in range(set_total):\n",
    "        set_tmp = []\n",
    "        with open(os.path.join(current_path, \"cnn\", \"set\", \"Set-\" + str(set_t+1) + \"_ytest_raw.csv\"), \"w\") as set_file:\n",
    "            set_file.write(\"Leafnode\\tPTypeSet\\tTypeSet\\tContentid\\tPathid\\tSimseqid\\tPath\\tContent\\tLabel\\n\")\n",
    "            for pages in tqdm(range(len(Set_data))):\n",
    "                count = 0\n",
    "                for node in Set_data[pages][set_t]:\n",
    "                    count += 1\n",
    "                    set_file.write(str(feature_test_1[pages*max_num+node])+\"\\t\")\n",
    "                    set_file.write(str(feature_test_2[pages*max_num+node])+\"\\t\")\n",
    "                    set_file.write(str(feature_test_3[pages*max_num+node])+\"\\t\")\n",
    "                    set_file.write(str(feature_test_4[pages*max_num+node])+\"\\t\")\n",
    "                    set_file.write(str(feature_test_5[pages*max_num+node])+\"\\t\")\n",
    "                    set_file.write(str(feature_test_6[pages*max_num+node])+\"\\t\")\n",
    "                    set_file.write(str(list(path_test[pages*max_num+node])))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(list(content_test[pages*max_num+node])))\n",
    "                    set_file.write(\"\\t\")\n",
    "                    set_file.write(str(0) + \"\\n\")\n",
    "                set_tmp.append(count)\n",
    "        set_data_count.append(set_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    with open(os.path.join(current_path, \"cnn\", \"set\", \"set_test_count.txt\"), \"w\") as file:\n",
    "        file.write(str(set_data_count))\n",
    "        if DEBUG:\n",
    "            print(set_data_count)\n",
    "    with open(os.path.join(current_path, \"cnn\", \"set\", \"word_size.txt\"), \"w\") as file:\n",
    "        file.write(str(path_word_size)+\"\\n\")\n",
    "        file.write(str(con_word_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page_c = len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_acc = model.evaluate([path_train, content_train, feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6], label_train, batch_size=BATCH_SIZE)\n",
    "print(\"\\n\\nLoss {}, Acc {}\".format(model_loss, model_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(label_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_max_len = 30    # padding length\n",
    "path_emb_size = 10    # embedding size\n",
    "\n",
    "con_max_len = 50    # padding length\n",
    "con_emb_size = 10    # embedding size\n",
    "\n",
    "feature_emb_size = 5\n",
    "\n",
    "EPOCHS = 5000        # Train epochs\n",
    "conv_num = 20        # First cnn filter num\n",
    "UNTIL_LOSS = 0.001    # When achieve loss then stop\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001) # Set learning rate\n",
    "NO_IMPROVE = 50     # Stop when no improve for epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_num_set(set_data_count, set_total):\n",
    "    max_set = []\n",
    "    for i in range(set_total):\n",
    "        max_set.append(0)\n",
    "    for sets in range(len(set_data_count)):\n",
    "        max_set[sets] = max(set_data_count[sets])\n",
    "    return max_set\n",
    "\n",
    "def feature_padding_set(df, set_count, set_num):\n",
    "    feature = []\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            feature.append(df[count])\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                feature.append(9999)\n",
    "    return feature\n",
    "\n",
    "def emb_padding_set(df, set_count, set_num, pad_len):\n",
    "    emb = []\n",
    "    tmp = []\n",
    "    for i in range(pad_len):\n",
    "        tmp.append(0)\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            emb.append(eval(df[count]))\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                emb.append(tmp)\n",
    "    if DEBUG:\n",
    "        print(count)\n",
    "    return emb\n",
    "\n",
    "def one_of_n(ans, total):\n",
    "    tmp = []\n",
    "    for i in range(int(total)):\n",
    "        if ans == i:\n",
    "            tmp.append(1.0)\n",
    "        else:\n",
    "            tmp.append(0.0)\n",
    "    return tmp\n",
    "\n",
    "def label_padding_set(df, set_count, set_num):\n",
    "    label = []\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            label.append(df[count])\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                label.append(0)\n",
    "    return label\n",
    "\n",
    "def to_train_array_set(df, set_count, set_num):\n",
    "    feature_1 = np.array(feature_padding_set(df['Leafnode'], set_count, set_num))\n",
    "    feature_2 = np.array(feature_padding_set(df['PTypeSet'], set_count, set_num))\n",
    "    feature_3 = np.array(feature_padding_set(df['TypeSet'], set_count, set_num))\n",
    "    feature_4 = np.array(feature_padding_set(df['Contentid'], set_count, set_num))\n",
    "    feature_5 = np.array(feature_padding_set(df['Pathid'], set_count, set_num))\n",
    "    feature_6 = np.array(feature_padding_set(df['Simseqid'], set_count, set_num))\n",
    "    feature_1 = feature_1.flatten()\n",
    "    feature_2 = feature_2.flatten()\n",
    "    feature_3 = feature_3.flatten()\n",
    "    feature_4 = feature_4.flatten()\n",
    "    feature_5 = feature_5.flatten()\n",
    "    feature_6 = feature_6.flatten()\n",
    "    \n",
    "    path = np.array(emb_padding_set(df['Path'], set_count, set_num, path_max_len))\n",
    "    path = np.reshape(path, [len(set_count[set_num-1])*max_set[set_num-1], path_max_len])\n",
    "    content = np.array(emb_padding_set(df['Content'], set_count, set_num, con_max_len))\n",
    "    content = np.reshape(content, [len(set_count[set_num-1])*max_set[set_num-1], con_max_len])\n",
    "    \n",
    "    label = np.array(label_padding_set(df['Label'], set_count, set_num))\n",
    "    label = np.reshape(label, [len(set_count[set_num-1])*max_set[set_num-1], 1])\n",
    "    return feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, path, content, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:\n",
    "    Set_data = []\n",
    "    set_train_count = []\n",
    "    set_test_count = []\n",
    "    with open(os.path.join(current_path, \"cnn\", \"set\", \"Set_data.txt\"), \"r\") as set_file:\n",
    "        Set_data = eval(set_file.readline())\n",
    "    with open(os.path.join(current_path, \"cnn\", \"set\", \"set_train_count.txt\"), \"r\") as set_file:\n",
    "        set_train_count = eval(set_file.readline())\n",
    "    with open(os.path.join(current_path, \"cnn\", \"set\", \"set_test_count.txt\"), \"r\") as set_file:\n",
    "        set_test_count = eval(set_file.readline())\n",
    "    with open(os.path.join(current_path, \"cnn\", \"set\", \"word_size.txt\"), \"r\") as file:\n",
    "        path_word_size = eval(file.readline())\n",
    "        con_word_size = eval(file.readline())\n",
    "    max_num_train = max_num_set(set_train_count, set_total)\n",
    "    max_num_test = max_num_set(set_test_count, set_total)\n",
    "    max_set = []\n",
    "    for i in range(len(max_num_train)):\n",
    "        max_set.append(max(max_num_train[i], max_num_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ALL Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop all the set for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_total > 0:    \n",
    "    for num in range(set_total):\n",
    "        set_num = num + 1\n",
    "        # Load Train file & Test file\n",
    "        df = get_df(os.path.join(current_path, \"cnn\", \"set\", \"Set-\" + str(set_num) + \"_train_raw.csv\"))\n",
    "        max_num = max_set[set_num-1]\n",
    "        max_label = max(df['Label'])\n",
    "        BATCH_SIZE = max_num      # Training bath size\n",
    "        VAL_BATCH_SIZE = max_num  # Validation batch size\n",
    "        feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6, path_train, content_train, label_train = to_train_array_set(df, set_train_count, set_num)\n",
    "        \n",
    "        # Design Model\n",
    "        def get_model():\n",
    "            path_input = tf.keras.Input(shape=(path_max_len,), name='Path_emb_input')\n",
    "            content_input = tf.keras.Input(shape=(con_max_len,), name='Content_emb_input')\n",
    "            feature_input_1 = tf.keras.Input(shape=(1,), name='Feature_input1')\n",
    "            feature_input_2 = tf.keras.Input(shape=(1,), name='Feature_input2')\n",
    "            feature_input_3 = tf.keras.Input(shape=(1,), name='Feature_input3')\n",
    "            feature_input_4 = tf.keras.Input(shape=(1,), name='Feature_input4')\n",
    "            feature_input_5 = tf.keras.Input(shape=(1,), name='Feature_input5')\n",
    "            feature_input_6 = tf.keras.Input(shape=(1,), name='Feature_input6')\n",
    "\n",
    "            if DEBUG:\n",
    "                print(path_input.shape)\n",
    "\n",
    "            path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_input)\n",
    "            content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_input)\n",
    "            \n",
    "            f_1_emb = tf.keras.layers.Embedding(10000, feature_emb_size)(feature_input_1)\n",
    "            f_2_emb = tf.keras.layers.Embedding(10000, feature_emb_size)(feature_input_2)\n",
    "            f_3_emb = tf.keras.layers.Embedding(10000, feature_emb_size)(feature_input_3)\n",
    "            f_4_emb = tf.keras.layers.Embedding(10000, feature_emb_size)(feature_input_4)\n",
    "            f_5_emb = tf.keras.layers.Embedding(10000, feature_emb_size)(feature_input_5)\n",
    "            f_6_emb = tf.keras.layers.Embedding(10000, feature_emb_size)(feature_input_6)\n",
    "\n",
    "            path_emb = tf.reshape(path_emb, [-1, max_num, path_max_len*path_emb_size])\n",
    "            content_emb = tf.reshape(content_emb, [-1, max_num, con_max_len*con_emb_size])\n",
    "\n",
    "            path_emb = tf.expand_dims(path_emb, -1)\n",
    "            content_emb = tf.expand_dims(content_emb, -1)\n",
    "\n",
    "            path_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3,  path_max_len*path_emb_size), strides=(1, path_max_len*path_emb_size), name='Conv_for_Path_emb', padding='same')(path_emb)\n",
    "            content_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3, con_max_len*con_emb_size), strides=(1, con_max_len*con_emb_size), name='Conv_for_Content_emb', padding='same')(content_emb)\n",
    "\n",
    "            path = tf.reshape(path_feature, [-1, conv_num])\n",
    "            content = tf.reshape(content_feature, [-1, conv_num])\n",
    "            \n",
    "            f_1_emb = tf.reshape(f_1_emb, [-1, feature_emb_size])\n",
    "            f_2_emb = tf.reshape(f_2_emb, [-1, feature_emb_size])\n",
    "            f_3_emb = tf.reshape(f_3_emb, [-1, feature_emb_size])\n",
    "            f_4_emb = tf.reshape(f_4_emb, [-1, feature_emb_size])\n",
    "            f_5_emb = tf.reshape(f_5_emb, [-1, feature_emb_size])\n",
    "            f_6_emb = tf.reshape(f_6_emb, [-1, feature_emb_size])\n",
    "\n",
    "            combine = tf.keras.layers.concatenate([path, content, f_1_emb, f_2_emb, f_3_emb, f_4_emb, f_5_emb, f_6_emb], -1)\n",
    "            d = combine\n",
    "            d = tf.keras.layers.Dense(max_label+200, activation='tanh')(d)\n",
    "            d = tf.keras.layers.Dense(max_label+1, activation='softmax')(d)\n",
    "            output = d\n",
    "            model = tf.keras.Model(inputs=[path_input, content_input, feature_input_1, feature_input_2, feature_input_3, feature_input_4, feature_input_5, feature_input_6], outputs=output)\n",
    "\n",
    "            return model\n",
    "        \n",
    "        def model_word_only():\n",
    "            path_input = tf.keras.Input(shape=(path_max_len,), name='Path_emb_input')\n",
    "            content_input = tf.keras.Input(shape=(con_max_len,), name='Content_emb_input')\n",
    "            feature_input_1 = tf.keras.Input(shape=(1,), name='Feature_input1')\n",
    "            feature_input_2 = tf.keras.Input(shape=(1,), name='Feature_input2')\n",
    "            feature_input_3 = tf.keras.Input(shape=(1,), name='Feature_input3')\n",
    "            feature_input_4 = tf.keras.Input(shape=(1,), name='Feature_input4')\n",
    "            feature_input_5 = tf.keras.Input(shape=(1,), name='Feature_input5')\n",
    "            feature_input_6 = tf.keras.Input(shape=(1,), name='Feature_input6')\n",
    "\n",
    "            if DEBUG:\n",
    "                print(path_input.shape)\n",
    "\n",
    "            path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_input)\n",
    "            content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_input)\n",
    "\n",
    "            path_emb = tf.reshape(path_emb, [-1, max_num, path_max_len*path_emb_size])\n",
    "            content_emb = tf.reshape(content_emb, [-1, max_num, con_max_len*con_emb_size])\n",
    "\n",
    "            path_emb = tf.expand_dims(path_emb, -1)\n",
    "            content_emb = tf.expand_dims(content_emb, -1)\n",
    "\n",
    "            path_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3,  path_max_len*path_emb_size), strides=(1, path_max_len*path_emb_size), name='Conv_for_Path_emb', padding='same')(path_emb)\n",
    "            content_feature = tf.keras.layers.Conv2D(conv_num, kernel_size=(3, con_max_len*con_emb_size), strides=(1, con_max_len*con_emb_size), name='Conv_for_Content_emb', padding='same')(content_emb)\n",
    "\n",
    "            path = tf.reshape(path_feature, [-1, conv_num])\n",
    "            content = tf.reshape(content_feature, [-1, conv_num])\n",
    "\n",
    "            combine = tf.keras.layers.concatenate([path, content], -1)\n",
    "            d = combine\n",
    "            d = tf.keras.layers.Dense(max_label+200, activation='tanh')(d)\n",
    "            d = tf.keras.layers.Dense(max_label+1, activation='softmax')(d)\n",
    "            output = d\n",
    "            model = tf.keras.Model(inputs=[path_input, content_input, feature_input_1, feature_input_2, feature_input_3, feature_input_4, feature_input_5, feature_input_6], outputs=output)\n",
    "\n",
    "            return model\n",
    "        \n",
    "        # Model\n",
    "        model = model_word_only()\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(model.summary())\n",
    "        history = LossHistory()\n",
    "        stop_when_no_improve = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=0, patience = NO_IMPROVE, restore_best_weights=True)\n",
    "        until_loss = EarlyStoppingByLossVal(monitor='loss', value=UNTIL_LOSS, verbose=1)\n",
    "        callbacks = [history, stop_when_no_improve, until_loss]\n",
    "        \n",
    "        # Train\n",
    "        start = time.time()\n",
    "        model.fit([path_train, content_train, feature_train_1, feature_train_2, feature_train_3, feature_train_4, feature_train_5, feature_train_6], label_train, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True, batch_size=BATCH_SIZE)\n",
    "        t += time.time()-start\n",
    "        \n",
    "        # Save model\n",
    "        model.save(os.path.join(current_path, \"cnn\", \"set\", \"set-\" + str(set_num) + \"_model.h5\"))\n",
    "        del model\n",
    "        \n",
    "        #Load model\n",
    "        model = tf.keras.models.load_model(os.path.join(current_path, \"cnn\", \"set\", \"set-\" + str(set_num) + \"_model.h5\"))\n",
    "        '''model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy']\n",
    "        )'''\n",
    "        #model.load_weights(\"./cnn/set/set-\"+str(set_num)+\"_cnn-mlp.h5\")\n",
    "        \n",
    "        # Load Test file\n",
    "        df = get_df(os.path.join(current_path, \"cnn\", \"set\", \"Set-\" + str(set_num) + \"_ytest_raw.csv\"))\n",
    "        feature_test_1, feature_test_2, feature_test_3, feature_test_4, feature_test_5, feature_test_6, path_test, content_test, label_test = to_train_array_set(df, set_test_count, set_num)\n",
    "        \n",
    "        # Load word size\n",
    "        with open(os.path.join(current_path, \"cnn\", \"set\", \"word_size.txt\"), \"r\") as file:\n",
    "            path_word_size = eval(file.readline())\n",
    "            con_word_size = eval(file.readline())\n",
    "            \n",
    "        # Prediction\n",
    "        ts_start = time.time()\n",
    "        predictions = model.predict([path_test, content_test, feature_test_1, feature_test_2, feature_test_3, feature_test_4, feature_test_5, feature_test_6], batch_size=VAL_BATCH_SIZE)\n",
    "        ts += time.time()-ts_start\n",
    "        \n",
    "        # Get result\n",
    "        result = []\n",
    "        count = 0\n",
    "        for page in range(len(set_test_count[set_num-1])):\n",
    "            tmp = []\n",
    "            for node in range(max_num):\n",
    "                tmp.append(np.argmax(predictions[count]))\n",
    "                count += 1\n",
    "            result.append(tmp)\n",
    "        \n",
    "        # Read Col\n",
    "        col_type = []\n",
    "        with open(os.path.join(current_path, \"cnn\", \"set\", \"Set-\" + str(set_num) + \"_coltype.txt\"), \"r\") as file:\n",
    "            tmp = file.readline()\n",
    "            slot = eval(tmp)\n",
    "            col_type = slot\n",
    "            \n",
    "        # Output\n",
    "        Set = []\n",
    "        with open(os.path.join(current_path, \"cnn\", \"set\", \"set-\" + str(set_num) + \".csv\"), \"w\") as file: # Create prediction file\n",
    "            for col in col_type: # loop to write the Col type\n",
    "                file.write(col + \"\\t\")\n",
    "                if DEBUG:\n",
    "                    print(col + \"\\t\", end='')\n",
    "            if DEBUG:\n",
    "                print(\"\")\n",
    "            file.write(\"\\n\")\n",
    "            current_pos = 1\n",
    "            for page in tqdm(range(len(result))): # Loop each page\n",
    "                p_tmp = []\n",
    "                for cols in range(max_label+1):\n",
    "                    c_tmp = []\n",
    "                    for node in range(len(result[page])):\n",
    "                        r = result[page][node]\n",
    "                        if r == cols:\n",
    "                            c_tmp.append(node)\n",
    "                    p_tmp.append(c_tmp)\n",
    "                Set.append(p_tmp)\n",
    "            Set_tmp = Set.copy()\n",
    "            for page in range(len(Set_tmp)):\n",
    "                empty = False\n",
    "                col = []\n",
    "                for i in range(len(Set_tmp[page])):\n",
    "                    col.append(False)\n",
    "                col[0] = True\n",
    "                while(not empty):\n",
    "                    for cols in range(len(Set_tmp[page])):\n",
    "                        if len(Set_tmp[page][cols]) == 0:\n",
    "                            col[cols] = True\n",
    "                            if cols != 0:\n",
    "                                if DEBUG:\n",
    "                                    print(\"\\t\", end=\"\")\n",
    "                                file.write(\"\\t\")\n",
    "                        else:\n",
    "                            n = str(int(feature_test_1[page*max_num+Set_tmp[page][cols][0]]))\n",
    "                            if cols != 0:\n",
    "                                if DEBUG:\n",
    "                                    print(n+\"\\t\", end=\"\")\n",
    "                                file.write(n+\"\\t\")\n",
    "                            del Set_tmp[page][cols][0]\n",
    "                            if len(Set_tmp[page][cols]) == 0:\n",
    "                                col[cols] = True\n",
    "                        empty = True\n",
    "                        for i in col:\n",
    "                            if i == False:\n",
    "                                empty = False\n",
    "                                break\n",
    "                    if DEBUG:\n",
    "                        print(\"\\n\", end=\"\")\n",
    "                    file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timef = open(os.path.join(current_path, \"cnn\", \"data\", \"time_cnn.txt\"),\"w\")\n",
    "print(\"\\ntrain time:\"+str(t))\n",
    "timef.write(\"train:\"+str(t)+\"\\n\")\n",
    "print(\"test time:\"+str(ts))\n",
    "print(\"per page:\"+ str(float(ts)/page_c)+\"\\n\")\n",
    "timef.write(\"test:\"+str(ts)+\"\\n\")\n",
    "timef.write(\"per page:\"+ str(float(ts)/page_c)+\"\\n\")\n",
    "timef.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
