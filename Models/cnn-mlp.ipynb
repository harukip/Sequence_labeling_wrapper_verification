{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, unicode_literals, print_function, absolute_import\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from crflayer import CRF\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn_crfsuite import metrics\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import func\n",
    "import prepare_train_with_set as prepare\n",
    "import set_func\n",
    "import glob\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Set\n",
    "DEBUG = False        # Print element\n",
    "path_max_len = 30    # padding length\n",
    "path_emb_size = 5    # embedding size\n",
    "\n",
    "con_max_len = 50    # padding length\n",
    "con_emb_size = 5    # embedding size\n",
    "\n",
    "EPOCHS = 10000        # Train epochs\n",
    "conv_num = 5        # First cnn filter num\n",
    "#max_num = 206       # How many nodes should pad\n",
    "UNTIL_LOSS = 0.001    # When achieve loss then stop\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005) # Set learning rate\n",
    "NO_IMPROVE = 3     # Stop when no improve for epochs\n",
    "current_path = os.path.join(os.path.expanduser(\"~\"), \"jupyter\", \"Sequence_Labeling_Wrapper_Verification\", \"data\")\n",
    "data_path = os.path.join(current_path, \"data\")\n",
    "set_total = len(glob.glob(os.path.join(data_path, \"Set-*\")))\n",
    "print(\"Set:\", set_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_limit(num):\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only use the first GPU\n",
    "      try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[num], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "      except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tokenizer to convert words to encoding for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer():\n",
    "    tokenizer_path = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "    tokenizer_content = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "    return tokenizer_path, tokenizer_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_training(set_total, current_path, tokenizer_path, tokenizer_content, path_len, con_len):\n",
    "    train_data, Set_dict = prepare.train_file_generate(set_total, current_path)\n",
    "    test_data = prepare.test_file_generate(current_path)\n",
    "    max_num_train, max_label_train = func.load_data_num(train_data, True)\n",
    "    max_num_test = func.load_data_num(test_data, False)\n",
    "    max_num = max(max_num_train, max_num_test)\n",
    "    col_set_dict = dict(map(reversed, Set_dict.items()))\n",
    "    feature_train, word_train, label_train, out_train = func.cnn_process_data(train_data, tokenizer_path, tokenizer_content, path_len, con_len)\n",
    "    return test_data, feature_train, word_train, label_train, max_label_train, max_num, col_set_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingByLossVal(tf.keras.callbacks.Callback):\n",
    "    '''\n",
    "    Early stop when training value less than setting value.\n",
    "    '''\n",
    "    def __init__(self, monitor='loss', value=UNTIL_LOSS, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_model(max_num, max_label):\n",
    "    '''\n",
    "    Model definition for our experiments using tensorflow keras.\n",
    "    '''\n",
    "    path_input = tf.keras.Input(shape=(path_max_len,), name='Path_emb_input')\n",
    "    content_input = tf.keras.Input(shape=(con_max_len,), name='Content_emb_input')\n",
    "    feature_input = tf.keras.Input(shape=(6,), name='Feature_input')\n",
    "    \n",
    "    path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_input)\n",
    "    content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_input)\n",
    "    \n",
    "    feature = tf. reshape(feature_input, [-1, max_num, 6])\n",
    "    path_emb = tf.reshape(path_emb, [-1, max_num, path_max_len*path_emb_size, 1])\n",
    "    content_emb = tf.reshape(content_emb, [-1, max_num, con_max_len*con_emb_size, 1])\n",
    "    \n",
    "    path = tf.keras.layers.Conv2D(conv_num, (3, path_max_len*path_emb_size), (1, path_max_len*path_emb_size), padding='same')(path_emb)\n",
    "    con = tf.keras.layers.Conv2D(conv_num, (3, con_max_len*con_emb_size), (1, con_max_len*con_emb_size), padding='same')(content_emb)\n",
    "    \n",
    "    path_emb = tf.reshape(path, [-1, max_num, conv_num])\n",
    "    content_emb = tf.reshape(con, [-1, max_num, conv_num])\n",
    "    \n",
    "    combine = tf.keras.layers.concatenate([feature, path_emb, content_emb], -1)\n",
    "    \n",
    "    mlp = combine\n",
    "    d = tf.reshape(mlp, [-1, 6 + conv_num*2])\n",
    "    d = tf.keras.layers.Dense(max_label+200, activation='tanh')(d)\n",
    "    d = tf.keras.layers.Dense(max_label+1, activation='softmax')(d)\n",
    "    output = d\n",
    "    model = tf.keras.Model(inputs=[feature_input, path_input, content_input], outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_word_only(max_num, max_label):\n",
    "    '''\n",
    "    Model definition for our experiments using tensorflow keras.\n",
    "    '''\n",
    "    path_input = tf.keras.Input(shape=(path_max_len,), name='Path_emb_input')\n",
    "    content_input = tf.keras.Input(shape=(con_max_len,), name='Content_emb_input')\n",
    "    feature_input = tf.keras.Input(shape=(6,), name='Feature_input')\n",
    "    \n",
    "    path_emb = tf.keras.layers.Embedding(path_word_size+1, path_emb_size)(path_input)\n",
    "    content_emb = tf.keras.layers.Embedding(con_word_size+1, con_emb_size)(content_input)\n",
    "    \n",
    "    path_emb = tf.reshape(path_emb, [-1, max_num, path_max_len*path_emb_size, 1])\n",
    "    content_emb = tf.reshape(content_emb, [-1, max_num, con_max_len*con_emb_size, 1])\n",
    "    \n",
    "    path = tf.keras.layers.Conv2D(conv_num, (3, path_max_len*path_emb_size), (1, path_max_len*path_emb_size), padding='same')(path_emb)\n",
    "    con = tf.keras.layers.Conv2D(conv_num, (3, con_max_len*con_emb_size), (1, con_max_len*con_emb_size), padding='same')(content_emb)\n",
    "    \n",
    "    path_emb = tf.reshape(path, [-1, max_num, conv_num])\n",
    "    content_emb = tf.reshape(con, [-1, max_num, conv_num])\n",
    "    \n",
    "    combine = tf.keras.layers.concatenate([path_emb, content_emb], -1)\n",
    "    \n",
    "    mlp = combine\n",
    "    d = tf.reshape(mlp, [-1, conv_num*2])\n",
    "    d = tf.keras.layers.Dense(max_label+200, activation='tanh')(d)\n",
    "    d = tf.keras.layers.Dense(max_label+1, activation='softmax')(d)\n",
    "    output = d\n",
    "    model = tf.keras.Model(inputs=[feature_input, path_input, content_input], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    import pickle\n",
    "    #model.save_weights(\"./cnn/data/cnn.h5\")\n",
    "    model.save(os.path.join(current_path, \"cnn\", \"data\", \"model.h5\"))\n",
    "    # saving\n",
    "    with open(os.path.join(current_path, \"cnn\", \"data\", \"tokenizer_path.pickle\"), \"wb\") as handle:\n",
    "        pickle.dump(tokenizer_path, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(current_path, \"cnn\", \"data\", \"tokenizer_content.pickle\"), \"wb\") as handle:\n",
    "        pickle.dump(tokenizer_content, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    import pickle\n",
    "    model = tf.keras.models.load_model(os.path.join(current_path, \"cnn\", \"data\", \"model.h5\"))\n",
    "    model.summary()\n",
    "    # loading\n",
    "    with open(os.path.join(current_path, \"cnn\", \"data\", \"tokenizer_path.pickle\"), 'rb') as handle:\n",
    "        tokenizer_path = pickle.load(handle)\n",
    "    with open(os.path.join(current_path, \"cnn\", \"data\", \"tokenizer_content.pickle\"), 'rb') as handle:\n",
    "        tokenizer_content = pickle.load(handle)        \n",
    "    path_word_size = len(tokenizer_path.index_docs)\n",
    "    con_word_size = len(tokenizer_content.index_docs)\n",
    "    return model, tokenizer_path, tokenizer_content, path_word_size, con_word_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(predictions, max_num):\n",
    "    result = []\n",
    "    count = 0\n",
    "    for page in range(int(len(predictions)/max_num)):\n",
    "        tmp = []\n",
    "        for node in range(max_num):\n",
    "            tmp.append(np.argmax(predictions[count]))\n",
    "            count += 1\n",
    "        result.append(tmp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_process_data(df, tokenizer_path, tokenizer_content, path_max_len, con_max_len):\n",
    "    '''\n",
    "    Load the csv file and convert it to np array.\n",
    "    '''\n",
    "    max_num, max_label = func.load_data_num(df, True)\n",
    "    num, index = func.node_num(df['Leafnode'])\n",
    "    \n",
    "    num_cols = ['Leafnode', 'PTypeSet', 'TypeSet', 'Contentid', 'Pathid', 'Simseqid']\n",
    "    features = []\n",
    "    word_features = []\n",
    "    tokenizer_path.fit_on_texts(df['Path'])\n",
    "    tokenizer_content.fit_on_texts(df['Content'])\n",
    "    path_encoded = tokenizer_path.texts_to_sequences(df['Path'])\n",
    "    df['Content'] = df['Content'].str.replace('/|\\.|\\?|:|=|,|<|>|&|@|\\+|-|#|~|\\|', ' ')\n",
    "    df['Content'] = df['Content'].astype(str)\n",
    "    content_encoded = tokenizer_content.texts_to_sequences(df['Content'])\n",
    "    path_pad = tf.keras.preprocessing.sequence.pad_sequences(path_encoded, path_max_len, padding='post')\n",
    "    content_pad = tf.keras.preprocessing.sequence.pad_sequences(content_encoded, con_max_len, padding='post')\n",
    "    \n",
    "    word_cols = [path_pad, content_pad]\n",
    "    word_max_len = [path_max_len, con_max_len]\n",
    "    \n",
    "    for c in range(len(num_cols)):\n",
    "        features.append(np.array(func.node_data(df[num_cols[c]], num, max_num)).astype('int32'))\n",
    "        features[c] = np.expand_dims(features[c], -1)\n",
    "    \n",
    "    for c in range(len(word_cols)):\n",
    "        word_features.append(np.array(func.node_emb(word_cols[c], num, word_max_len[c], max_num)).astype('int32'))\n",
    "    label_array = np.array(func.label_padding(df['Label'], num, max_num)).astype('int32')\n",
    "    m_label = df['Label'].max()\n",
    "    \n",
    "    feature = np.concatenate([feature for feature in features], -1)\n",
    "    feature = np.reshape(feature, [features[0].shape[0]*max_num, 6])\n",
    "    \n",
    "    word = [np.reshape(word_features[c], [word_features[c].shape[0]*max_num, word_max_len[c]]) for c in range(len(word_cols))]\n",
    "    feature = feature.astype('float32')\n",
    "    label_array = label_array.flatten()\n",
    "    return feature, word, label_array, m_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_padding_set(df, set_count, set_num, pad_len):\n",
    "    emb = []\n",
    "    tmp = []\n",
    "    for i in range(pad_len):\n",
    "        tmp.append(0)\n",
    "    count = 0\n",
    "    for pages in set_count[set_num-1]:\n",
    "        set_len = pages\n",
    "        for i in range(set_len):\n",
    "            emb.append(df[count])\n",
    "            count += 1\n",
    "        if set_len != max_set[set_num-1]:\n",
    "            for i in range(max_set[set_num-1]-set_len):\n",
    "                emb.append(tmp)\n",
    "    if DEBUG:\n",
    "        print(count)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_process_set(df, set_count, set_num, max_set, path_max_len, con_max_len):\n",
    "    num, index = func.node_num(df['Leafnode'])\n",
    "    cols = ['Leafnode', 'PTypeSet', 'TypeSet', 'Contentid', 'Pathid', 'Simseqid']\n",
    "    features = []\n",
    "    word_features = []\n",
    "    for c in range(len(cols)):\n",
    "        features.append(np.array(set_func.feature_padding_set(df[cols[c]], set_count, set_num, max_set)).astype('int32'))\n",
    "        features[c] = np.expand_dims(features[c], -1)\n",
    "    \n",
    "    word_cols = [\"Path\", \"Content\"]\n",
    "    word_max_len = [path_max_len, con_max_len]\n",
    "    for c in range(len(word_cols)):\n",
    "        word_features.append(np.array(emb_padding_set(df[word_cols[c]], set_count, set_num, word_max_len[c])).astype('int32'))\n",
    "    \n",
    "    features = np.concatenate([feature for feature in features], -1)\n",
    "    features = np.reshape(features, [len(set_count[set_num]) * max_set[set_num], 6])\n",
    "    \n",
    "    features = features.astype('float32')\n",
    "    label = np.array(set_func.label_padding_set(df['Label'], set_count, set_num, max_set)).astype('int32')\n",
    "    label = label.flatten()\n",
    "    return features, word_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # How many Set\n",
    "    set_total = 1\n",
    "    model_name = \"cnn\"\n",
    "    current_path = os.path.join(os.path.expanduser(\"~\"), \"jupyter\", \"Sequence_Labeling_Wrapper_Verification\", \"data\")\n",
    "    \n",
    "    # GPU\n",
    "    gpu_limit(1)\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer_path, tokenizer_content = tokenizer()\n",
    "    \n",
    "    # Process training file\n",
    "    train_data, Set_dict = prepare.train_file_generate(set_total, current_path)\n",
    "    test_data = prepare.test_file_generate(current_path)\n",
    "    max_num_train, max_label_train = func.load_data_num(train_data, True)\n",
    "    max_num_test = func.load_data_num(test_data, False)\n",
    "    max_num = max(max_num_train, max_num_test)\n",
    "    col_set_dict = dict(map(reversed, Set_dict.items()))\n",
    "    X_train, word_train, y_train, _ = cnn_process_data(train_data, tokenizer_path, \n",
    "                                                                         tokenizer_content, path_max_len, \n",
    "                                                                         con_max_len)\n",
    "    \n",
    "    BATCH_SIZE = max_num      # batch size\n",
    "    VAL_BATCH_SIZE = max_num  # Validation batch size\n",
    "    path_word_size = len(tokenizer_path.index_docs)\n",
    "    con_word_size = len(tokenizer_content.index_docs)\n",
    "    page_num = int(len(y_train)/max_num)\n",
    "    \n",
    "    # Define model\n",
    "    model = full_model(max_num, max_label_train)\n",
    "    history = func.LossHistory()\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(model.summary())\n",
    "    stop_when_no_improve = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=0, patience = NO_IMPROVE, restore_best_weights=True)\n",
    "    until_loss = EarlyStoppingByLossVal(monitor='loss', value=UNTIL_LOSS, verbose=1)\n",
    "    callbacks = [history, stop_when_no_improve, until_loss]\n",
    "    \n",
    "    # Start training\n",
    "    start = time.time()\n",
    "    model.fit([X_train, word_train[0], word_train[1]], y_train, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True, batch_size=BATCH_SIZE)\n",
    "    t = time.time()-start\n",
    "    \n",
    "    # Graph\n",
    "    #history.loss_plot('epoch')\n",
    "    \n",
    "    # Load test feature\n",
    "    X_test, word_test, y_test, _ = cnn_process_data(test_data, tokenizer_path, tokenizer_content, path_max_len, con_max_len)\n",
    "    \n",
    "    # Start testing\n",
    "    ts_start = time.time()\n",
    "    pred = model.predict([X_test, word_test[0], word_test[1]], batch_size=VAL_BATCH_SIZE)\n",
    "    ts = time.time()-ts_start\n",
    "    \n",
    "    # Process output\n",
    "    result = get_result(pred, max_num)\n",
    "    col_type = func.get_col_type(current_path)\n",
    "    Set_data = func.predict_output(set_total, current_path, model_name, col_type, result, max_label_train, col_set_dict)\n",
    "    set_train_data, set_train_count = set_func.Set_train_file_generate(set_total, current_path, model_name, X_train, word_train, max_num)\n",
    "    set_test_data, set_test_count = set_func.Set_test_file_generate(set_total, current_path, model_name, Set_data, X_test, word_test, max_num)\n",
    "    page_c = len(result)\n",
    "    \n",
    "    # Process set\n",
    "    max_num_train = set_func.max_num_set(set_total, set_train_count)\n",
    "    max_num_test = set_func.max_num_set(set_total, set_test_count)\n",
    "    max_set = []\n",
    "    for i in range(len(max_num_train)):\n",
    "        max_set.append(max(max_num_train[i], max_num_test[i]))\n",
    "    \n",
    "    for num in range(set_total):\n",
    "        set_X_train, set_word_train, set_y_train = cnn_process_set(set_train_data[num], \n",
    "                                                                             set_train_count, num, \n",
    "                                                                             max_set, path_max_len, \n",
    "                                                                             con_max_len)\n",
    "        max_num = max_set[num]\n",
    "        max_label = max(set_train_data[num]['Label'])\n",
    "        BATCH_SIZE = max_num      # batch size\n",
    "        VAL_BATCH_SIZE = max_num  # Validation batch size\n",
    "        page_num = int(len(set_X_train)/max_num)\n",
    "        set_model = full_model(max_num, max_label)\n",
    "        history = func.LossHistory()\n",
    "        set_model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(set_model.summary())\n",
    "        stop_when_no_improve = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', min_delta=0, patience = NO_IMPROVE, restore_best_weights=True)\n",
    "        until_loss = EarlyStoppingByLossVal(monitor='loss', value=UNTIL_LOSS, verbose=1)\n",
    "        callbacks = [history, stop_when_no_improve, until_loss]\n",
    "\n",
    "        # Train\n",
    "        start = time.time()\n",
    "        set_model.fit([set_X_train, set_word_train[0], set_word_train[1]], set_y_train, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True, batch_size=BATCH_SIZE)\n",
    "        tst = time.time()-start\n",
    "        t += tst\n",
    "\n",
    "        # Load Test file\n",
    "        set_X_test, set_word_test, set_y_test = cnn_process_set(set_test_data[num], \n",
    "                                                                set_test_count, num, \n",
    "                                                                max_set, path_max_len, \n",
    "                                                                con_max_len)\n",
    "        \n",
    "        page_test = int(len(set_X_test) / max_num)\n",
    "\n",
    "        # Prediction\n",
    "        ts_start = time.time()\n",
    "        pred = set_model.predict([set_X_test, set_word_test[0], set_word_test[1]], batch_size=VAL_BATCH_SIZE)\n",
    "        tsp = time.time()-ts_start\n",
    "        ts += tsp\n",
    "        result = get_result(pred, max_num)\n",
    "        \n",
    "        # Read Col\n",
    "        set_col_type = set_func.get_col_type(current_path, num)\n",
    "\n",
    "        # Output\n",
    "        set_func.predict_output(current_path, model_name, num, set_col_type, result, max_label, set_X_test, max_num)\n",
    "    \n",
    "    # Process time\n",
    "    func.process_time(current_path, model_name, t, ts, page_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
